[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenData",
    "section": "",
    "text": "OpenData is a database of publicly available behavioral datasets. To browse the database, click on the links above or use the search bar at the top-right of this page. The goal of this project is simply to make it easier for researchers to find and use publicly available behavioral data as part of research."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "OpenData",
    "section": "💼 Get In Touch",
    "text": "💼 Get In Touch\nEmail me at yuanbopsy@gmail.com, or DM me on  or  if you’d like to chat!"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "OpenData",
    "section": "📮 Blog Posts",
    "text": "📮 Blog Posts\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n5/5/23\n\n\nNafcha & Hertz, 2023\n\n\nAsymmetric cognitive learning mechanisms underlying the persistence of intergroup bias\n\n\n\n\n1/27/23\n\n\nAbir et al. (2023)\n\n\nHuman Exploration Strategically Balances Approaching and Avoiding Uncertainty\n\n\n\n\n11/24/21\n\n\nAlgermissen et al. (2021)\n\n\nStriatal BOLD and midfrontal theta power express motivation for action\n\n\n\n\n4/4/16\n\n\nAlbrecht et al. (2016)\n\n\nReduction of Pavlovian Bias in Schizophrenia: Enhanced Effects in Clozapine-Administered Patients\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#subscribe",
    "href": "index.html#subscribe",
    "title": "OpenData",
    "section": "📬 Subscribe",
    "text": "📬 Subscribe\nSubscribe via  RSS or enter your email below:"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "🎤 Talks",
    "section": "",
    "text": "These are a list of talks I’ve given:\n\nHow to evaluate ML Tooling: Guest Lecutre for Stanford CS 329S ML Systems Design, Feb 2022. Slides, Video\nTensorFlow World, 2019: “Automating your developer workflow on GitHub with Tensorflow”. Slides, Link\nKubeCon 2018, “Natural Language Code Search With Kubeflow”. Slides, Video\nKDD, London August 2018: Hands on tutorial, “Feature Extraction and Summarization With Sequence to Sequence Learning”. Tutorial-site\nMl4all, Portland May 2018: “How to Create Magical Data Products Using Sequence-to-Sequence Models”. Slides, Video"
  },
  {
    "objectID": "blog/posts/Hertz2023.html",
    "href": "blog/posts/Hertz2023.html",
    "title": "Nafcha & Hertz, 2023",
    "section": "",
    "text": "An individual can learn about someone else’s traits by accumulating experiences from the specific relationship and by incorporating group identity information. Here we examine how group identity affects learning about others and the social learning mechanisms underlying the persistence of intergroup bias. Participants played a game with four other bot-players that entailed collecting stars and could sacrifice a move to zap another player, who would then lose three turns. The bot-players’ avatars were either the same or a different color than the participant’s avatar. Over six experimental conditions, participants were exposed to different behavioral patterns of bot-players—either zappers or zap-avoiders. We found that participants adjusted their zapping behavior according to the bot-player’s behavior and were more likely to zap zappers than avoiders. Participants were also more likely to zap outgroup players and less likely to zap ingroup players, regardless of the player’s behavior, indicating a persistent intergroup bias. Using a computational learning model, we identified the contribution of three learning mechanisms to this tendency. Prior beliefs about players’ zapping behavior were higher for outgroup than for ingroup players. Learning rates were very low for outgroup players, making it very difficult to overcome prior beliefs, whereas learning rates for ingroup players were high. Finally, participants attributed the negative behavior of one outgroup player to all other outgroup players, making beliefs about outgroup players more homogeneous. Our results show how group identity shapes and confounds social learning and highlight how intergroup bias can persist despite interaction and experience.\n一个人可以通过积累与特定关系的经验和融入群体身份信息来了解别人的特点。在这里，我们研究群体身份如何影响对他人的学习以及支持种族偏见持续存在的社会学习机制。参与者与四个其他机器玩家一起玩一个收集星星并可以牺牲移动来击中另一个玩家（然后该玩家将失去三次回合）的游戏。机器玩家的头像要么与参与者的头像相同，要么不同颜色。在六个实验条件下，参与者接触到不同行为模式的机器玩家——即打手或避开打手。我们发现，参与者根据机器玩家的行为调整自己击中其他人（zapping） 的行为，并更有可能攻击打手而非避开打手。无论是什么情况下，参与者都更有可能攻击外部组成员而不是内部组成员，并且很少攻击内部组成员，表明种族间偏见具有持续性质。使用计算学习模型，我们确定了三种学习机制对此倾向性产生贡献：关于球员 zapping 行为先前信念对外团队比内团队更高。对于外团队的学习率非常低，很难克服先前的信念，而内团队的学习率则较高。最后，参与者将一个外部组成员的负面行为归因于所有其他外部组成员，使得关于外部组成员的信仰更加同质化。我们的结果展示了群体身份如何塑造和混淆社会学习，并强调种族偏见如何在互动和经验中持续存在。"
  },
  {
    "objectID": "blog/posts/Hertz2023.html#references",
    "href": "blog/posts/Hertz2023.html#references",
    "title": "Nafcha & Hertz, 2023",
    "section": "References",
    "text": "References\njournal: PsyArXiv\npaper_url: https://psyarxiv.com/b95ev\ndata_url: https://osf.io/nx2hv/"
  },
  {
    "objectID": "blog/posts/abir2023.html",
    "href": "blog/posts/abir2023.html",
    "title": "Abir et al. (2023)",
    "section": "",
    "text": "The purpose of exploration is to reduce goal-relevant uncertainty. This can be achieved by choosing to explore the parts of the environment one is most uncertain about. Humans, however, often choose to avoid uncertainty. How do humans balance approaching and avoiding uncertainty during exploration? To answer this question, we developed a task requiring participants to explore a simulated environment towards a clear goal. We compared human choices to the predictions of the optimal exploration policy and a hierarchy of simpler strategies. We found that participants generally explored the object they were more uncertain about. However, when overall uncertainty about choice options was high, participants avoided objects they were more uncertain about, learning instead about better known objects. We examined reaction times and individual differences to understand the costs and benefits of this strategy. We conclude that balancing approaching and avoiding uncertainty ameliorates the costs of exploration in a resource-rational manner."
  },
  {
    "objectID": "blog/posts/abir2023.html#references",
    "href": "blog/posts/abir2023.html#references",
    "title": "Abir et al. (2023)",
    "section": "References",
    "text": "References\njournal: PsyArXiv\npaper_url: https://doi.org/10.31234/osf.io/gtxam\ndata_url: https://osf.io/6zyev/"
  },
  {
    "objectID": "blog/posts/algermissen2021.html",
    "href": "blog/posts/algermissen2021.html",
    "title": "Algermissen et al. (2021)",
    "section": "",
    "text": "Action selection is biased by the valence of anticipated outcomes. To assess mechanisms by which these motivational biases are expressed and controlled, we measured simultaneous EEG-fMRI during a motivational Go/NoGo learning task (N = 36), leveraging the temporal resolution of EEG and subcortical access of fMRI. VmPFC BOLD encoded cue valence, importantly predicting trial-by-trial valence-driven response speed differences and EEG theta power around cue onset. In contrast, striatal BOLD encoded selection of active Go responses and correlated with theta power around response time. Within trials, theta power ramped in the fashion of an evidence accumulation signal for the value of making a Go response, capturing the faster responding to reward cues. Our findings reveal a dual nature of midfrontal theta power, with early components reflecting the vmPFC contribution to motivational biases, and late components reflecting their striatal translation into behavior, in line with influential recent value of work theories of striatal processing."
  },
  {
    "objectID": "blog/posts/algermissen2021.html#references",
    "href": "blog/posts/algermissen2021.html#references",
    "title": "Algermissen et al. (2021)",
    "section": "References",
    "text": "References\njournal: Cereb. Cortex\npaper_url: https://doi.org/10.1093/cercor/bhab391\ndata_url: https://data.donders.ru.nl/collections/di/dccn/DSC_3017042.02_604?6"
  },
  {
    "objectID": "blog/posts/albrecht2016.html",
    "href": "blog/posts/albrecht2016.html",
    "title": "Albrecht et al. (2016)",
    "section": "",
    "text": "The negative symptoms of schizophrenia (SZ) are associated with a pattern of reinforcement learning (RL) deficits likely related to degraded representations of reward values. However, the RL tasks used to date have required active responses to both reward and punishing stimuli. Pavlovian biases have been shown to affect performance on these tasks through invigoration of action to reward and inhibition of action to punishment, and may be partially responsible for the effects found in patients. Forty-five patients with schizophrenia and 30 demographically-matched controls completed a four-stimulus reinforcement learning task that crossed action (Go or NoGo) and the valence of the optimal outcome (reward or punishment-avoidance), such that all combinations of action and outcome valence were tested. Behaviour was modelled using a six-parameter RL model and EEG was simultaneously recorded. Patients demonstrated a reduction in Pavlovian performance bias that was evident in a reduced Go bias across the full group. In a subset of patients administered clozapine, the reduction in Pavlovian bias was enhanced. The reduction in Pavlovian bias in SZ patients was accompanied by feedback processing differences at the time of the P3a component. The reduced Pavlovian bias in patients is suggested to be due to reduced fidelity in the communication between striatal regions and frontal cortex. It may also partially account for previous findings of poorer Go-learning in schizophrenia where Go responses or Pavlovian consistent responses are required for optimal performance. An attenuated P3a component dynamic in patients is consistent with a view that deficits in operant learning are due to impairments in adaptively using feedback to update representations of stimulus value.\n精神分裂症（SZ）的负性症状与一种弱化的奖励价值表征相关的强化学习（RL）缺陷模式有关。然而，迄今为止使用的RL任务需要对奖励和惩罚刺激做出积极反应。巴甫洛夫偏差已被证明会通过激发对奖励的行动和抑制对惩罚的行动来影响这些任务的表现，并且可能部分解释病人的影响。45名患有精神分裂症的患者和30名与之匹配的对照组完成了一个四刺激强化学习任务，该任务交叉了行动（Go或NoGo）和最佳结果的价值（奖励或惩罚-避免），以测试所有行动和结果价值的组合。使用六参数RL模型对行为进行建模，并同时记录脑电图（EEG）。患者表现出巴甫洛夫性能偏差的减少，这在全组中表现为Go偏差的减少。在接受氯氮平治疗的患者子集中，巴甫洛夫偏差的减少得到了增强。患者中巴甫洛夫偏差的减少伴随着P3a成分时的反馈处理差异。患者中巴甫洛夫性能偏差的降低被认为是由于纹状体区域和前额皮层之间通信的保真度降低。这也可能部分解释了以前发现的精神分裂症患者学习Go或巴甫洛夫一致反应时表现较差的情况。患者中P3a成分动态的减弱支持了一种观点，即操作性学习的缺陷是由于不能适应性地使用反馈来更新刺激价值表征而导致的。"
  },
  {
    "objectID": "blog/posts/albrecht2016.html#references",
    "href": "blog/posts/albrecht2016.html#references",
    "title": "Albrecht et al. (2016)",
    "section": "References",
    "text": "References\njournal: PLoS One\npaper_url: https://doi.org/10.1371/journal.pone.0152781\ndata_url: https://zenodo.org/record/29601"
  },
  {
    "objectID": "blog/secret.html",
    "href": "blog/secret.html",
    "title": "Hamel’s Blog",
    "section": "",
    "text": "This page is supposed to be secret!\n\n\n\nA listing of all my blog posts can be found here\n\n\n\n\n\n\n\n\n\n\n\n\nNafcha & Hertz, 2023\n\n\nAsymmetric cognitive learning mechanisms underlying the persistence of intergroup bias\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\nAbir et al. (2023)\n\n\nHuman Exploration Strategically Balances Approaching and Avoiding Uncertainty\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\nAlgermissen et al. (2021)\n\n\nStriatal BOLD and midfrontal theta power express motivation for action\n\n\n\n\n\n\n\n\n\nNov 24, 2021\n\n\n\n\n\n\n\n\nAlbrecht et al. (2016)\n\n\nReduction of Pavlovian Bias in Schizophrenia: Enhanced Effects in Clozapine-Administered Patients\n\n\n\n\n\n\n\n\n\nApr 4, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About OpenData",
    "section": "",
    "text": "OpenData is a database of publicly available behavioral datasets. To browse the database, click on the links above or use the search bar at the top-right of this page.\n\nWhat is the goal of OpenData?\n\nThe goal of this project is simply to make it easier for researchers to find and use publicly available behavioral data as part of research. There’s already so much out there that can be used to:\n\nTest new hypotheses or models\nCalculate effect sizes for power analysis\nEstimate meta-analytic effects across studies\n\n\nWhat is the scope of OpenData?\n\nThe scope of this project is to catalogue any and all open datasets involving experimental or cognitive tasks (e.g., Stroop, delay discounting, 2-arm bandits). Datasets involving more naturalistic behaviors are also welcomed. The only firm requirement is that trial-level data must be available.\n\nWho maintains OpenData?\n\nThis project is maintained by Bo Yuan with help from other members of the SCMD Lab.\n\nHow can I add a dataset?\n\nPlease see the contributing page.\n\nHow can I report an issue?\n\nPlease open an issue on our Github or directly contact the maintainer."
  },
  {
    "objectID": "guest-blog.html",
    "href": "guest-blog.html",
    "title": "Guest Blogs",
    "section": "",
    "text": "nbdev + Quarto: A new secret weapon for productivity, the fastai blog, July 2022.\nNotebooks in production with Metaflow Introduces a new Metaflow feature that allows users to use notebooks in production ML workflows.\nPython Concurrency: The Tricky Bits: An exploration of threads, processes, and coroutines in Python, with interesting examples that illuminate the differences between each.\nghapi, a new third-party Python client for the GitHub API by Jeremy Howard & Hamel Husain, GitHub Repo.\nNbdev: A literate programming environment that democratizes software engineering best practices by Hamel Husain, Jeremy Howard, The GitHub Blog.\nfastcore: An Underrated Python Library by Hamel Husain, Jeremy Howard, GitHub Repo.\nData Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes: An end-to-end example of deploying a machine learning product using Jupyter, Papermill, Tekton, GitOps and Kubeflow. by Jeremy Lewi, Hamel_Husain, The Kubeflow Blog.\nIntroducing fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. by Jeremy Howard & Hamel Husain, GitHub Repo\nGitHub Actions: Providing Data Scientists With New Superpowers by Jeremy Howard & Hamel Husain.\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search: by Miltiadis Allamanis, Marc Brockschmidt, Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit GitHub Repo\nHow To Create Natural Language Semantic Search for Arbitrary Objects With Deep Learning. (Related: GitHub engineering blog article, Live demo)\nHow To Create Magical Data Products Using Sequence-to-Sequence Models\nHow to Automate Tasks on GitHub With Machine Learning for Fun and Profit\nHow Docker Can Make You A More Effective Data Scientist\nAutomated Machine Learning, A Paradigm Shift That Accelerates Data Scientst Productivity At Airbnb"
  },
  {
    "objectID": "notes/how-to-learn/index.html",
    "href": "notes/how-to-learn/index.html",
    "title": "How to learn",
    "section": "",
    "text": "I read the book Mindshift and it was unituitively so good that I decided to take this class. As a parent, I learned a bunch of things that I think will be beneficial to my children’s education.\nNotes from class Learning how to learn. These notes are for me and may not make sense for others."
  },
  {
    "objectID": "notes/how-to-learn/index.html#focused-vs-diffused-mode",
    "href": "notes/how-to-learn/index.html#focused-vs-diffused-mode",
    "title": "How to learn",
    "section": "Focused vs Diffused Mode",
    "text": "Focused vs Diffused Mode\nYou can not access focus and diffused mode simultaneously.\nPeople have tried to access diffuse mode of thinking by bringing themselves to the point of sleep and waking up just as they fall asleep. For example, Salvador Dali - holding keys in your hand, and let the sound of keys falling the ground wake you up.\nExercise, going for a walk good way to access diffuse thinking. You must take notes right away b/c diffuse thoughts may evaporate very fast."
  },
  {
    "objectID": "notes/how-to-learn/index.html#procrastination-memory-and-sleep",
    "href": "notes/how-to-learn/index.html#procrastination-memory-and-sleep",
    "title": "How to learn",
    "section": "Procrastination Memory and Sleep",
    "text": "Procrastination Memory and Sleep\nThey advocate the Pomodoro technique to combating procrastination. Its like HITT.\nPeriodic relaxation (every ~ 30 minutes) is important for accessing your diffuse mode. “Its important for the mortar to dry”.\nSpaced repetition (like Anki) is important for building memory. i\nGo over what you want to learn about right before you go to sleep, this will substantially improve the chances you will dream about it and form new connections about the subject.\nExercise can help create new neurons in your hippocampus (new neurons can be created there in adulthood) and help them survive longer."
  },
  {
    "objectID": "notes/how-to-learn/index.html#writing-tips-diffuse-mode",
    "href": "notes/how-to-learn/index.html#writing-tips-diffuse-mode",
    "title": "How to learn",
    "section": "Writing Tips Diffuse Mode",
    "text": "Writing Tips Diffuse Mode\nDiffuse mode is very important for writing. Editing is like focus mode and creating ideas is diffuse mode. Some rules of thumb: - Do not outline, make a mind map - Do not edit while you are writing (this is really hard to do -> turn off monitor and just write).https://writeordie.com - app that forces you to stay in diffuse mode. You really cannot look at the screen. - Repeating again, do not look at screen while you are writing! Only when editing."
  },
  {
    "objectID": "notes/how-to-learn/index.html#chunking",
    "href": "notes/how-to-learn/index.html#chunking",
    "title": "How to learn",
    "section": "Chunking",
    "text": "Chunking\n“Tying your shoes”. Best chunks are subconscious. Spoken language is the best example of chunking. You have to practice to build chunks, you cannot just observe. You have to perform the task yourself.\nYou should scan a chapter before you read it: section headings, pictures, etc. This can help you build chunks."
  },
  {
    "objectID": "notes/how-to-learn/index.html#illusions-of-competence",
    "href": "notes/how-to-learn/index.html#illusions-of-competence",
    "title": "How to learn",
    "section": "Illusions of competence",
    "text": "Illusions of competence\nRight after you read something, look away and repeat to yourself what you recall. You can also draw a concept map. The recall process actually improves memory.\nRecall is better than re-reading. Re-reading is effective when you let time pass so you get spaced repetition. You need to test yourself to make sure you are competent. Recall is a form of testing.\nRecall outside your place of study to strengthen your memory. This is because you can get queues from where you are studying."
  },
  {
    "objectID": "notes/how-to-learn/index.html#deliberate-practice",
    "href": "notes/how-to-learn/index.html#deliberate-practice",
    "title": "How to learn",
    "section": "Deliberate Practice",
    "text": "Deliberate Practice\nFocus on the bits that you find difficult. Interleaving is important, meaning learning different subjects or even sections within one subject at once. Thomas S. Khun discovered that two types of people tend to make scientific breakthroughs: (1) young people (2) those who are trained in another discipline."
  },
  {
    "objectID": "notes/how-to-learn/index.html#procrastination-and-memory",
    "href": "notes/how-to-learn/index.html#procrastination-and-memory",
    "title": "How to learn",
    "section": "Procrastination and Memory",
    "text": "Procrastination and Memory\nYou have already learned about the Pomodoro technique. There are other techniques.\nFocus on the process, not the product. Don’t focus on completing the homework, focus on the process that leads you to complete the homework. Process is the small chunks of time to chip away at the task. This is the idea behind the Pomodoro. Your only goal is to finish the Pomodoro, for example."
  },
  {
    "objectID": "notes/how-to-learn/index.html#juggling-life-and-learning",
    "href": "notes/how-to-learn/index.html#juggling-life-and-learning",
    "title": "How to learn",
    "section": "Juggling Life and Learning",
    "text": "Juggling Life and Learning\nYou should make to-do list the night before for the next day and write it down. This will allow your subconscious to work on how it will conquer that task. Furthermore, writing it down will allow you to free it from working memory.\nPlan your quitting time is important."
  },
  {
    "objectID": "notes/concurrency.html",
    "href": "notes/concurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "Understand the world of Python concurrency: threads, processes, coroutines and asynchronous programming with a realistic examples.\nSee this blog article."
  },
  {
    "objectID": "notes/pandoc/index.html",
    "href": "notes/pandoc/index.html",
    "title": "pandoc filters",
    "section": "",
    "text": "Two python packages\nThe tutorial on pandoc filters can help you get oriented to the general idea. If rolling your own filters, you probably want to use the JSON filters. Furthermore you can understand the pandoc AST by using the -t native flag (examples of this are shown later)."
  },
  {
    "objectID": "notes/pandoc/index.html#the-minimal-notebook",
    "href": "notes/pandoc/index.html#the-minimal-notebook",
    "title": "pandoc filters",
    "section": "The minimal notebook",
    "text": "The minimal notebook\nHere is minimal notebook we are working with:\njson title=\"minimal.ipynb\" {  \"cells\": [   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## A minimal notebook\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"<MyTag></MyTag>\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": 1,    \"metadata\": {},    \"outputs\": [     {      \"name\": \"stdout\",      \"output_type\": \"stream\",      \"text\": [       \"2\\n\"      ]     }    ],    \"source\": [     \"# Do some arithmetic\\n\",     \"print(1+1)\"    ]   }  ],  \"metadata\": {   \"interpreter\": {    \"hash\": \"42fd40e048e0585f88ec242f050f7ef0895cf845a8dd1159352394e5826cd102\"   },   \"kernelspec\": {    \"display_name\": \"Python 3.9.7 ('base')\",    \"language\": \"python\",    \"name\": \"python3\"   },   \"language_info\": {    \"codemirror_mode\": {     \"name\": \"ipython\",     \"version\": 3    },    \"file_extension\": \".py\",    \"mimetype\": \"text/x-python\",    \"name\": \"python\",    \"nbconvert_exporter\": \"python\",    \"pygments_lexer\": \"ipython3\",    \"version\": \"3.9.7\"   }  },  \"nbformat\": 4,  \"nbformat_minor\": 4 }"
  },
  {
    "objectID": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-pandoc",
    "href": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-pandoc",
    "title": "pandoc filters",
    "section": "Minimal ipynb to md converstion with pandoc",
    "text": "Minimal ipynb to md converstion with pandoc\n$ pandoc --to gfm minimal.ipynb\n<div class=\"cell markdown\">\n\n## A minimal notebook\n\n</div>\n\n<div class=\"cell markdown\">\n\n<MyTag></MyTag>\n\n</div>\n\n<div class=\"cell code\" execution_count=\"1\">\n\n``` python\n# Do some arithmetic\nprint(1+1)\n```\n\n<div class=\"output stream stdout\">\n\n    2\n\n</div>\n\n</div>"
  },
  {
    "objectID": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-quarto",
    "href": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-quarto",
    "title": "pandoc filters",
    "section": "Minimal ipynb to md converstion with quarto",
    "text": "Minimal ipynb to md converstion with quarto\n$ quarto render minimal.ipynb --to gfm\npandoc\n  to: gfm+footnotes+tex_math_dollars-yaml_metadata_block\n  output-file: minimal.md\n  standalone: true\n  default-image-extension: png\n  filters:\n    - crossref\n\nOutput created: minimal.md\nThis creates\n\n## A minimal notebook\n\n<MyTag></MyTag>\n\n``` python\n# Do some arithmetic\nprint(1+1)\n```\n\n    2\nRunning Pandoc With those Extensions\nrunning pandoc with --standalone --to gfm+footnotes+tex_math_dollars-yaml_metadata_block still adds the divs and looks different than quarto. Somewhere, maybe quarto is removing the divs. We can see the Div elements in the AST when we explore panflute in the sections below."
  },
  {
    "objectID": "notes/pandoc/index.html#how-to-use-panflute",
    "href": "notes/pandoc/index.html#how-to-use-panflute",
    "title": "pandoc filters",
    "section": "How to use panflute",
    "text": "How to use panflute\nThe examples are helpful.\nThis filter places CodeOutput blocks around code as well as changes the codefence to have file=script.py in order to hack the code fence.\n#!/Users/hamel/opt/anaconda3/bin/python\n#flute.py\nfrom typing import Text\nfrom panflute import *\nfrom logging import warning\n\n\ndef increase_header_level(elem, doc):\n    if type(elem) == CodeBlock and type(elem.parent.prev) == CodeBlock:\n        return ([RawBlock(\"<CodeOutput>\"), elem, RawBlock(\"</CodeOutput>\")])\n    elif type(elem) == CodeBlock:\n        elem.classes = ['file=script.py']\n\n\ndef main(doc=None):\n    return run_filter(increase_header_level, doc=doc)\n\n\nif __name__ == \"__main__\":\n    main()\nThis is how we can use this filter and see the rendered output:\n$ pandoc --to gfm minimal.ipynb --filter \"flute.py\"\n<div class=\"cell markdown\">\n\n## A minimal notebook\n\n</div>\n\n<div class=\"cell markdown\">\n\n<MyTag></MyTag>\n\n</div>\n\n<div class=\"cell code\" execution_count=\"1\">\n\n``` file=script.py\n# Do some arithmetic\nprint(1+1)\n```\n\n<div class=\"output stream stdout\">\n\n<CodeOutput>\n\n    2\n\n</CodeOutput>\n\n</div>\n\n</div>\nNote: we could probably replace the inner div with the output class with <CodeOutput> tag\nJust for completeness, this is the schema of the minimal notebook using the --to native flag prior to applying the filter:\n$pandoc --to native minimal.ipynb\n[ Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Header\n        2\n        ( \"a-minimal-notebook\" , [] , [] )\n        [ Str \"A\" , Space , Str \"minimal\" , Space , Str \"notebook\" ]\n    ]\n, Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Para\n        [ RawInline (Format \"html\") \"<MyTag>\"\n        , RawInline (Format \"html\") \"</MyTag>\"\n        ]\n    ]\n, Div\n    ( \"\"\n    , [ \"cell\" , \"code\" ]\n    , [ ( \"execution_count\" , \"1\" ) ]\n    )\n    [ CodeBlock\n        ( \"\" , [ \"python\" ] , [] )\n        \"# Do some arithmetic\\nprint(1+1)\"\n    , Div\n        ( \"\" , [ \"output\" , \"stream\" , \"stdout\" ] , [] )\n        [ CodeBlock ( \"\" , [] , [] ) \"2\\n\" ]\n    ]\n]\nAnd after applying the filter:\n$pandoc --to native minimal.ipynb --filter flute.py\n[ Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Header\n        2\n        ( \"a-minimal-notebook\" , [] , [] )\n        [ Str \"A\" , Space , Str \"minimal\" , Space , Str \"notebook\" ]\n    ]\n, Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Para\n        [ RawInline (Format \"html\") \"<MyTag>\"\n        , RawInline (Format \"html\") \"</MyTag>\"\n        ]\n    ]\n, Div\n    ( \"\"\n    , [ \"cell\" , \"code\" ]\n    , [ ( \"execution_count\" , \"1\" ) ]\n    )\n    [ CodeBlock\n        ( \"\" , [ \"file=script.py\" ] , [] )\n        \"# Do some arithmetic\\nprint(1+1)\"\n    , Div\n        ( \"\" , [ \"output\" , \"stream\" , \"stdout\" ] , [] )\n        [ RawBlock (Format \"html\") \"<CodeOutput>\"\n        , CodeBlock ( \"\" , [] , [] ) \"2\\n\"\n        , RawBlock (Format \"html\") \"</CodeOutput>\"\n        ]\n    ]\n]"
  },
  {
    "objectID": "notes/docker/index.html",
    "href": "notes/docker/index.html",
    "title": "Docker",
    "section": "",
    "text": "Notes from the book Docker In Action\n;\n\nChapter 1\n\nDocker containers are faster than VMs to start, partly because they do NOT offer any hardware virtualization.\n\nVMs provide hardware abstractions so you can run operating systems.\n\nDocker uses Linux namespaces and cgropus\n\nHamel: I don’t know what this is\n\n\n\n\nChapter 2\n\nGetting help:\n\ndocker help cp\ndocker help run\n\nLinking containers: docker run --link\n\nthis is apparently deprecated per the docs\nOpens a secure tunnel between two containers automatically\nAlso exposes environment variables and other things (see the docs)\n\ndocker cp copy files from a container to local filesystem\nDetach an interactive container:\n\nHold down Control and press P then Q\n\nGet logs docker logs <container name>\n\nHamel: This is like kubectl logs\n\nRun a new command in a running container docker exec\n\ndocker exec <container_name> ps will run the ps command and emit that to stdout\n\nRename a container with docker rename <current_name> <new_name>\ndocker exec run additional processes in an already running container\ndocker create is the same as docker run except that the container is created in a stopped state.\ndocker run --read-only allows you to run a container in a read only state, which you only need to do in special circumstances (you probably never need to use this). You can make exceptions to the read only constraint with the -v flag:\n\n\n\nOverride the entrypoint using the --entrypoint flag (this is discussed in part 2 of the book).\n\n\n\nInjecting environment variables\nWith the --env or -e flags.\nA nice trick to see all the environment variables in a docker container is to use the Unix command env\n\nSetting multiple environment variables: use \\ for multiline like this:\ndocker create \\\n  --env WORDPRESS_DB_HOST=<my database hostname> \\\n  --env WORDPRESS_DB_USER=site_admin \\\n  --env WORDPRESS_DB_PASSWORD=MeowMix42 \\ \nwordpress:4\n\n\nAutomatically restarting containers\nDocker uses an exponential backoff strategy - double the previous time waiting until restarting.\ndocker run -d --restart always ...\nSee these restart policies\n\nno\non-failure[:max-retries]\nalways\nunless-stopped\n\n\n\nRemoving containers vs. images\nContainers are the actual instantiation of an image, just like how an object is an instantion of an instance of a class.\ndocker rm: remove a container docker rmi: remove an image\n\n\n\nChapter 3\n\nTwo ways to publish an image\n\nBuild locally, push image to registry\nMake a Dockerfile and use DockerHub’s build system. This is preferred and considered to be safer, and DockerHub will mark your image as trusted if you do this because it is the only way to provide transparency to what is in your image.\n\nSearch dockerhub by keyword , sorted descending by stars\n\ndocker search <keyword>\nexample: docker search postgres\n\nUsing Alternative registries\n\ndocker pull quay.io/dockerinaction/ch3_hello_registry:latest\n\n\n\nImages as files\nYou can transport, save and load images as files! (You don’t have to push them to a registry).\n\nYou can then load the image:\ndocker load -i myfile.tar\n\n\n\nChapter 4 Persistent Storage &. Shared State with Volumes\n-v and --volume are aliases\n--volumes-from=\"<container-name>\" Mount all volumes from the given container\n\nDifferent kind of Volumes\n\nBind mount - this is what you always use\nDocker managed volume (2 kinds)\n\nAnonymous\nNamed volume (a special case of Anonymous)\n\n\nUse volumes | Docker Documentation - Named vs. Anonymous volumes: article - Hamel: maybe? You might use named volumes to persist data between containers.\n\n\nTo persist data with named volumes\nNamed volume is a kind of anonymous volume where the mount point is managed by Docker. Example of how you used a named volume:\n\nStart container with a named volume: docker run --name myDatabaseWithVolume -v appdata:/var/lib/mysql  mysql save a table in the mysql database\nStart a new container with the same named volume docker run --name myDatabaseWithVolume2 -v appdata:/var/lib/mysql mysql You should be able to see the same table you created in the last container b/c data has been persisted.\n\n\n\nSee where Docker anonymous volumes store information\nUnlike a bind mount, where you explicitly name the host location, docker will manage the storage location of anonymous volumes. But how do you know where the files are stored on the host?\nYou can use docker inspect command filtered for the Volumes key to find the storage location on the host.\nCreate a container with an anonymous volume. docker run -v /some/location --name cass-shared alpine\ndocker inspect -f \"{{json .Volumes}}\" cass-shared\nThis will output a json blob which will show the mount points.\n\n\nOther things you didn’t know about volumes\n\nwhen you mount a volume, it overrides any files already at that location\n\nYou can mount specific files which avoid this\nif you specify a host directory that doesn’t exist Docker will create it for you\n\nexception: If you are mounting a file instead of a directory and it doesn’t exist on the host, Docker will throw an error\n\n\nyou can mount a volume as read only -v /source:/destination:ro\n\nsee docs (there is this optional third argument for volumes)\n\n\n\n\nThe volumes-from flag\nAllows you to share volumes with another container. When you use this flag, the same volumes are mounted into your container at the same location.\n\nVolumes are copied transitively, so this will automatically mount volumes that are also mounted this way from another container.\nCaveats - You cannot mount a shared volume to a different location within a container. This is a limitation of --volumes-from - If you have a collision in the destination mount point among several volumes-from only one volume will survive, which you can ascertain from docker inpsect - see above for how to use docker inspect - You cannot change the write permission of the volume, you inherit whatever the permission is in the source container.\n\n\nCleaning up volumes\n-v flag\ndocker rm -v will delete any managed volumes referenced by the target container\nHowever, if you delete all containers but forget a -v flag you will be left with an orphaned volume. This is bad b/c it takes up disk space until cleaned up. You have to run complicated cleanup steps to get rid of orphans.\nSolution: There is none, its a good habit to use -v anytime you call docker rm\nHamel: this means that- - Don’t use managed volumes unless you really need it - If you do use them, try to include makefiles that include -v as a part of things\n\n\nAdvanced Volume Stuff\n\nYou can have a volume container p. 72 so that you can reference --volume-from from all your containers.\n\nData-paced volume containers, you can pre-load volume containers with data p. 73\nYou can change the behavior of currently running containers by mounting configuration files and application in volumes. In a way, Hamel\n\n\n\n\nChapter 5 Single Host Networking\n\nTerminology:\n\nprotocols: tcp, http\ninterfaces: IP addresses\nports: you know what this means\n\nCustomary ports:\n\nHTTP: 80\nMySQL: 3306\nMemcached: 11211\n\n\n\n\nDiscuss advanced networking and creating a network using the docker network command. Hamel: I don’t see an immediate use for this.\n\nSpecial container networks:\n\nhost\n\ndocker run --network host allows you to pretend like the host is your local machine, and you can expose any port and that will bind to the host.\n\nnone\n\ndocker run --network none closes all connection to the outside world. This is useful for security.\n\n\n\n\nexposing ports\n-p 8080 This binds port 8080 to a random port on the host! you can find the port that the container is bound to by docker port <image name> example: docker run -p 8080 --name listener alpine docker port listener\nThis will give you output that looks like container --> host (which is reverse the other nomenclature of host:container\n-p 8080:8080 this binds the container’s port to the host’s port 8080\n-p 0.0.0.0:8080:8080/tcp same as above but specifies the interface and the tcp protocol.\nSyntax is -p <host-interface>:<host-port>:<target-port>/<protocol>\n\n\n\nChapter 6 Isolation\n\nLimit resources: Memory, CPU,\n\n-m or --memory\n\nnumber, where unit = b, k, m or g\nmemory limits are not reservations, just caps\n\n--cpu-shares\n\nis a weight you set that is used to calculate % of CPU usage allowed\n% is calculated as weight / (sum of all weights)\nonly enforced when there is contention for a CPU\n\n--cpuset-cpus : limits process to a specific CPU\n\ndocker run -d --cpuset-cpus 0 Restricts to CPU number 0\nCan specify a list or 0,1,2 or a range 0-2\n\n--device\n\nmount your webcam: docker run --device /dev/video0:/dev/video0\n\nShared memory : Hamel this was too advanced for me\n\n\n\nRunning as a user\n\nYou can only inspect the default run-as User by creating or pulling the image\n\nsee p. 113\n\nChange run-as user\n\ndocker run --user nobody\nThe user has to exist in the image when doing this or you will get an error. The user will not be created automatically for you.\nSee available users:\n\ndocker run --rm busybox:latest awk -F: '$0=$1' /etc/passwd\n\n\n\n\n\nPrivileged Containers: TRY NOT TO DO THIS\n\nThis is how you run Docker-in-Docker\nPriviliged containers have root privileges on the host.\n\n--privilged on docker create or docker run\n\n\n\n\nChapter 7 packaging software\nAside: cleaning up your docker environment\ndocker image prune -a and docker container prune\n\nRecovering changes to a stopped container\nI always thought you have to commit changes in order to preserve changes to an image you made in a container. This is not true (although committing changes is a good idea).\nAny changes you make to a container is saved even if the container is exited\nTo recover changes to a container\n\nFind the container (if you didn’t name it with docker run --name it will be named for you), using docker ps -a\nStart the container using docker start -ai <container_name> the -ai flags mean to attach and run interactively\nNow you are in the container you can verify that everything you installed is still there!\n\nNote: if you run your container initially with docker run --rm this automatically removes your container upon exit, so this might not be recommended as your changes are not recoverable if you forget to commit\n\n\n\nSeeing changes to a container from the base image\ndocker diff <container name> will output a long list of of file changes: - A: file added - D: file deleted - C: file changed\n\n\nOther tricks\nYou can override the entry point to the container permanently by using the --entrypoint flag: docker run --entrypoint\n\n\nUnderstanding Images & Layers\n\nfiles are stored in a Union file system, so they are stored in specific layers. The file system you are seeing as an end user are a union of all the layers. Each time a change is made to a union file system, that change is recorded on a new layer on top of all of the others. The “union” of all of those layers, or top-down view, is what the container (and user) sees when accessing the file system.\n\nThis means if you are not careful you can bloat the file system by making a bunch of unnecessary changes to add/delete files.\n\ndocker commit commits the top-layer changes to an image, meaning all the files changes are saved.\n\nSee image size with\ndocker images. Even though you remove a file, the image size will increase! This is because of the Union File System\nSee size of all layers\ndocker history <image name>\nflatten an image This is kind of complicated, you can do this by exporting and importing the filesystem into a base image See pg. 140. BUT there is an experimental feature called docker build --squash -t <image> .You can enable experimental features by following these instructions: dockerd Docker Documentation. For Mac, you can turn on experimental features by setting experimental: true in `settings> Command Line > enable experimental\n\n\n\nChapter 8 Build Automation\n\nuse .dockerignore to prevent certain files from being copied\nYou can set multiple environment variables at once in Dockerfile\nYou can use environment variables in the LABEL command\n\nThe metadata makes it clear that the environment variable substitution works. You can use this form of substitution in the ENV, ADD, COPY, WORKDIR, VOLUME, EXPOSE, and USER instructions.\n\n\nENV APPROOT \"/app\" APP \"mailer.sh\" VERSION \"0.6\"\nLABEL base.name \"Mailer Archetype\" base.version \"${VERSION}\"\n\nview metadata using the command docker inspect <image name>\n\n\nENTRYPOINT something arugment vs. ENTRYPOINT [“something”, “argument”]\nTLDR; use the ugly list approach\nThere are two instruction forms shell form and exec form docker - Dockerfile CMD shell versus exec form - Stack Overflow\nThe ENTRYPOINT instruction has two forms: the shell form and an exec form. The shell form looks like a shell command with whitespace-delimited arguments. The exec form is a string array where the first value is the command to execute and the remaining values are arguments. .\nMost importantly, if the shell form is used for ENTRYPOINT, then all other arguments provided by the CMD instruction or at runtime as extra arguments to docker run will be ignored. This makes the shell form of ENTRYPOINT less flexible.\nOther commands can use the exec form too! You must use the exec form when any of the arguments contain a whitespace:\nFROM dockerinaction/mailer-base:0.6 \nCOPY [\"./log-impl\", \"${APPROOT}\"] \nRUN chmod a+x ${APPROOT}/${APP} && \\ chown example:example /var/log \nUSER example:example \nVOLUME [\"/var/log\"]  # each value in this array will be created as a new volume definition\nCMD [\"/var/log/mailer.log\"]\nNote: you usually don’t want to specify a volume at build time.\n\n\nCMD vs. ENTRYPOINT (You should really try to always use both!)\nCMD is actually an argument list for the ENTRYPOINT.\n\nLogically when you run a container it runs as <default shell program> ENTRYPOINT CMD\nYou can override the ENTRYPOINT with docker run --entrypoint, and you can override commands by just passing commands to docker run : docker run <image name> <command>\n\nFROM ubuntu\n\nENTRYPOINT [ \"ls\" ]\nCMD [\"-lah\"]\nAs you can see using ENTRYPOINT as well as CMD separately provides your downstream users with the most flexibility.\n\n\nCOPY vs ADD\nUse COPY. ADD has additional functionality like ability to download from urls and decompress files, which proved opaque over time and you shouldn’t use it.\n\n\nONBUILD\nThe ONBUILD instruction defines instructions to execute if the resulting image is used as a base for another build. those ONBUILD instructions are executed after the FROM instruction and before the next instruction in a Dockerfile.\nFROM busybox:latest \nWORKDIR /app RUN touch /app/base-evidence \nONBUILD RUN ls -al /app\n\n\nOther Stuff\n\nYou should always validate the presence of required environment variables in a startup shell script like entrypoint.sh\n\n\n\nDocker Digests\nReference the exact SHA of a Container which is the only way to guarantee the image you are referencing has not changed. @ symbol followed by the digest.\nHamel: doesn’t look like a good way to find history of digests, but you can see the current SHA when you use docker pull , you can see the SHA as well if you call docker images --digests\nFROM debian@sha256:d5e87cfcb730...\n\n\n\nChapter 10 (skipped Ch 9)\n\nYou can run your own customized registry. Simplest version can be hosted from a Docker Container!\n\n# start a local registry on port 5000\ndocker run -d --name personal_registry\n \\ -p 5000:5000 --restart=always \n \\ registry:2\n\n# push an image to the registry (using the same image that created the registry for convenience)\ndocker tag registry:2 localhost:5000/distribution:2 \ndocker push localhost:5000/distribution:2\nNote that docker push syntax is actually docker push <registry url>/org/repo\nThis chapter discusses many more things which are skipped: - Centralized registries - Enhancements - Durable blog storage - Integrating through notifications\n\n\nChapter 11 Docker Compose\nDocker compose for fastpages:\nversion: \"3\"\nservices:\n  fastpages: &fastpages\n    working_dir: /data\n    environment:\n        - INPUT_BOOL_SAVE_MARKDOWN=false\n    build:\n      context: ./_action_files\n      dockerfile: ./Dockerfile\n    image: fastpages-dev\n    logging:\n      driver: json-file\n      options:\n        max-size: 50m\n    stdin_open: true\n    tty: true\n    volumes:\n      - .:/data/\n\n  converter:\n    <<: *fastpages\n    command: /fastpages/action_entrypoint.sh\n\n  watcher:\n    <<: *fastpages\n    command: watchmedo shell-command --command /fastpages/action_entrypoint.sh --pattern *.ipynb --recursive --drop\n\n  jekyll:\n    working_dir: /data\n    image: hamelsmu/fastpages-jekyll\n    restart: unless-stopped\n    ports:\n      - \"4000:4000\"\n    volumes:\n      - .:/data/\n    command: >\n     bash -c \"gem install bundler\n     && jekyll serve --trace --strict_front_matter\"\nThe above uses YAML anchors: YAML anchors - Atlassian Documentation\nStart a particular service: docker-compose up <service name> Rebuild a service docker-compose build <service name>\nYou can express dependencies with depends_on which is useful for compose to know which services to restart or start in a specified order.\nSee examples of Docker Compose files on p 243\n\nScaling Up w/Docker Compose\nThat’s right you don’t need docker swarm. This example uses ch11_coffee_api/docker-compose.yml at master · dockerinaction/ch11_coffee_api · GitHub\n\nGet list of containers that are currently providing the service.\n\ndocker-compose ps coffee\n          Name                 Command       State            Ports\n----------------------------------------------------------------------------\nch11_coffee_api_coffee_1   ./entrypoint.sh   Up      0.0.0.0:32768->3000/tcp\n\nScale it up with docker-compose up --scale\n\ndocker-compose up --scale coffee=5\nWhen you run docker-compose ps coffee:\ndocker-compose ps coffee                                                                                                                         ✔\n          Name                 Command       State            Ports\n----------------------------------------------------------------------------\nch11_coffee_api_coffee_1   ./entrypoint.sh   Up      0.0.0.0:32768->3000/tcp\nch11_coffee_api_coffee_2   ./entrypoint.sh   Up      0.0.0.0:32769->3000/tcp\nch11_coffee_api_coffee_3   ./entrypoint.sh   Up      0.0.0.0:32771->3000/tcp\nch11_coffee_api_coffee_4   ./entrypoint.sh   Up      0.0.0.0:32770->3000/tcp\nch11_coffee_api_coffee_5   ./entrypoint.sh   Up      0.0.0.0:32772->3000/tcp\nNote that the coffee service binds to port 0 on your host, which is an ephemeral port, which just means that your host machine assigns the service to a random port. This is required if you plan on using docker compose up --scale\nThe service was bound to port 0 on the host with\ncoffee:\n  build: ./coffee\n  user: 777:777\n  restart: always\n  expose:\n    - 3000\n  ports:\n    - \"0:3000\"\n...\n\nLoad balancer\n\nProblem with this kind of scaling is you don’t know the ports in advance , and you don’t want to hit these individual endpoints, you need a load balancer. This blog post shows you how to luse NGINX as a load balancer.\nYou will need something like this in your compose file\n  nginx:\n    image: nginx:latest\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - pspdfkit\n    ports:\n      - \"4000:4000\"\n\n\nTemplating Docker Compose Files\nYou can read about this here: Share Compose configurations between files and projects | Docker Documentation, allows you to override certain things from a base compose file.\n\n\n\nChapter 12 Clusters w/Machine & Swarm\nHamel: I skipped this completely"
  },
  {
    "objectID": "notes/serving/fastapi/index.html",
    "href": "notes/serving/fastapi/index.html",
    "title": "FastAPI",
    "section": "",
    "text": "FastAPI is a web framework for Python. People like to use this framework for serving prototypes of ML models."
  },
  {
    "objectID": "notes/serving/fastapi/index.html#impressions",
    "href": "notes/serving/fastapi/index.html#impressions",
    "title": "FastAPI",
    "section": "Impressions",
    "text": "Impressions\n\nModel serving frameworks (TF Serving, TorchServe, etc) are probably the way to go for production / enterprise deployments, especially for larger models. They offer more features, and latency will be more predictable (even if slower). I think that for smaller models (< 200MB) FastAPI is fine.\nIt is super easy to get started with FastAPI.\nI was able to confirm Sayak’s Benchmark where FastAPI is faster than TF Serving, but also less consistent overall. FastAPI is also more likely to fail, although I haven’t been able to cause that. In my experiments FastAPI was much faster for this small model, but this could change with larger models.\nMemory is consumed linearly as you increase the number of Uvicorn workers. Model serving frameworks like TF-Serving seem to work more efficiently. You should be careful to set the environment variable TF_FORCE_GPU_ALLOW_GROWTH=true if you are running inference on GPUs. I think in many cases you would be doing inference on CPUs, so this might not be relevant most of the time.\nFastAPI seems like it could be really nice on smaller models and scoped hardware where there is only one worker per node and you load balance across nodes (because you aren’t replicating the model with each worker).\nDebugging FastAPI is amazing, as its pure python and you get a nice docs page at http://<IP>/docs that lets you test out your endpoints right on the page! The documentation for FastPI is also amazing.\nIf you want the request parameters to be sent in the body (as you often do with ML b/c you want to send data to be scored), you have to use Pydantic. This is very opinionated, but easy enough to use."
  },
  {
    "objectID": "notes/serving/fastapi/index.html#load-model-make-predictions",
    "href": "notes/serving/fastapi/index.html#load-model-make-predictions",
    "title": "FastAPI",
    "section": "Load Model & Make Predictions",
    "text": "Load Model & Make Predictions\nGoing to use the model trained in the TF Serving tutorial. Furthermore, we are going to load this from the SavedModel format.\n\n# this cell is exported to a script\n\nfrom fastapi import FastAPI, status\nfrom pydantic import BaseModel\nfrom typing import List\nimport tensorflow as tf\nimport numpy as np\n\ndef load_model(model_path='/home/hamel/hamel/notes/serving/tfserving/model/1'):\n    \"Load the SavedModel Object.\"\n    sm = tf.saved_model.load(model_path)\n    return sm.signatures[\"serving_default\"] # this is the default signature when you save a model\n\n\n# this cell is exported to a script\n\ndef pred(model: tf.saved_model, data:np.ndarray, pred_layer_nm='dense_3'):\n    \"\"\"\n    Make a prediction from a SavedModel Object.  `pred_layer_nm` is the last layer that emits logits.\n    \n    https://www.tensorflow.org/guide/saved_model\n    \"\"\"\n    data = tf.convert_to_tensor(data, dtype='int32')\n    preds = model(data)\n    return preds[pred_layer_nm].numpy().tolist()\n\n\nTest Data\n\n_, (x_val, _) = tf.keras.datasets.imdb.load_data(num_words=20000)\nx_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=200)[:2, :]\n\n\n\nMake a prediction\n\nmodel = load_model()\npred(model, x_val[:2, :])\n\n[[0.8761785626411438, 0.12382148206233978],\n [0.0009457750129513443, 0.9990542531013489]]"
  },
  {
    "objectID": "notes/serving/fastapi/index.html#build-the-fastapi-app",
    "href": "notes/serving/fastapi/index.html#build-the-fastapi-app",
    "title": "FastAPI",
    "section": "Build The FastApi App",
    "text": "Build The FastApi App\n\n# this cell is exported to a script\n\napp = FastAPI()\n\nitems = {}\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"Load the model on startup https://fastapi.tiangolo.com/advanced/events/\"\n    items['model'] = load_model()\n\n\n@app.get(\"/\")\ndef health(status_code=status.HTTP_200_OK):\n    \"A health-check endpoint\"\n    return 'Ok'\n\nWe want to send the data for prediction in the Request Body (not with path parameters). According the docs:\n\nFastAPI will recognize that the function parameters that match path parameters should be taken from the path, and that function parameters that are declared to be Pydantic models should be taken from the request body.\n\n\n# this cell is exported to a script\n\nclass Sentence(BaseModel):\n    tokens: List[List[int]]\n\n@app.post(\"/predict\")\ndef predict(data:Sentence, status_code=status.HTTP_200_OK):\n    preds = pred(items['model'], data.tokens)\n    return preds"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html",
    "href": "notes/serving/tfserving/gpu.html",
    "title": "GPUs & Batching",
    "section": "",
    "text": "Warning\n\n\n\nI was not able to simulate a situation where dynamic batching is better than not batching. Apparently it can take time and lots of experiments to get right. Follow this guide for more information. This is a topic I may revisit in the future.\n\n\nAccording to the docs:\n\nModel Server has the ability to batch requests in a variety of settings in order to realize better throughput. The scheduling for this batching is done globally for all models and model versions on the server to ensure the best possible utilization of the underlying resources no matter how many models or model versions are currently being served by the server. You can enable this by using the --enable_batching flag and control it with the --batching_parameters_file.\n\nThis is an example batching parameters file:\n\n%%writefile batch-config.cfg\nmax_batch_size { value: 1000 }\nbatch_timeout_micros { value: 1000 }\nmax_enqueued_batches { value: 16 }\nnum_batch_threads { value: 16 }\n\nOverwriting batch-config.cfg\n\n\n\n\n\n\n\n\nGuidance on batch configuration\n\n\n\nGuidance for these config files are here there is no “right answer”. For GPUs, the guidance is this:\nGPU: One Approach\nIf your model uses a GPU device for part or all of your its inference work, consider the following approach:\n\nSet num_batch_threads to the number of CPU cores.\nTemporarily set batch_timeout_micros to a really high value while you tune max_batch_size to achieve the desired balance between throughput and average latency. Consider values in the hundreds or thousands.\nFor online serving, tune batch_timeout_micros to rein in tail latency. The idea is that batches normally get filled to max_batch_size, but occasionally when there is a lapse in incoming requests, to avoid introducing a latency spike it makes sense to process whatever’s in the queue even if it represents an underfull batch. The best value for batch_timeout_micros is typically a few milliseconds, and depends on your context and goals. Zero is a value to consider; it works well for some workloads. (For bulk processing jobs, choose a large value, perhaps a few seconds, to ensure good throughput but not wait too long for the final (and likely underfull) batch.)"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#test-the-server",
    "href": "notes/serving/tfserving/gpu.html#test-the-server",
    "title": "GPUs & Batching",
    "section": "Test the server",
    "text": "Test the server\nThe model we are going to serve is generated in this note.\nI’m going to start two TF Serving instances, one thats regular CPU and one that does batching on GPU. I’m running both commands from the /home/hamel/tf-serving/ directory.\n\nCPU Version\ndocker run \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model/,target=/models/model \\\n--net=host -t tensorflow/serving --grpc_max_threads=1000\n\n\n\n\n\n\nNote\n\n\n\n--net=host binds all ports to the host, which is convenient for testing\n\n\nTest the CPU version:\n\n! curl http://localhost:8501/v1/models/model\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"1\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\n\n\nGPU Version\n\n\nPre-requisites\nYou must install nvidia-docker first\n\n\nDocker Command\nYou can pass additional arguments like --enable_batching to the docker run ... command just like you would if you were running tfserving locally.\nNote that we need the --gpus all flag to enable GPUs with nvidia-Docker. Furthermore, use the latest-gpu tag to enable GPUs as well as the --port and --rest_api_port so that it doesn’t conflict with the other tf serving instance I have running:\ndocker run --gpus all \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models \\\n--net=host -t tensorflow/serving:latest-gpu --enable_batching \\\n--batching_parameters_file=/models/batch-config.cfg --port=8505 \\\n--rest_api_port=8506 --grpc_max_threads=1000\n\n\n\n\n\n\n--grpc_max_threads flag\n\n\n\nI found that in non-batch mode I can easily overwhelm the server with gRPC requests. I wasn’t able to overwhelm the server over REST. Setting --grpc_max_threads=1000 takes care of this.\n\n\n\n\n\n\n\n\nOther flags\n\n\n\nThere are lots of flags. Hannes uses these additional ones, and they seem to make things a bit faster.\n--enable_model_warmup  \\\n--tensorflow_intra_op_parallelism=4 \\\n--tensorflow_inter_op_parallelism=4\n\n\n\n\n\n\n\n\nUnderstanding the volume mount\n\n\n\nOn the host, the config file is located at /home/hamel/hamel/notes/serving/tfserving/batch-config.cfg and the model is located at /home/hamel/hamel/notes/serving/tfserving/model/\nThe Docker file will try to import the model like this:\n# Set where models should be stored in the container\nENV MODEL_BASE_PATH=/models\nRUN mkdir -p ${MODEL_BASE_PATH}\n\n# The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=model\n\n# Create a script that runs the model server so we can use environment variables\n# while also passing in arguments from the docker command line\nRUN echo '#!/bin/bash \\n\\n\\\ntensorflow_model_server --port=8500 --rest_api_port=8501 \\\n--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n&& chmod +x /usr/bin/tf_serving_entrypoint.sh\nBy default it will try to get models from ${MODEL_BASE_PATH}/${MODEL_NAME} which is /models/model. So when we mount /home/hamel/hamel/notes/serving/tfserving from the host to /models in the container.\nIn the container:\n\nThe model files will be available at models/model as expected\nThe config file will be available at models/batch-config.cfg\n\n\n\nTest the TF-Serving GPU api:\n\n! curl http://localhost:8506/v1/models/model\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"1\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#prepare-the-data",
    "href": "notes/serving/tfserving/gpu.html#prepare-the-data",
    "title": "GPUs & Batching",
    "section": "Prepare the data",
    "text": "Prepare the data\n\nfrom tensorflow import keras\n\nvocab_size = 20000  # Only consider the top 20k words\nmaxlen = 200  # Only consider the first 200 words of each movie review\n\n_, (x_val, _) = keras.datasets.imdb.load_data(num_words=vocab_size)\nx_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n\nsample_data = x_val[:5, :]\ndata = [sample_data] * 10000"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#the-prediction-code",
    "href": "notes/serving/tfserving/gpu.html#the-prediction-code",
    "title": "GPUs & Batching",
    "section": "The prediction code",
    "text": "The prediction code\n\nimport json, requests\nimport numpy as np\n\nfrom fastcore.parallel import parallel\nfrom functools import partial\nparallel_pred = partial(parallel, threadpool=True, n_workers=500)\n\n\ndef predict_rest(data, port):\n    json_data = json.dumps(\n    {\"signature_name\": \"serving_default\", \"instances\": data.tolist()}\n    )\n    url = f\"http://localhost:{port}/v1/models/model:predict\"\n\n    json_response = requests.post(url, data=json_data)\n    response = json.loads(json_response.text)\n    rest_outputs = np.array(response[\"predictions\"])\n    return rest_outputs\n\n\nrest_outputs = predict_rest(sample_data, '8501')\nrest_outputs\n\narray([[0.89650154, 0.10349847],\n       [0.00330466, 0.9966954 ],\n       [0.13089457, 0.8691054 ],\n       [0.49083445, 0.50916553],\n       [0.0377177 , 0.96228224]])\n\n\n\ngRPC\nThis is the code that will be used to make gRPC prediction requests. For more discussion about gRPC, see this note\n\nimport grpc\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n# Create a channel that will be connected to the gRPC port of the container\n\n\n\ndef predict_grpc(data, input_name='input_1', port='8505'):\n    \n    options = [('grpc.max_receive_message_length', 100 * 1024 * 1024)]\n    channel = grpc.insecure_channel(f\"localhost:{port}\", options=options) # the gRPC port for the GPU server was set at 8505\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    # Create a gRPC request made for prediction\n    request = predict_pb2.PredictRequest()\n\n    # Set the name of the model, for this use case it is \"model\"\n    request.model_spec.name = \"model\"\n\n    # Set which signature is used to format the gRPC query\n    # here the default one \"serving_default\"\n    request.model_spec.signature_name = \"serving_default\"\n\n    # Set the input as the data\n    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data))\n\n    # Send the gRPC request to the TF Server\n    result = stub.Predict(request)\n    return result"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#cpu-server",
    "href": "notes/serving/tfserving/gpu.html#cpu-server",
    "title": "GPUs & Batching",
    "section": "CPU Server",
    "text": "CPU Server\nThe CPU server is running on port 8501.\n\nREST CPU\nThe REST API endpoint on the CPU-bound server.\n\ncpu_pred = partial(predict_rest, port = '8501')\n\n\n%%time\nresults = parallel_pred(cpu_pred, data)\n\nCPU times: user 27.7 s, sys: 5.56 s, total: 33.3 s\nWall time: 26 s\n\n\n\n\ngrpc CPU\nThis is using the same CPU-bound TF Serving server, but is hitting the gRPC endpoint.\n\npredict_grpc_cpu = partial(predict_grpc, port='8500')\n\n\n%%time\nresults = parallel_pred(predict_grpc_cpu, data)\n\nCPU times: user 7.5 s, sys: 2.33 s, total: 9.84 s\nWall time: 7.63 s"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#gpu-server-with-batching",
    "href": "notes/serving/tfserving/gpu.html#gpu-server-with-batching",
    "title": "GPUs & Batching",
    "section": "GPU Server with batching",
    "text": "GPU Server with batching\nThe GPU server is running on port 8506 (we already started it above).\n\nREST\n\ngpu_pred = partial(predict_rest, port = '8506')\n\n\n%%time\nresults = parallel_pred(gpu_pred, data)\n\nCPU times: user 27.1 s, sys: 3.44 s, total: 30.6 s\nWall time: 27 s\n\n\n\n\ngRPC with batch\nThis is much faster than the REST endpoint! This is also much faster than the CPU version on this specific example. However, the batching part doesn’t appear to be providing any speedup at all, because the non-batch gRPC version is almost the same speed (if not a little bit faster).\n\n%%time\nresult = parallel(predict_grpc, data)\n\nCPU times: user 2.71 s, sys: 551 ms, total: 3.26 s\nWall time: 6.6 s"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#gpu-server-without-batching",
    "href": "notes/serving/tfserving/gpu.html#gpu-server-without-batching",
    "title": "GPUs & Batching",
    "section": "GPU server without batching",
    "text": "GPU server without batching\ndocker run --gpus all --mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models --net=host -t tensorflow/serving:latest-gpu --port=8507 --rest_api_port=8508\n\nREST\n\ngpu_pred_no_batch = partial(predict_rest, port = '8508')\n\n\n%%time\nresults = parallel_pred(gpu_pred_no_batch, data)\n\nCPU times: user 26.9 s, sys: 3.61 s, total: 30.5 s\nWall time: 25.7 s\n\n\n\n\ngRPC without batching\nWhen I initially did this I got an error that said “Resources Exhausted”. I was able to solve this by increasing the threads with the flag --grpc_max_threads=1000 when running the Docker container.\n\npredict_grpc_no_batch = partial(predict_grpc, port='8507')\n\n\n%%time\nresult = parallel_pred(predict_grpc_no_batch, data)\n\nCPU times: user 5.06 s, sys: 1.42 s, total: 6.48 s\nWall time: 6.65 s"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html",
    "href": "notes/serving/tfserving/tf-serving-basics.html",
    "title": "Basics",
    "section": "",
    "text": "These notes use code from here and this tutorial on tf serving."
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#create-the-model",
    "href": "notes/serving/tfserving/tf-serving-basics.html#create-the-model",
    "title": "Basics",
    "section": "Create The Model",
    "text": "Create The Model\n\n\n\n\n\n\nNote\n\n\n\nI didn’t want to use an existing model file from a tfserving tutorial, so I’m creating a new model from scratch.\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nfrom train import get_model\n\nvocab_size = 20000  # Only consider the top 20k words\nmaxlen = 200  # Only consider the first 200 words of each movie review\nembed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\n\n(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\nprint(len(x_train), \"Training sequences\")\nprint(len(x_val), \"Validation sequences\")\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n\n25000 Training sequences\n25000 Validation sequences\n\n\n\n\n\n\n\n\nImportant\n\n\n\nget_model is defined here\n\n\n\nmodel = get_model(maxlen=maxlen, vocab_size=vocab_size, \n                  embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should be careful to specify dtype properly for the input layer, so that the tfserving api validation will work properly. Like this:\ninputs = layers.Input(shape=(maxlen,), dtype='int32')\n\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 200)]             0         \n                                                                 \n token_and_position_embeddin  (None, 200, 32)          646400    \n g (TokenAndPositionEmbeddin                                     \n g)                                                              \n                                                                 \n transformer_block (Transfor  (None, 200, 32)          10656     \n merBlock)                                                       \n                                                                 \n global_average_pooling1d (G  (None, 32)               0         \n lobalAveragePooling1D)                                          \n                                                                 \n dropout_2 (Dropout)         (None, 32)                0         \n                                                                 \n dense_2 (Dense)             (None, 20)                660       \n                                                                 \n dropout_3 (Dropout)         (None, 20)                0         \n                                                                 \n dense_3 (Dense)             (None, 2)                 42        \n                                                                 \n=================================================================\nTotal params: 657,758\nTrainable params: 657,758\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nTrain Model\n\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n)\n\nEpoch 1/2\n782/782 [==============================] - 49s 58ms/step - loss: 0.3977 - accuracy: 0.8056 - val_loss: 0.2856 - val_accuracy: 0.8767\nEpoch 2/2\n782/782 [==============================] - 19s 24ms/step - loss: 0.1962 - accuracy: 0.9258 - val_loss: 0.3261 - val_accuracy: 0.8608\n\n\n\n\nSave Model\nYou can serialize your tensorflow models to a SavedModel format using tf.saved_model.save(...). This format is documented here. We are saving two versions of the model in order to discuss features of how TF Serving can serve multiple model versions.\n\n!rm -rf ./model\n\n\ndef save_model(model_version, model_dir=\"./model\"):\n\n    model_export_path = f\"{model_dir}/{model_version}\"\n\n    tf.saved_model.save(\n        model,\n        export_dir=model_export_path,\n    )\n\n    print(f\"SavedModel files: {os.listdir(model_export_path)}\")\n\nsave_model(model_version=1)\nsave_model(model_version=2)\n\nWARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ./model/1/assets\n\n\nINFO:tensorflow:Assets written to: ./model/1/assets\n\n\nSavedModel files: ['fingerprint.pb', 'variables', 'assets', 'saved_model.pb']\n\n\nWARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ./model/2/assets\n\n\nINFO:tensorflow:Assets written to: ./model/2/assets\n\n\nSavedModel files: ['fingerprint.pb', 'variables', 'assets', 'saved_model.pb']\n\n\nModel versioning is done by saving your model into a directory with an integer. By default, the directory with the highest integer will be served. You can change this with config files.\n\n!ls model/\n\n1  2"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#validate-the-api-schema",
    "href": "notes/serving/tfserving/tf-serving-basics.html#validate-the-api-schema",
    "title": "Basics",
    "section": "Validate the API Schema",
    "text": "Validate the API Schema\nThe output of the below command will show the input schema and shape, as well as the output shape of the API we will create with tfserving.\nThie below flags are mostly boilerplate. I don’t know what signature really means just yet.\n\n!saved_model_cli show --dir ./model/2 --tag_set serve --signature_def serving_default\n\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['input_1'] tensor_info:\n      dtype: DT_INT32\n      shape: (-1, 200)\n      name: serving_default_input_1:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['dense_3'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 2)\n      name: StatefulPartitionedCall:0\nMethod name is: tensorflow/serving/predict"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#launch-the-docker-container",
    "href": "notes/serving/tfserving/tf-serving-basics.html#launch-the-docker-container",
    "title": "Basics",
    "section": "Launch the docker container",
    "text": "Launch the docker container\nThe TFServing docs really want you to use docker. But you can use the CLI tensorflow_model_server instead, which is what is packaged in the Docker container. This is what their docs say:\n\nThe easiest and most straight-forward way of using TensorFlow Serving is with Docker images. We highly recommend this route unless you have specific needs that are not addressed by running in a container.\n\n\nTIP: This is also the easiest way to get TensorFlow Serving working with GPU support.\n\nIt worth looking at The Dockerfile for TFServing:\nENV MODEL_BASE_PATH=/models\nRUN mkdir -p ${MODEL_BASE_PATH}\n\n# The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=model\n\n\nRUN echo '#!/bin/bash \\n\\n\\\ntensorflow_model_server --port=8500 --rest_api_port=8501 \\\n--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n\nthis means that it is looking in /models/model by default. We can consider this when mounting the local model directory into the container.\nSuppose my local model is located at /home/hamel/hamel/notes/serving/tfserving/model. This is how you would run the Docker container:\ndocker run -p 8500:8500 \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model,target=/models/model \\\n--net=host -t tensorflow/serving\n\nTFServing on a GPU\nSee the note on using GPUs in TF Serving.\nHowever, it probably only makes sense to enable the GPU if you are going to enable batching, or if a single prediction are GPU intensive (like Stable Diffusion)"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#testing-the-api",
    "href": "notes/serving/tfserving/tf-serving-basics.html#testing-the-api",
    "title": "Basics",
    "section": "Testing the API",
    "text": "Testing the API\nAccording to the documentation we can see the status of our model like this:\nGET http://host:port/v1/models/${MODEL_NAME}, which for us is:\ncurl https://localhost:8501/v1/models/model\n\n! curl http://localhost:8501/v1/models/model\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"2\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\nNote how this shows the highest version number by default. You can access different model versions through different endpoints and supplying the right config files."
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#model-versioning",
    "href": "notes/serving/tfserving/tf-serving-basics.html#model-versioning",
    "title": "Basics",
    "section": "Model Versioning",
    "text": "Model Versioning\nModels that you save into the directory have a version number, for example our model is saved at home/hamel/hamel/notes/serving/tfserving/model with directories with versions 1 and 2.\n\n!ls /home/hamel/hamel/notes/serving/tfserving/model\n\n1  2\n\n\nBy default, TF Serving will always serve the model with the highest version number. However, you can change that with a model server config. You can also serve multiple versions of a model, add labels to models, etc. This is probably one of the most useful aspects of TF Serving. Here are some configs that allow you to serve multiple versions at the same time:\n\n%%writefile ./model/models.config\n\n\nmodel_config_list {\n config {\n    name: 'model'\n    base_path: '/models/model/'\n    model_platform: 'tensorflow'\n    model_version_policy: {all: {}}\n        }\n}\n\nOverwriting ./model/models.config\n\n\nIf you wanted to specify specific models to serve, you could name the versions instead of specifying all like this:\n\n%%writefile ./model/models-specific.config\n\nmodel_config_list {\n config {\n    name: 'model'\n    base_path: '/models/model/'\n    model_platform: 'tensorflow'\n    model_version_policy {\n      specific {\n        versions: 1\n        versions: 2\n      }\n    }\n  }\n}\n\nOverwriting ./model/models-specific.config\n\n\nTo read the config files, we need to pass these additional flags when running the container:\ndocker run \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model,target=/models/model \\\n--net=host \\\n-t tensorflow/serving \\\n--model_config_file=/models/model/models-specific.config \\\n--model_config_file_poll_wait_seconds=60 \nThe flag --model_config_file_poll_wait_seconds=60 tells the server to check for a new config file at the path every 60 seconds. This is optional but likely a good idea so you can change your config file without rebooting the server.\nTo access a specific version of the model, you would make a request to\nhttp://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict. For example, for version 1 the endpoint would be http://localhost:8501/v1/models/model/versions/1:predict.\nIf you did not care about the version, and just wanted the highest version we can use the general endpoint without the version which will serve the highest version by default:\nhttp://localhost:8501/v1/models/model:predict\nWe can test that all of these version is avialable to serve like so:\n\n! curl http://localhost:8501/v1/models/model/versions/2\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"2\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\n\n! curl http://localhost:8501/v1/models/model/versions/1\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"1\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTF Serving doesn’t make all versions available by default, only the latest one (with the highest number). You have to supply a config file if you want multiple versions to be made available at once. You probably should use labels to make URLs consistent in production scenarios."
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#rest",
    "href": "notes/serving/tfserving/tf-serving-basics.html#rest",
    "title": "Basics",
    "section": "REST",
    "text": "REST\nTime to make a prediction request. We will first try the REST API, which says the api endpoint is as follows: Note that v1 is just a hardcoded thing that has to do with the version of tfServing, not the version of the model:\nPOST http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict\n\nimport json, requests\nimport numpy as np\n\nsample_data = x_val[:2, :]\n\ndata = json.dumps(\n    {\"signature_name\": \"serving_default\", \"instances\": sample_data.tolist()}\n)\nurl = \"http://localhost:8501/v1/models/model:predict\" # this would be \"http://localhost:8501/v1/models/model/versions/1:predict\" for version 1\n\n\ndef predict_rest(json_data, url):\n    json_response = requests.post(url, data=json_data)\n    response = json.loads(json_response.text)\n    rest_outputs = np.array(response[\"predictions\"])\n    return rest_outputs\n\nrest_outputs = predict_rest(data, url)\n\n\nrest_outputs\n\narray([[0.94086391, 0.05913605],\n       [0.00317052, 0.99682945]])\n\n\n\nmodel_outputs = model.predict(sample_data)\n\n1/1 [==============================] - 0s 210ms/step\n\n\nLet’s compare this to our model’s output. It’s close enough :)\n\nassert np.allclose(rest_outputs, model_outputs, rtol=1e-4)"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#grpc",
    "href": "notes/serving/tfserving/tf-serving-basics.html#grpc",
    "title": "Basics",
    "section": "gRPC",
    "text": "gRPC\n\nThe payload format for grpc uses Protocol Buffers which are compressed better than JSON, which might make latency lower. This makes a difference for higher payload sizes, like images.\n\ngRPC has some kind of bi-directional streaming whereas REST is just a response/request model. I don’t know what this means.\ngRPC uses a newer HTTP protocol than REST. I don’t know what this means.\n\n\nimport grpc\n\n# Create a channel that will be connected to the gRPC port of the container\nchannel = grpc.insecure_channel(\"localhost:8500\")\n\n\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\nstub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n\n\n# Get the serving_input key\nloaded_model = tf.saved_model.load(model_export_path)\ninput_name = list(\n    loaded_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\n)[0]\n\n\ninput_name\n\n'input_1'\n\n\n\ndef predict_grpc(data, input_name, stub):\n    # Create a gRPC request made for prediction\n    request = predict_pb2.PredictRequest()\n\n    # Set the name of the model, for this use case it is \"model\"\n    request.model_spec.name = \"model\"\n\n    # Set which signature is used to format the gRPC query\n    # here the default one \"serving_default\"\n    request.model_spec.signature_name = \"serving_default\"\n\n    # Set the input as the data\n    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data))\n\n    # Send the gRPC request to the TF Server\n    result = stub.Predict(request)\n    return result\n\nsample_data = tf.convert_to_tensor(x_val[:2, :], dtype='int32')\n\ngrpc_outputs = predict_grpc(sample_data, input_name, stub)\n\n\nInspect the gRPC response\nWe can see all the fields that the gRPC response has. In this situation, the name of the final layer of our model will be the key that containst the predictions, which is dense_3 in this case.\n\ngrpc_outputs\n\noutputs {\n  key: \"dense_3\"\n  value {\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: 2\n      }\n      dim {\n        size: 2\n      }\n    }\n    float_val: 0.9408639073371887\n    float_val: 0.059136051684617996\n    float_val: 0.0031705177389085293\n    float_val: 0.9968294501304626\n  }\n}\nmodel_spec {\n  name: \"model\"\n  version {\n    value: 2\n  }\n  signature_name: \"serving_default\"\n}\n\n\nWe can also get the name of the last layer of the model like this:\n\nloaded_model.signatures[\"serving_default\"].structured_outputs\n\n{'dense_3': TensorSpec(shape=(None, 2), dtype=tf.float32, name='dense_3')}\n\n\n\n\nReshaping the Response\n\nshape = [x.size for x in grpc_outputs.outputs['dense_3'].tensor_shape.dim]\n\ngrpc_preds = np.reshape(grpc_outputs.outputs['dense_3'].float_val, shape)\ngrpc_preds\n\narray([[0.94086391, 0.05913605],\n       [0.00317052, 0.99682945]])\n\n\nThe predictions are close enough. I am not sure why they wouldn’t be exactly the same.\n\nassert np.allclose(model_outputs, grpc_preds,rtol=1e-4)"
  },
  {
    "objectID": "notes/serving/torchserve/basic-torchserve.html",
    "href": "notes/serving/torchserve/basic-torchserve.html",
    "title": "Basics",
    "section": "",
    "text": "The key to understanding TorchServe is to first understand torch-model-archiver which packages model artifacts into a single model archive file (.mar). torch-model-archive needs the following inputs:\n\n\nNeed a model checkpoint file\n\n\n\nNeed a model definition file and a state_dict file.\n\n\n\nThe CLI produces a .mar file. Below is an example of archiving an eager mode model.\n\n!torch-model-archiver --model-name densenet161 \\\n    --version 1.0 \\\n    --model-file ./_serve/examples/image_classifier/densenet_161/model.py \\\n    --serialized-file densenet161-8d451a50.pth \\\n    --export-path model_store \\\n    --extra-files ./_serve/examples/image_classifier/index_to_name.json \\\n    --handler image_classifier \\\n    -f\n\nWARNING - Overwriting model_store/densenet161.mar ...\n\n\nThis is the model file:\n\n\n_serve/examples/image_classifier/densenet_161/model.py\n\n\n\nOptions for model archiver:\n\n! torch-model-archiver --help\n\nusage: torch-model-archiver [-h] --model-name MODEL_NAME\n                            [--serialized-file SERIALIZED_FILE]\n                            [--model-file MODEL_FILE] --handler HANDLER\n                            [--extra-files EXTRA_FILES]\n                            [--runtime {python,python2,python3}]\n                            [--export-path EXPORT_PATH]\n                            [--archive-format {tgz,no-archive,default}] [-f]\n                            -v VERSION [-r REQUIREMENTS_FILE]\n\nTorch Model Archiver Tool\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --model-name MODEL_NAME\n                        Exported model name. Exported file will be named as\n                        model-name.mar and saved in current working directory if no --export-path is\n                        specified, else it will be saved under the export path\n  --serialized-file SERIALIZED_FILE\n                        Path to .pt or .pth file containing state_dict in case of eager mode\n                        or an executable ScriptModule in case of TorchScript or TensorRT\n                        or a .onnx file in the case of ORT.\n  --model-file MODEL_FILE\n                        Path to python file containing model architecture.\n                        This parameter is mandatory for eager mode models.\n                        The model architecture file must contain only one\n                        class definition extended from torch.nn.modules.\n  --handler HANDLER     TorchServe's default handler name\n                         or Handler path to handle custom inference logic.\n  --extra-files EXTRA_FILES\n                        Comma separated path to extra dependency files.\n  --runtime {python,python2,python3}\n                        The runtime specifies which language to run your inference code on.\n                        The default runtime is \"python\".\n  --export-path EXPORT_PATH\n                        Path where the exported .mar file will be saved. This is an optional\n                        parameter. If --export-path is not specified, the file will be saved in the\n                        current working directory. \n  --archive-format {tgz,no-archive,default}\n                        The format in which the model artifacts are archived.\n                        \"tgz\": This creates the model-archive in <model-name>.tar.gz format.\n                        If platform hosting TorchServe requires model-artifacts to be in \".tar.gz\"\n                        use this option.\n                        \"no-archive\": This option creates an non-archived version of model artifacts\n                        at \"export-path/{model-name}\" location. As a result of this choice, \n                        MANIFEST file will be created at \"export-path/{model-name}\" location\n                        without archiving these model files\n                        \"default\": This creates the model-archive in <model-name>.mar format.\n                        This is the default archiving format. Models archived in this format\n                        will be readily hostable on native TorchServe.\n  -f, --force           When the -f or --force flag is specified, an existing .mar file with same\n                        name as that provided in --model-name in the path specified by --export-path\n                        will overwritten\n  -v VERSION, --version VERSION\n                        Model's version\n  -r REQUIREMENTS_FILE, --requirements-file REQUIREMENTS_FILE\n                        Path to a requirements.txt containing model specific python dependency\n                         packages.\n\n\n\n\n\nTorchServe has the following handlers built-in that do post and pre-processing:\n\nimage_classifier\nobject_detector\ntext_classifier\nimage_segmenter\n\nYou can implement your own custom handler by following these docs. Most of the time you only need to subclass BaseHandler and override preprocess and/or postprocess.\n\n\nFrom the docs:\n\nimage_classifier, text_classifier and object_detector can all automatically map from numeric classes (0,1,2…) to friendly strings. To do this, simply include in your model archive a file, index_to_name.json, that contains a mapping of class number (as a string) to friendly name (also as a string)."
  },
  {
    "objectID": "notes/serving/torchserve/basic-torchserve.html#serving",
    "href": "notes/serving/torchserve/basic-torchserve.html#serving",
    "title": "Basics",
    "section": "Serving",
    "text": "Serving\nAfter archiving you can start the modeling server:\ntorchserve --start --ncs \\\n    --model-store model_store \\\n    --models densenet161.mar\nTorchServe uses default ports 8080 / 8081 / 8082 for REST based inference, management & metrics APIs and 7070 / 7071 for gRPC APIs.\n\n!torchserve --help\n\nusage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG]\n                  [--model-store MODEL_STORE]\n                  [--workflow-store WORKFLOW_STORE]\n                  [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\n                  [--log-config LOG_CONFIG] [--foreground]\n                  [--no-config-snapshots] [--plugins-path PLUGINS_PATH]\n\nTorchserve\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         Return TorchServe Version\n  --start               Start the model-server\n  --stop                Stop the model-server\n  --ts-config TS_CONFIG\n                        Configuration file for model server\n  --model-store MODEL_STORE\n                        Model store location from where local or default\n                        models can be loaded\n  --workflow-store WORKFLOW_STORE\n                        Workflow store location from where local or default\n                        workflows can be loaded\n  --models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]\n                        Models to be loaded using [model_name=]model_location\n                        format. Location can be a HTTP URL or a model archive\n                        file in MODEL_STORE.\n  --log-config LOG_CONFIG\n                        Log4j configuration file for model server\n  --foreground          Run the model server in foreground. If this option is\n                        disabled, the model server will run in the background.\n  --no-config-snapshots, --ncs\n                        Prevents to server from storing config snapshot files.\n  --plugins-path PLUGINS_PATH, --ppath PLUGINS_PATH\n                        plugin jars to be included in torchserve class path\n\n\n\n!curl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  7341  100  7341    0     0   108k      0 --:--:-- --:--:-- --:--:--  108k\n\n\n\n!curl http://127.0.0.1:8080/predictions/densenet161 -T kitten_small.jpg\n\n{\n  \"tabby\": 0.4783327877521515,\n  \"lynx\": 0.19989627599716187,\n  \"tiger_cat\": 0.1682717651128769,\n  \"tiger\": 0.061949197202920914,\n  \"Egyptian_cat\": 0.05116736516356468\n}\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI wouldn’t recommend installing torchserve and running it on a VM. It’s probably easier to use Docker.\ndocker pull pytorch/torchserve\n\n\n\nDocker\nSee these docs. We have to mount the necessary files and run the same commands. We also have to expose all the ports, etc.\n\n\n\n\n\n\nImportant\n\n\n\nNote that you have to supply the torchserve command, which implies you can run other things (but I don’t know what those are).\n\n\ndocker run --rm -it --gpus '\"device=0\"' \\\n    -p 8080:8080 \\\n    -p 8081:8081 \\\n    -p 8082:8082 \\\n    -p 7070:7070 \\\n    -p 7071:7071 \\\n    --mount type=bind,source=/home/hamel/hamel/notes/serving/torchserve/model_store,target=/tmp/models \\\n    pytorch/torchserve:latest-gpu \\\n    torchserve \\\n    --model-store /tmp/models \\\n    --models densenet161.mar\n\n!curl http://127.0.0.1:8080/predictions/densenet161 -T kitten_small.jpg\n\n{\n  \"tabby\": 0.4783327877521515,\n  \"lynx\": 0.19989627599716187,\n  \"tiger_cat\": 0.1682717651128769,\n  \"tiger\": 0.061949197202920914,\n  \"Egyptian_cat\": 0.05116736516356468\n}"
  },
  {
    "objectID": "notes/serving/torchserve/basic-torchserve.html#other-notes",
    "href": "notes/serving/torchserve/basic-torchserve.html#other-notes",
    "title": "Basics",
    "section": "Other Notes",
    "text": "Other Notes\nI found these articles to be very important:\n\nSource code for BaseHandler.\nPerformance guide: Concurrency and number of workers.\nconfig.properties example 1 and example 2 of how you can pass configuration files"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html",
    "href": "notes/serving/torchserve/hf.html",
    "title": "Serving Your Own Model",
    "section": "",
    "text": "Before we try to load models into Torch Serve, I’m going to download two different HuggingFace models and make sure I can do inference in a notebook.\n\n\nGPT-2 looks archaic compared to GPT-3\n\nfrom transformers import pipeline\npipe = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n\n\npreds = pipe([\"How do you use Torch Serve for model inference?\", \n              \"The quick brown fox jumps over the lazy\"])\npreds\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n[[{'generated_text': 'How do you use Torch Serve for model inference? Or just use Http to help. Or as a framework, where the actual code runs as expected, like something like JRuby on top of the project? Or maybe you use some way to get'}],\n [{'generated_text': 'The quick brown fox jumps over the lazy wolf, then hops over the hoot and follows him.'}]]\n\n\n\n\n\nThis definitely requires a GPU\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\n\nrepo_id = \"stabilityai/stable-diffusion-2\"\npipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=\"fp16\")\n\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda:0\")\n\n\n\n\n\nprompt = \"A Butterly in space\"\nimage = pipe(prompt, num_inference_steps=25)\nimg = image.images[0]\nimg\n\n\n\n\n\n\n\nYou can convert PIL to JSON serializable structures like this:\n\nimport numpy as np\nimg_as_list = np.array(img).tolist()"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#gpt-handler",
    "href": "notes/serving/torchserve/hf.html#gpt-handler",
    "title": "Serving Your Own Model",
    "section": "GPT Handler",
    "text": "GPT Handler\n\n%%writefile gpt_handler.py\n#gpt_handler.py\nimport logging\nimport torch\nfrom transformers import pipeline\nfrom ts.torch_handler.base_handler import BaseHandler\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Starting GPT Handler\")\n\nclass GptHandler(BaseHandler):\n    def __init__(self):\n        self.initialized = False\n        \n    def initialize(self, ctx):\n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        \n        self.device = torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\"))\n            if torch.cuda.is_available() and properties.get(\"gpu_id\") is not None\n            else \"cpu\"\n        )\n        \n        # you might normaly get the model from disk, but we don't have to in this case.\n        self.pipe = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n        self.initialized = True\n        \n    def preprocess(self, data): \n        text = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n        logging.info(f'Here is the text: {text}')\n        sentences = text.decode('utf-8')\n        return sentences\n    \n    def inference(self, data): return self.pipe(data)\n    \n    def postprocess(self, data): return data\n\nOverwriting gpt_handler.py"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#diffusion-handler",
    "href": "notes/serving/torchserve/hf.html#diffusion-handler",
    "title": "Serving Your Own Model",
    "section": "Diffusion Handler",
    "text": "Diffusion Handler\n\n%%writefile diffusion_handler.py\n#diffusion_handler.py\nimport logging\nimport torch\nimport numpy as np\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom ts.torch_handler.base_handler import BaseHandler\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Starting Diffusion Handler\")\n\nclass DiffusionHandler(BaseHandler):\n    def __init__(self):\n        self.initialized = False\n        \n    def initialize(self, ctx):\n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        \n        self.device = torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\"))\n            if torch.cuda.is_available() and properties.get(\"gpu_id\") is not None\n            else \"cpu\"\n        )\n        \n        repo_id = \"stabilityai/stable-diffusion-2\"\n        self.pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=\"fp16\")\n        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n        self.pipe = self.pipe.to(self.device)\n        self.initialized = True\n        \n    def preprocess(self, data): \n        text = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n        prompt = text.decode('utf-8')\n        return prompt\n    \n    def inference(self, data): \n        image = self.pipe(data, num_inference_steps=25)\n        img = image.images[0]\n        return np.array(img)\n    \n    def postprocess(self, data): \n        return [data.tolist()]\n\nOverwriting diffusion_handler.py"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#create-the-model-archive",
    "href": "notes/serving/torchserve/hf.html#create-the-model-archive",
    "title": "Serving Your Own Model",
    "section": "Create the model archive",
    "text": "Create the model archive\n\n!mkdir -p model_store\n\n! torch-model-archiver \\\n--export-path model_store \\\n--model-name \"gpt\" --version 1.0 \\\n--handler \"./gpt_handler.py\" \\\n--force\n\n! torch-model-archiver \\\n--export-path model_store \\\n--model-name \"diffusion\" --version 1.0 \\\n--handler \"./diffusion_handler.py\" \\\n--force\n\nWARNING - Overwriting model_store/gpt.mar ...\nWARNING - Overwriting model_store/diffusion.mar ..."
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#set-config.properties",
    "href": "notes/serving/torchserve/hf.html#set-config.properties",
    "title": "Serving Your Own Model",
    "section": "Set config.properties",
    "text": "Set config.properties\nThe diffusion response is bigger than the allowable default so we must pass a config. An example is here. I don’t know why I have to set the different ports like this, since these are the defaults (If I do not set these, things do not work properly).\n\n%%writefile config/config.properties\ninference_address=http://0.0.0.0:8080\nmanagement_address=http://0.0.0.0:8081\nmetrics_address=http://0.0.0.0:8082\nload_models=all\nmax_response_size=655350000\n\nOverwriting config/config.properties"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#create-a-dockerfile",
    "href": "notes/serving/torchserve/hf.html#create-a-dockerfile",
    "title": "Serving Your Own Model",
    "section": "Create a Dockerfile",
    "text": "Create a Dockerfile\nWe can get ideas from their Dockerfile\n\n%%writefile Dockerfile.gpt\n\nFROM pytorch/torchserve:latest-gpu\nRUN python -m pip install transformers diffusers\n\nENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\nCMD [\"serve\"]\n\nOverwriting Dockerfile.gpt\n\n\nBuild the Dockerfile\n\n! docker build -f Dockerfile.gpt . -t pytorch/torchserve:gpu-hf;\n\nSending build context to Docker daemon  334.6MB\nStep 1/4 : FROM pytorch/torchserve:latest-gpu\n ---> 046086392ab2\nStep 2/4 : RUN python -m pip install transformers diffusers\n ---> Using cache\n ---> 13135ca5603f\nStep 3/4 : ENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\n ---> Using cache\n ---> 6910f9182230\nStep 4/4 : CMD [\"serve\"]\n ---> Using cache\n ---> bbed6fd312c2\nSuccessfully built bbed6fd312c2\nSuccessfully tagged pytorch/torchserve:gpu-hf"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#list-models",
    "href": "notes/serving/torchserve/hf.html#list-models",
    "title": "Serving Your Own Model",
    "section": "List Models",
    "text": "List Models\n\n!curl http://127.0.0.1:8081/models\n\n{\n  \"models\": [\n    {\n      \"modelName\": \"diffusion\",\n      \"modelUrl\": \"diffusion.mar\"\n    },\n    {\n      \"modelName\": \"gpt\",\n      \"modelUrl\": \"gpt.mar\"\n    }\n  ]\n}"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#make-predictions",
    "href": "notes/serving/torchserve/hf.html#make-predictions",
    "title": "Serving Your Own Model",
    "section": "Make Predictions",
    "text": "Make Predictions\n\nGPT\nWith curl\n\n! echo \"The quick brown fox jumps over the lazy\" > gpt.txt\n! cat gpt.txt\n\nThe quick brown fox jumps over the lazy\n\n\n\n!curl http://127.0.0.1:8080/predictions/gpt -T gpt.txt\n\n{\n  \"generated_text\": \"The quick brown fox jumps over the lazy\\nI do find some funny gifs to do.\\nI used to have cats but I never find one\\nI used to have dogs. But I couldn't really find a cute dog but now I enjoy\"\n}\n\n\nWith requests\n\nimport requests\nresp = requests.post('http://127.0.0.1:8080/predictions/gpt',\n                     data={'data': \"The quick brown fox jumps over the lazy\"})\nresp.text\n\n'{\\n  \"generated_text\": \"The quick brown fox jumps over the lazy blonde to win, the latter has to wait for the rest to come on, and she also needs a hug and a hug. The adorable feline can barely contain itself, but the kitten is quite responsive,\"\\n}'\n\n\n\n\nDiffusion\n\nimport requests\nimport json\nfrom PIL import Image\n\nresp = requests.post('http://127.0.0.1:8080/predictions/diffusion',\n                     data={'data': \"A butterfly in space with glasses.\"})\n\n\ndimg = np.array(json.loads(resp.text), dtype='uint8')\nImage.fromarray(dimg)"
  },
  {
    "objectID": "notes/programming-languages/index.html",
    "href": "notes/programming-languages/index.html",
    "title": "programming languages",
    "section": "",
    "text": "High level takeaways after completing the 3-Part Coursera class Programming Languages with Dan Grossman.\nYour GitHub repo for this class (private) is here."
  },
  {
    "objectID": "notes/programming-languages/index.html#sml-standard-ml-part-a",
    "href": "notes/programming-languages/index.html#sml-standard-ml-part-a",
    "title": "programming languages",
    "section": "SML (Standard ML) Part A",
    "text": "SML (Standard ML) Part A\n\nYou setup vim to have an IDE for this. See notes in the VIM section below.\nML is a statically typed language with magical type inference that works really well. It automatically determines the types and is very intuitive and helpful.\nLearned how to use recursion everywhere instead of loops, particularly with hd, tl and cons.\nLocal variable binding with let is very important (which also allows you to bind local/private functions as well)\ncons allows you to append to the beginning of a list\nThere is an option type that is NONE or SOME v\nThis language doesn’t encourage mutation, which is a feature. Otherwise, you can use a reference which is like a pointer to mutate a variable.\npattern matching with a case expression: This is one of the coolest things that I learned, and something similar is coming to Python v 3.10.\n\nYou can have nested patterns\nYou can pattern match against function arguments which allow for really nice syntax for achieving multiple dispatch type of functionality.. (not sure about python)\nYou can pattern match against types as well as data structures.\nYou can have constants in there as well.\n\ncase name \n     NameType name => ...\n   | (first, \"MyLastName\") => ...\n   | (first, last) => ...\n   | name => ...\n   | _ => ...\nTail recursion with accumulators. Ex- factorial\nThe fn keyword is used to define anonymous functions.\nML uses lexical scope which means function is evaluated in the environment where the function was defined. dynamic scope, which is usually not desired, is the alternative where the function is evaluated in the in the environment it is called.\nClosure - the call stack has a “pair” that is the (function, environment when the function was defined). This pair is called the closure. The call stack has a snapshot of what the environment looked like at the time the function was defined.\nfold is like reduce.\nML supports function composition like this with the keyword o: f1 o f2 o f3\n\nbest to do a val binding to avoid unnecessary wrapping: val newfunc = f1 o f2\nwith o you apply functions from right to left so f1 o f2 x is the same as f1(f2(x)) there is an alternative that is left to right called the pipeline operator.\n\nCurrying and partial application\n\nUniversal way to make a func curryable: ml       fun myfunc x           let fun f2 (z) = z               fun f1 (y) = f2(y)           begin               f1           end\nML has first class support for currying so you don’t have to do the above hack.\n\nML supports mutual recursion just like let-rec in racket."
  },
  {
    "objectID": "notes/programming-languages/index.html#racket-part-b",
    "href": "notes/programming-languages/index.html#racket-part-b",
    "title": "programming languages",
    "section": "Racket (Part B)",
    "text": "Racket (Part B)\nRacket is related to Lisp and Scheme. Everything is a function. Parenthesis for everything. The position of parenthesis changes the meaning of the code.\n\nRacket has dynamic typing, unlike SML.\nThunks: Wrap a function in a zero argument function to delay evaluation. Applications:\n\nStreams: the function will return a tuple of (value, func), and when you call func it will return (value, func) so you get one value at a time. This is not specific to Racket.\nLazy evaluation: You can use thunks to delay execution like a promise to a later time. This is an example of lazy evalution that doesn’t actually evaluate anything until being forced to:\n\n\n(define (my-delay f) (mcons #f f))\n\n(define (my-force th)\n\n(if (mcar th) (mcdr th) (begin (set-mcar! th #t) (set-mcdr! th ((mcdr th))) (mcdr th))))\nRacket allows you use macros that will evaluate before the code is run and that will “expand” into valid racket syntax.\nYou implemented your own small programming language. This used recursive calls to evluate expressions with the base case being the values (Integer, strings, etc). - Interperter: write a program in another language A that takes programs in B and produces answers directly. A better term would be “evaluator”. - Compiler: write program in another language A that takes programs in B and produces an equivalent program in langauage C. A better term here would be “translator”.\nClosures: for lexical scope, the interpreter has a stack of tuples. The tuples are (1) the function to be called (2) the environment, which contains the value of all variables at the time the function was defined. You also have to track the arguments for the function seperately, so you can evaluate the arguments in the environment the function was run in."
  },
  {
    "objectID": "notes/programming-languages/index.html#ruby-part-c",
    "href": "notes/programming-languages/index.html#ruby-part-c",
    "title": "programming languages",
    "section": "Ruby (Part C)",
    "text": "Ruby (Part C)\nI didn’t spend too much time some concepts I was mostly familiar with this.\n\nRuby is OOP, dynamically typed.\nRuby is pure OOP, even top level functions and variables are part of the built-in Object class.\nThey have fastcore like shortcuts for getters and setters:\n\nattr_reader :y, :z # defines getters \nattr_accessor :x # defines getters and setters\nnewlines are important. The syntax can change without them.\nDynamic class definitions. The following code will result in Class with the methods foo and bar! The second one doesn’t override the first one!\nclass Class\n    def foo\n        ...\n    end\nend\n\nclass Class\n    def bar\n        ...\n    end\nend\n\nBlocks\nThey also have a very convenient lambda like thing called Blocks:\nsum = 0 \n[4,6,8].each { |x| sum += x \n               puts sum }\nYou can use Blocks to make accumulators too, and even use inject to initialize the accumulator:\nsum = [4,6,8].inject(0) { |acc,elt| acc + elt }\nTo use blocks in a method, you will have to look that up in the docs. This involves the yield keyword. For example, this code will print “hi” 3 times:\ndef foo x \n  if x \n    yield \n   else \n    yield \n    yield \n   end \nend \n\nfoo (true) { puts \"hi\" } \nfoo (false) { puts \"hi\" }\nBlocks are not first class functions even though they kind of look like lambdas. Lets say you wanted to map over an array but wanted to return an array of functions instead of values. The way to do this is to use the keyword lambda:\nc = a.map {|x| {|y| x >= y} } # wrong, a syntax error\n\nc = a.map {|x| lambda {|y| x >= y} } # this will work\n\nSubclassing\n\nsuper calls the same method in the parent class. You dont have to do super.method_name(), just super.\nInstance variables are preceeded with @\n\nChild classes are defined like this:\nclass Child < Parent\n ...\nend\n\n\n\nTyping\nThey discussed the various ways different type systems are constructed. The interface idiom, that is familar to you from Golang (but not specific to Golang) was introduced."
  },
  {
    "objectID": "notes/programming-languages/index.html#vim",
    "href": "notes/programming-languages/index.html#vim",
    "title": "programming languages",
    "section": "VIM",
    "text": "VIM\nFor the Standard ML programming language I decided to force myself to use vim. I added the following things to my .vimrc to make it manageable. Note the plugin jez/vim-better-sml\n\" from https://github.com/jez/vim-as-an-ide\nset nocompatible\n\ninoremap <C-e> <C-o>A\n\n\nfiletype off\n\nset rtp+=~/.vim/bundle/Vundle.vim\ncall vundle#begin()\n\nPlugin 'VundleVim/Vundle.vim'\n\n\" ----- Making Vim look good ------------------------------------------\nPlugin 'altercation/vim-colors-solarized'\nPlugin 'tomasr/molokai'\nPlugin 'vim-airline/vim-airline'\nPlugin 'vim-airline/vim-airline-themes'\n\n\" ----- Vim as a programmer's text editor -----------------------------\nPlugin 'scrooloose/nerdtree'\nPlugin 'jistr/vim-nerdtree-tabs'\nPlugin 'vim-syntastic/syntastic'\nPlugin 'xolox/vim-misc'\nPlugin 'xolox/vim-easytags'\nPlugin 'majutsushi/tagbar'\nPlugin 'ctrlpvim/ctrlp.vim'\n\" ----- Working with Git ----------------------------------------------\nPlugin 'airblade/vim-gitgutter'\nPlugin 'tpope/vim-fugitive'\nPlugin 'Raimondi/delimitMate'\nPlugin 'jez/vim-better-sml'\nPlugin 'christoomey/vim-tmux-navigator'\nPlugin 'benmills/vimux'\ncall vundle#end()\n\nfiletype plugin indent on\n\nset number\nset ruler\nset showcmd\nset incsearch\nset hlsearch\nset backspace=indent,eol,start\n\nsyntax on\nset mouse=a"
  },
  {
    "objectID": "notes/fastai/03_data.html",
    "href": "notes/fastai/03_data.html",
    "title": "Data",
    "section": "",
    "text": "from fastbook import *"
  },
  {
    "objectID": "notes/fastai/03_data.html#hello-world-datablock",
    "href": "notes/fastai/03_data.html#hello-world-datablock",
    "title": "Data",
    "section": "Hello World DataBlock",
    "text": "Hello World DataBlock\nThe argument get_x and get_y operate on an iterable. Let’s define an interable as our data:\n\ndata = list(range(100))\n\n\ndef get_x(r): return r\ndef get_y(r): return r + 10\ndblock = DataBlock(get_x=get_x, get_y = get_y)\ndsets = dblock.datasets(data)\n\nYou can see a dataset like so:\n\ndsets.train[0]\n\n(89, 99)\n\n\nYou can also see a DataLoader like so:\n\ndls = dblock.dataloaders(data, bs=5)\n\n\nnext(iter(dls.train))\n\n(tensor([57, 66, 73, 30, 14]), tensor([67, 76, 83, 40, 24]))\n\n\n\nWith A DataFrame\nSimilarly, you can operate on one row at a time:\n\nimport pandas as pd\ndf = pd.DataFrame({'x': range(100), 'y': range(100) })\ndf.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      0\n      0\n    \n    \n      1\n      1\n      1\n    \n    \n      2\n      2\n      2\n    \n    \n      3\n      3\n      3\n    \n    \n      4\n      4\n      4\n    \n  \n\n\n\n\n\ndef get_x(r): return r.x\ndef get_y(r): return r.y + 10\ndblock = DataBlock(get_x=get_x, get_y=get_y)\ndsets = dblock.datasets(df)\n\n\ndsets.train[0]\n\n(78, 88)\n\n\n\ndls = dblock.dataloaders(df, bs=3)\nnext(iter(dls.train))\n\n(tensor([90, 55, 11]), tensor([100,  65,  21]))\n\n\n\ndef tracer(nm):\n    def f(x, nm):\n        # print(f'{nm}:')\n        # print(f'\\tinput: {x}')\n        # import ipdb; ipdb.set_trace()\n        return str(x)\n    return partial(f, nm=nm)\n\n\ndef mult_0(x): return x * 0\ndef add_1(x): return x +1 \ntb = TransformBlock(item_tfms=[tracer('item_tfms')])\n# def get_y(l): return sum(l)\ndb = DataBlock(blocks=(TransformBlock, TransformBlock),\n               get_x=mult_0,\n               get_y=add_1,\n               item_tfms=lambda x: str(x))\n\n\ndata = L(range(10))\nresult = db.datasets(data)\n\n\ndb.summary(data)\n\nSetting-up type transforms pipelines\nCollecting items from [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFound 10 items\n2 datasets of sizes 8,2\nSetting up Pipeline: mult_0\nSetting up Pipeline: add_1\n\nBuilding one sample\n  Pipeline: mult_0\n    starting from\n      1\n    applying mult_0 gives\n      0\n  Pipeline: add_1\n    starting from\n      1\n    applying add_1 gives\n      2\n\nFinal sample: (0, 2)\n\n\nCollecting items from [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFound 10 items\n2 datasets of sizes 8,2\nSetting up Pipeline: mult_0\nSetting up Pipeline: add_1\nSetting up after_item: Pipeline: <lambda> -> ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: \n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: <lambda> -> ToTensor\n    starting from\n      (0, 2)\n    applying <lambda> gives\n      (0, 2)\n    applying ToTensor gives\n      (0, 2)\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nNo batch_tfms to apply\n\n\n\nresult.train[0]\n\n(0, 5)\n\n\n\nresult = db.dataloaders(data, bs=3)\n\n\nthing = iter(result.train)\n\n\nnext(thing)\n\n(('0', '0', '0'), ('6', '7', '4'))\n\n\n\nnext(thing)\n\n(('0', '0', '0'), ('9', '5', '3'))\n\n\n\n??TransformBlock\n\n\ndb = DataBlock(blocks=(TransformBlock, tb),\n              get_y=lambda x: str(x),\n              batch_tfms=tracer('batch_tfms'))\n\n\nresult = db.datasets(data)\nresult = db.dataloaders(data, bs=3)\n\n\nresult\n\n<fastai.data.core.DataLoaders>\n\n\n\nthing = iter(result.train)\n\n\nnext(thing)\n\n(('1', '5', '6'), ('1', '5', '6'))\n\n\n\nf = aug_transforms()[0]\n\n\nf\n\nFlip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5}:\nencodes: (TensorImage,object) -> encodes\n(TensorMask,object) -> encodes\n(TensorBBox,object) -> encodes\n(TensorPoint,object) -> encodes\ndecodes:"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#dataloaders",
    "href": "notes/fastai/01_fundamentals.html#dataloaders",
    "title": "Fundamentals",
    "section": "DataLoaders",
    "text": "DataLoaders\nDataLoaders is a thin class around DataLoader, and makes them available as train and valid.\nSame thing applies to Datasets and Dataset.\nIn pytorch, Dataset is fed into a DataLoader."
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#datablocks",
    "href": "notes/fastai/01_fundamentals.html#datablocks",
    "title": "Fundamentals",
    "section": "DataBlocks",
    "text": "DataBlocks\n\nUse this to create DataLoaders\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\nDataBlocks are a template for creating DataLoaders, and need to be instantiated somehow - for example given a path where to find the data:\ndls = bears.dataloaders(path)\nYou can modify the settings of a DataBlock with new:\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) #book has more examples\ndls = bears.dataloaders(path)\nYou can sanity check / see transformed data with show_batch:\n>>> dls.train.show_batch(max_n=8, nrows=2, unique=True)\n... images\nYou also use DataBlocks for data augmentation, with batch_tfms:\nbears = bears.new(\n    item_tfms=Resize(128),        \n    batch_tfms=aug_transforms(mult=2)\n)\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#training",
    "href": "notes/fastai/01_fundamentals.html#training",
    "title": "Fundamentals",
    "section": "Training",
    "text": "Training\nMost things use learn.fine_tune(), when you cannot fine-tune like tabular data, you often use learn.fit_one_cycle\nYou can also do learn.show_results(...)\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): \n    return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n        path=str(path), \n        fnames=get_image_files(path), \n        valid_pct=0.2, \n        seed=42,\n        label_func=is_cat, \n        item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\nMore info on what this is in later sections.\n\nInterpetability\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\nAlso see top losses:\ninterp.plot_top_losses(5, nrows=1)\n\n\nCleaning\nYou can get a ImageClassifierCleaner which allows you to choose (1) a category and (2) data partition (train/val) and shows you the highest loss items so you can decide whether to Keep, Delete, Change etc.\ncleaner = ImageClassifierCleaner(learn)\ncleaner\nThe thing doesn’t actually delete/change anything but gives you the idxs that allow you to do things with them\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\nLoading / Saving\nSaving a model can be done with learn.export, when you do this, fastai will save a file called “export.pkl”\nlearn.export()\nload_learner can be used to load a model\nlearn_inf = load_learner(path/'export.pkl')\n\n\nPredicting\nWhen you call predict, you will get three things: (1) class, (2) the index of the predicted category (3) Probabilities of each category\n>>> learn_inf.predict('images/grizzly.jpg')\n('grizzly', tensor(1), tensor([9.0767e-06, 9.9999e-01, 1.5748e-07]))\nYou can see all the classes with dls.vocab:\n>>> learn_inf.dls.vocab\n(#3) ['black','grizzly','teddy']\nZach: learn.dls.vocab or learn.dls.categorize.vocab is another way to get the class names."
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#computer-vision",
    "href": "notes/fastai/01_fundamentals.html#computer-vision",
    "title": "Fundamentals",
    "section": "Computer Vision",
    "text": "Computer Vision\nYou can open an image with Pilow (PIL)\nim3 = Image.open(im3_path)\nim3\n\n#convert to numpy\narray(im3)\n# convert to pytorch tensor\ntensor(im3)\n\nPixel Similarity Baseline\n\nCompute avg pixel value for 3’s and 7’s\nAt inference time, see which one its similar too, using RMSE (L2 Norm) and MAE (L1 Norm)\n\nKind of like KNN\nTaking an inference tensor, a_3 and calculate distance to mean 3 and 7:\n# MAE & RMSE for 3  vs avg3\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n\n# MAE & RMSE for 3  vs avg7\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n\n# Use Pytorch Losses to do the same thing for 3 vs avg 7\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n\nnumpy\nTake the mean over an axis:\ndef mnist_distance(a,b): \n    #(-2,1) means take the average of the last 2 axis\n    return (a-b).abs().mean((-2,-1))"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#sgd-from-scratch",
    "href": "notes/fastai/01_fundamentals.html#sgd-from-scratch",
    "title": "Fundamentals",
    "section": "SGD from scratch",
    "text": "SGD from scratch\n\nMinimal Example\n# the loss function\ndef mse(y, yhat): \n    return (y - yhat).square().mean().sqrt()\n\n# the function that produces the data\ndef quadratic(x, params=[.75, -25.5, 15]):\n    a,b,c = params\n    noise = (torch.randn(len(x)) * 3)\n    return a*(x**2) + b*x +c + noise\n\n# generate training data\nx = torch.arange(1, 40, 1)\ny = quadratic(x)\n\n# define the training loop\ndef apply_step(params, pr=True):\n    lr = 1.05e-4\n    preds = quadratic(x, params)\n    loss = mse(preds, y)\n    loss.backward()\n    params.data -= params.grad.data * lr\n    if pr: print(f'loss: {loss}')\n    params.grad = None\n\n# initialize random params\nparams = torch.rand(3)\nparams.requires_grad_()\nassert params.requires_grad\n\n# train the model\nfor _ in range(1000):\n    apply_step(params)\n\n\nMNIST\nA Dataset in pytorch is required to return a tuple of (x,y) when indexed. You can do this in python as follows:\n# Turn mnist data into vectors 3dim -> 2dim\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n# Generate label tensor\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n# Create dataset\ndset = list(zip(train_x,train_y))\n\n# See shapes from first datum in the dataset\n>>> x,y = dset[0]\n>>> x.shape, y.shape\n(torch.Size([784]), torch.Size([1]))\n\n\n# Do the same thing for the validation set\n....\n\nMini Batch SGD\n# `@` and dot product is the same:\na, b = torch.rand(10), torch.rand(10)\nassert a.dot(b) == a@b\n\n# define model\ndef init_params(size, std=1.0): \n    return (torch.randn(size)*std).requires_grad_()\nweights = init_params((28*28,1))\nbias = init_params(1)\n\ndef linear1(xb): return xb@weights + bias\n\n#naive loss (for illustration)\ncorrects = (preds>0.0).float() == train_y\ncorrects.float().mean().item()\n\n# define loss\ndef mnist_loss(preds, targets):\n    preds = preds.sigmoid() #squash b/w 0 and 1\n    return torch.where(targets==1, 1-preds, preds).mean() # average distance loss\n\nCreate a dataloader\nYou want to load your data in batches, so you will want to create a dataloader. Recall that in pytorch, a Dataset is required to return a tuple of (x,y) when indexed, which is quite easy to do:\n# define a data loader using `dset`\ndset = list(zip(train_x,train_y))\nPytorch offers a utility to then create a Dataloader from a dataset, but Jeremy basically rolled his own (w/same api):\ndl = DataLoader(dset, batch_size=256)\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\n\nThe Training Loop\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_() #updates in place\n\n### Calculate metrics\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n# Train model\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n# Train model w/epochs\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n\n\nUsing Pytorch\nBlueprint: 1. Define a dataset and then a dataloader 2. Create a model, which will have parameters 3. Create an optimizer, that: - Updates the params: params.data -= parmas.grad.data * lr - Zeros out the gradients: setting params.grad = None or zeroing out the gradients with params.grad.zero_() 4. Generate the predictions 5. Calculate the loss 6. Calculate the gradients loss.backward() 7. Using the optimizer, update the weights step and zero out the gradients zero_grad 8. Put 4-7 in a loop.\nCreate an optimizer and use nn.Linear\nlinear_model = nn.Linear(28*28,1)\nw,b = linear_model.parameters()\n\n# Define an optimizer\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nopt = BasicOptim(linear_model.parameters(), lr)\n# alternative, fastai provides SGD\nopt = SGD(linear_model.parameters(), lr)\n\n# Define Metrics\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\n# Helper to calculate metrics on validation set\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\ntrain_model(linear_model, 20)\n\nUsing fastai\nWe can substitute the above with learner.fit from fastai We just have to supply the following:\n\nDataloaders\nModel\nOptimization function\nLoss function\nMetrics\n\ndls = DataLoaders(dl, valid_dl)\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, \n                loss_func=mnist_loss,\n                metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\nWhat if you used the full power of fastai? It would look like this:\ndls = ImageDataLoaders.from_folder(path)\n# Lots of things have defaults like optimization func\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, \n                     metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#simple-neural-nets",
    "href": "notes/fastai/01_fundamentals.html#simple-neural-nets",
    "title": "Fundamentals",
    "section": "Simple Neural Nets",
    "text": "Simple Neural Nets\nThe next step is to introduce a non-linearity\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n# Construct the learner as before\nlearn = learner(dls, simple_net, opt_func=SGD,\n               loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearner.fit(40, 0.1)\n\nInspecting Training History\nThe training history is saved in learn.recorder. You can plot your training progress with:\nplt.plot(learn.recorder.values).itemgot(2)"
  },
  {
    "objectID": "notes/fastai/batch_predicitions.html",
    "href": "notes/fastai/batch_predicitions.html",
    "title": "Batch Predictions",
    "section": "",
    "text": "How to make batch predictions in fastai\nMaking batch predictions on new data is not provided “out of the box” in fastai. This is how you can achieve that:\nAdd this method to learner:\n@patch\ndef predict_batch(self:Learner, item, rm_type_tfms=None, with_input=False):\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n    i = getattr(self.dls, 'n_inp', -1)\n    inp = (inp,) if i==1 else tuplify(inp)\n    dec_inp, nm = zip(*self.dls.decode_batch(inp + tuplify(dec_preds)))\n    res = preds,nm,dec_preds\n    if with_input: res = (dec_inp,) + res\n    return res\nYou can then use this method like so:\n>>> from fastai.text.all import *\n>>> from predict_batch import predict_batch # this file.  If you don't import just define in your script.\n>>> dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n>>> learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n>>> learn.fine_tune(4, 1e-2)\n>>> learn.predict_batch([\"hello world\"]*4)\n(TensorText([[0.0029, 0.9971],\n         [0.0029, 0.9971],\n         [0.0029, 0.9971],\n         [0.0029, 0.9971]]),\n ('pos', 'pos', 'pos', 'pos'),\n TensorText([1, 1, 1, 1]))\nAlternatively, you can just patch the predict function so it works on batches:\n@patch\ndef predict(self:Learner, item, rm_type_tfms=None, with_input=False):\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n    i = getattr(self.dls, 'n_inp', -1)\n    inp = (inp,) if i==1 else tuplify(inp)\n    dec = self.dls.decode_batch(inp + tuplify(dec_preds))\n    dec_inp,dec_targ = (tuple(map(detuplify, d)) for d in zip(*dec.map(lambda x: (x[:i], x[i:]))))\n    res = dec_targ,dec_preds,preds\n    if with_input: res = (dec_inp,) + res\n    return res\nOther notes h/t zach:\nlearn.dls.vocab or learn.dls.categorize.vocab is another way to get the class names."
  },
  {
    "objectID": "notes/fastai/02_cv.html#data-prep",
    "href": "notes/fastai/02_cv.html#data-prep",
    "title": "Image Classification",
    "section": "Data Prep",
    "text": "Data Prep\nRemember, Datablock helps create DataLoaders.\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\nDebugging\nYou can debug the Datablock by calling .summary(), which will show you if you have any errors.\npets.summary(path/\"images\")\nIf everything looks good, you can use the DataBlock to create a DataLoaders instance:\ndls = pets.dataloaders(path/\"images\")\nOnce you have a DataLoaders instance, it is a good idea to call show_batch to spot check that things look reasonable:\nYou can debug this by using show_batch:\n>>> dls.show_batch(nrows=1, ncols=3)\n... [shows images]\nFinally, you can see what a batch looks like by calling dls.one_batch()\nx,y = dls.one_batch()\nYou always want to train a model ASAP as your final debugging step. If you wait too long, you will not discover problems\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n\nExample of an error in data prep\nA common error is forgetting to use Resize in your DataBlock as an item transform. For example, the below code will cause an error:\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 #forgot to pass `item_tfms=Resize(...),`\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")\nThis will complain that it is not able to collate the images because they are of different sizes."
  },
  {
    "objectID": "notes/fastai/02_cv.html#interpretation",
    "href": "notes/fastai/02_cv.html#interpretation",
    "title": "Image Classification",
    "section": "Interpretation",
    "text": "Interpretation\nYou can get diagnostics for your model using this:\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\nWhich will return a confusion matrix. You can see the “most confused” items by doing this:\n>>> interp.most_confused(min_val=5)\n[('Bengal', 'Egyptian_Mau', 10),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 8),\n ('Ragdoll', 'Birman', 7),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 6),\n ('american_pit_bull_terrier', 'american_bulldog', 5)]"
  },
  {
    "objectID": "notes/fastai/02_cv.html#improving-the-model",
    "href": "notes/fastai/02_cv.html#improving-the-model",
    "title": "Image Classification",
    "section": "Improving the model",
    "text": "Improving the model\n\nLearning Rate Finder\nStart with a very, very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then we do another mini-batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\nThe last point where the loss was clearly decreasing\n\nThe learning rate finder computes those points and more on the curve to help you. Additional learning rate suggestion algorithms can be passed into the function, by default only the valley paradigm is used. The learning rate finder can be called with learn.lr_find:\n>>> learn = cnn_learner(dls, resnet34, metrics=error_rate)\n>>> lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\nThe default valley hueristic works just fine. Note, you will want to re-run this anytime you change your model such as unfreeze layers. You might want to run this periodically if you are checkpointing during training.\n\n\nFine Tuning models\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:\n\nTrains the randomly added layers for one epoch, with all other layers frozen\nUnfreezes all of the layers, and trains them all for the number of epochs requested\n\nAlthough this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The fine_tune method has a number of parameters you can use to change its behavior, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior.\nfit_one_cycle is the suggested way to train models without using fine_tune. We’ll see why later in the book; in short, what fit_one_cycle does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3) # train the head\nlearn.unfreeze() # unfreeze everything\nlearn.lr_find() # find new lr after unfreezing\nlearn.fit_one_cycle(6, lr_max=1e-5) #fine tune it all\n\n\nDiscriminative Learning Rates\nOne important aspect of fine tuning is discriminative learning rates: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers).\nfastai lets you pass a Python slice object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let’s use this approach to replicate the previous training, but this time we’ll only set the lowest layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let’s train for a while and see what happens:\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\nWe can accomplish everything we did above by calling fine_tune instead. fine_tune will automatically apply discriminative learning rates for you:\n>>> learn.fine_tune??\nSignature:\nlearn.fine_tune(\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n)\nSource:   \n@patch\n@delegates(Learner.fit_one_cycle)\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr /= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nFile:      ~/anaconda3/lib/python3.9/site-packages/fastai/callback/schedule.py\nType:      method"
  },
  {
    "objectID": "notes/fastai/02_cv.html#mixed-precision-training",
    "href": "notes/fastai/02_cv.html#mixed-precision-training",
    "title": "Image Classification",
    "section": "Mixed Precision Training",
    "text": "Mixed Precision Training\nYou can achieve mixed precision training to speed up training and give you more memory headroom for bigger models with to_fp16()\nfrom fastai.callback.fp16 import *\nlearn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\nNote how you can use the freeze_epochs parameter to keep the base frozen for longer."
  },
  {
    "objectID": "notes/fastai/02_cv.html#datablock-api-multi-label-data",
    "href": "notes/fastai/02_cv.html#datablock-api-multi-label-data",
    "title": "Image Classification",
    "section": "DataBlock API: Multi-Label Data",
    "text": "DataBlock API: Multi-Label Data\nLet’s say you have a Dataframe with filenames and multiple labels per filename. The best way to get started in to use the DataBlock api to construct Datasets and DataLoaders. A review of terminology:\n\nDataset: collection that returns a tuple of (x,y) for single item. Can do this with list(zip(x,y))\nDataLoader: an iterator that provides a stream of minibatches of (x,y) instead of a single item.\nDatasets: object that contains a training Dataset and a Validation dataset.\nDataLoaders: object that contains a training DataLoader and a validation DataLoader.\n\n\nCreating Datsets\nYou can use a DataBlock:\n>>> from fastbook import *\n>>> from fastai.vision.all import *\n\n>>> path = untar_data(URLs.PASCAL_2007)\n>>> df = pd.read_csv(path/'train.csv')\n>>> def get_x(r): return path/'train'/r['fname']\n>>> def get_y(r): return r['labels'].split(' ')\n\n>>> dblock = DataBlock(get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(Path('/home/hamel/.fastai/data/pascal_2007/train/006162.jpg'), ['aeroplane'])\nNext we need to convert our images into tensors. We can do this by using the ImageBlock and MultiCategoryBlock:\n\n\nUsing Blocks For Transforms\n>>> dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                       get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\nInspecting Vocabulary\nYou can inspect the vocabulary with the vocab attribute:\ndsets.train.vocab\n\n\nUsing a splitter\nThe dataframe has a column called is_valid, we can use that do a train validation split. By default, the DataBlock uses a RandomSplitter. By default, RandomSplitter uses 20% of the data for the validation set.\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\n\n\nCreating DataLoaders\nDataLoaders build upon Datasets by streaming mini-batches instead of one example at a time. One prerequisite to making DataLoaders is that all the images are the same size. To do this you can use RandomResizedCrop:\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\nWhen you are done with this, you want to debug things by calling show_batch:\ndls.show_batch(nrows=1, ncols=3)"
  },
  {
    "objectID": "notes/fastai/02_cv.html#multi-label-model",
    "href": "notes/fastai/02_cv.html#multi-label-model",
    "title": "Image Classification",
    "section": "Multi-Label Model",
    "text": "Multi-Label Model\nYou can create a learner like so:\nlearn = cnn_learner(dls, resnet18)\nOne useful thing is to debug / verify that the output shape conforms to what you are expecting. You can do this by running a tensor through your model and inspecting it’s output:\nx,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\n\nThis is what you would use to extract embeddings / activations out of another model\n\nIt’s a good idea to see what the activations look like:\n>>> activs[0]\nTensorBase([ 2.0858,  2.8195,  0.0460,  1.7563,  3.3371,  2.4251,  2.3295, -2.8101,  3.3967, -3.2120,  3.3452, -2.3762, -0.3137, -4.6004,  0.7441, -2.6875,  0.0873, -0.2247, -3.1242,  3.6477],\n       grad_fn=<AliasBackward0>)\nWe can see these are not b/w 0 and 1, because the sigmoid has not been applied yet.\n\nLoss Functions\nPyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names!\nF.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross-entropy on a one-hot-encoded target, but do not include the initial sigmoid. Normally for one-hot-encoded targets you’ll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example.\nThe equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax.\nSince we have a one-hot-encoded target, we will use BCEWithLogitsLoss:\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nWe don’t actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default.\n\n\nMetrics\nWe need to make sure we have a metric that works for multi-label classfication:\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\nWe can use partial to set the parameters we want in the metrics function and pass it like this:\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\nYou can change your metrics anytime and recalculate things. validate() will return the validation loss and metrics.\n>>> learn.metrics = partial(accuracy_multi, thresh=0.1)\n>>> learn.validate() # returns validation loss and metrics\n(#2) [0.10417556017637253,0.9376891851425171]\nYou can debug metrics by getting the predictions on the validation set with get_preds:\npreds,targs = learn.get_preds()\nassert preds.shape[0] == dls.valid.n\nOnce you have the predictions, you can run the metric function seperately:\naccuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\n\n\nChoosing A Prediction Threshold\nxs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);\n\nIn this case, we’re using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be overfitting to the validation set, since we’re trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we’re clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don’t try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it’s fine to do this)."
  },
  {
    "objectID": "notes/fastai/02_cv.html#image-regression",
    "href": "notes/fastai/02_cv.html#image-regression",
    "title": "Image Classification",
    "section": "Image Regression",
    "text": "Image Regression\nYes, X is images and y are floats. Ex: key point model -> predicting location of something like the center of someone’s face.\n\nGet Data\nFirst step is to get data with get_image_files\n# view the data and it's structure\npath = untar_data(URLs.BIWI_HEAD_POSE)\nPath.BASE_PATH = path\npath.ls().sorted()\n(path/'01').ls().sorted()\n\n# next get all the data systematically\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\nIt is a good idea to see what you are working with as a general rule.\n\nYou can inspect images with the following code\nim = PILImage.create(img_files[0])\nim.to_thumb(160)\n\nDefine the functions to extract the data you need from the files. You can ignore what this does and treat it as a helper function, b/c your problem is likely to be specific.\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nDefine the DataBlock\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)),  \n)\nNote the splitter function: we want to ensure that our model can generalize to people that it hasn’t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person’s images.\n\nPoints and Data Augmentation: We’re not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you’re working with another library, you may need to disable data augmentation for these kinds of problems.\n\nThe only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images\n\n\nDebug the DataBlock\nUsing showbatch:\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\nInspect the shape:\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n\n\nTrain The Model\nAs usual, we can use cnn_learner to create our Learner. Remember way back in <> how we used y_range to tell fastai the range of our targets? We’ll do the same here - coordinates in fastai and PyTorch are always rescaled between -1 and +1 by the PointBlock, which is why you pass (-1, 1) to y_range.\n\nSetting y_range\n# Always use y_range when predicting a continous target\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\nWe didn’t specify a loss function, which means we’re getting whatever fastai chooses as the default.\n>>> dls.loss_func\nFlattenedLoss of MSELoss()\nNote also that we didn’t specify any metrics. That’s because the MSE is already a useful metric for this task (although it’s probably more interpretable after we take the square root).\n\nYou should always set y_range when predicting continuous targets. y_range is implemented in fastai using sigmoid_range, which is defined as:\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\nThis is set as the final layer of the model, if y_range is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range (lo,hi).\nHere’s what it looks like:\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n\n\n\n\nFind the learning rate and then train\nlearn.lr_find()\nlr = 1e-2\nlearn.fine_tune(3, lr)\n\n\nInspect the Results\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))"
  },
  {
    "objectID": "notes/fastai/02_cv.html#loss-functions-1",
    "href": "notes/fastai/02_cv.html#loss-functions-1",
    "title": "Image Classification",
    "section": "Loss Functions",
    "text": "Loss Functions\nfastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want:\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "notes/linux/osx.html",
    "href": "notes/linux/osx.html",
    "title": "OSX Shell Tips",
    "section": "",
    "text": "Key Repeat Rate\nAdd days to your lifespan by Increasing the key repeat rate. Run the following in the terminal then restart. Protip by Michael Musson.\ndefaults write -g InitialKeyRepeat -int 13 # normal minimum is 15 (225 ms)\ndefaults write -g KeyRepeat -int 1 # normal minimum is 2 (30 ms)\n\n\nA better way to search text: ack\nInstall ack:\nbrew install ack\nSearch files for text, super fast and returns results in a very nice way. By default will search recursively from the current directory and it skips unimportant files by default.\nack \"search string\"\n\n\nKeyboard Tricks (OS X)\nSet your option key to Esc+ in iTerm under Profiles>Keys\n\n\ncontrol-W delete word backwards\noption-D delete word forwards\ncontrol-K delete until end of line\n\n\n\nMy .zshrc file\nStored at ~/.zshrc\nI used to have ohmyzsh but it made my shell too slow. This is good enough for me.\n# #speed startup time https://medium.com/@dannysmith/little-thing-2-speeding-up-zsh-f1860390f92\nautoload -Uz compinit\nfor dump in ~/.zcompdump(N.mh+24); do\n  compinit\ndone\ncompinit -C\n####\n\nPROMPT='%(?.%F{green}√.%F{red}?%?)%f %B%F{157}%1~%f%b %F{231}%# '\n\nautoload -Uz vcs_info\nprecmd_vcs_info() { vcs_info }\nprecmd_functions+=( precmd_vcs_info )\nsetopt prompt_subst\nRPROMPT=\\$vcs_info_msg_0_\nzstyle ':vcs_info:git:*' formats '%F{141}(%b)%r%f'\nzstyle ':vcs_info:*' enable git\n\nalias ls=\"colorls\"\nalias python=\"python3\"\n\n# install jupyter kernel with pipenv\nfunction install-jupyter {\n  if [ -n \"${PIPENV_ACTIVE+1}\" ]; then\n    VENV_NAME=`echo ${VIRTUAL_ENV} | cut -d '/' -f 7`\n    echo \"creating Jupyter kernel named $VENV_NAME\"\n    pipenv install --skip-lock ipykernel\n    python -m ipykernel install --user --name=$VENV_NAME\n  fi\n}\n\n## automatically activate pipenv shell upon cd\nfunction auto_pipenv_shell {\n    if [ ! -n \"${PIPENV_ACTIVE+1}\" ]; then\n        if [ -f \"Pipfile\" ] ; then\n            pipenv shell\n        fi\n    fi\n}\n\nfunction cd {\n    builtin cd \"$@\"\n    auto_pipenv_shell\n}\n\n#extra stuff\nexport CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\nGREP_OPTIONS=\"--color=always\";export GREP_OPTIONS\n__git_files () { \n    _wanted files expl 'local files' _files     \n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html",
    "href": "notes/linux/bash_scripting.html",
    "title": "Cheatsheet",
    "section": "",
    "text": "Link to class.\nLink to GitHub repo\n\n;\n\n\n\nwas originally a program called bin/sh\nBourne Shell: introduced more advanced structure into the shell.\n\nBourne Again Shell (Bash): Second iteration of Bourne Shell.\n\n\n\n\nls -a ~/ | grep bash\n      .bash_history\n      .bash_profile\n      .bash_profile.backup\n      .bash_profile.bensherman\n      .bash_profile_copy\n      .bash_sessions/\n      git-completion.bash\n\n\n\n.bash_profile: executed when you login -> configures the shell when you get an initial command prompt. This is different than .bashrc.\n\ncommonly loads the ~/.bashrc file as well.\nbin is traditionally the folder for binaries.\nbash_profile is designed to run when you login, so if you change it will not refresh until you login next time.\n\n\n\n\n\n.bashrc it is executed simply before the command shell comes up, does not have to wait until you login.\netc/bashrc are system bashrc files which is like a “template” for user bashrc files. Anytime a new user is created, it inherits from this template and sometimes automated customizations are applied. This is usually done by simply importing etc/bashrc from each user’s bashrc file.\nenv will list all env variables.\nto apply .bashrc you just have to run the command bash as it will start another shell from your current one. However, if you run bash you can now exit without closing the shell, because a shell is running inside another shell.\n\n\n\n\n\n~/.bash_history contains lots of history. By default will only capture last 100 but you can change this setting.\n\nyou can exlude something from saving to history (like passwords) by using an ignorespace\nthe environment variable HIST_CONTROL can be used to control how much history to keep and settings about what should not be logged. One way to turn off loggin is: bash     export HISTCONTROL=$HISTCONTROL:ignorespace this allow you to skip logging by adding a space to the the beginning of any command. If you want to see what is in HIST_CONTROL you will see:\n> cat ~/.bash_history | grep HISTCONTROL\nHISTCONTROL=ignoredups:ignorespace\nignoredups was already set to this variable.\n\n\n\n\n\nDoesn’t always exist on a system. in most cases the contents of the ~/.bash_logout will be empty or contain a comment.\nThe role of this file is to execute things when you exit the shell. If you close the shell it will not work, you have to do a clean exit instead.\nCommon use is to use this to clear out ~/.bashrc with the original to clear out any changes the user may have made. You can accomplish this by copying a backup:\ncp ~/.bashrc.original ~/.bashrc\n\n\n\n\n\nPut your shell scripts in a folder you can find them. We can put them in ~/bin:\n> mkdir bin\nMake sure in ~/.bash_profile you have:\nPATH=$PATH:$HOME/bin\nexport PATH\n\n\n\nTo make test.sh executable run command chmod u+x test.sh\n\nYou can also run chmod 755\n\n\n\n\ncan use any name that is not an environment variable (check with env).\nby convention variable names in ALLCAPS. bash     > FIRSTNAME=\"Hamel\"\n  - No space b/w = and value.\n  - Good idea to __always__ put value in double quotes `\"`, although this is not required in every case.  \nAs a practice you want to use export command to set is as an environment variable. This makes the variable available to any subprocess that starts from the shell. Read more about this here.\n> export FIRSTNAME\n> echo \"Hello, $FIRSTNAME\"\n\"Hello Hamel\"\n\n> export FIRSTNAME=\"Hamel\" # do this in one step\nThe above example could work without export, too just reinforcing that its a good idea to use this as a habit. You can do this in one step:\n\n\n\n> export TODAYSDATE=`date`  # executes date command\n\n\n\n\n```bash\nMYUSERNAME='hamel'\nMYPASSWORD='password'\nSTARTOFSCRIPT=`date`\n\necho \"My login name for this app is $MYUSERNAME\"\necho \"My login password for this app is $MYPASSWORD\"\necho \"I started this script at $STARTOFSCRIPT\"\n\nENDOFSCRIPT=`date`\n\necho \"I ended the script at $ENDOFSCRIPT\"\n```\n\nThese variables only live within the sub-shell that executes the script.\n\n\n\n\n\nMethod 1 (Static): Assign command result to variable. Only runs the command at time of variable assignment.\n\n    TODAYSDATE=`date`\n    USERFILES=`find /home -user user` # find all directories owned by the user \"user\"\n\n    echo \"Today's Date: $TODAYSDATE\"\n    echo \"All files owned by USER: $USERFILES\"\n\nMethod 2: Use an alias, which allows you to run a command every time you call the alias. For aliases to work this way you must use the shopt command, which allows aliases to be useable in shell scripts. Technically referred to as “expanding aliases within a subshell”.\n\n    #!/bin/bash\n    shopt -s expand_aliases\n\n    # notice that we don't use backticks here because the command we want to execute is put in \"..\"\n    alias TODAY=\"date\" \n    alias UFILES=\"find /home -user user\"\n\n\n    A=`TODAY` #Executes the command date\n    B=`UFILES`#Executes the command \n    echo \"With Alias, TODAY is: $A\" echo \"With Alias, UFILES is: $B\"\n\n\n\n\nValue = 0 means everything is ok\nValue != 0 means something is wrong.\nSee last exit status w/ the $? command:\n\n    > ls\n    > echo $?\n    0\n\n\n\nUnlike python, shell scripts will continue executing even if there is an error. You can prevent this by using set -e\n\n    set -e # means exit the shell if there is an error, don't continue.\n\n\n\n\n    expr 1 + 2\n    expr 2 \\* 2 # you have to escape the *\n    expr \\( 2 + 2 \\) \\* 4  # you must also escape the ( )\n\nCaveat: You need a space on each side of the operator.\n\n\n\n\n\n\nenv and printenv will tell you your global vars\nset will give you things from your session. This will also usually contain everything from your global scope. set is a superset of env.\nReserved names: see study guide or google it.\n\n\n\nunset MY_VAR\n\n\n\n\n\n$ escapes a single character.\nsingle quotes '..' treats something as a string, escapes the whole thing\ndouble quotes do not escape anything.\n\n> echo \"\\$COL\"  # this will escape the $\n$COL\n\n> echo '$COL' # single quotes escape things, means the literal string\n$COL\n\n> echo \"$COL\" # does not escape anything\n250\n\n> echo \"The date is: `date`\" # command substitution with bacticks\nThe date is Mon Jul 25"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#using-devnull",
    "href": "notes/linux/bash_scripting.html#using-devnull",
    "title": "Cheatsheet",
    "section": "Using dev/null",
    "text": "Using dev/null\nUse dev/null when you want to discard output and don’t want to put in the background. /dev/null is a device, and like everything is a file in linux. Everything you write to dev/null just dissapears.\nFor example:\n#!/bin/bash\n#redirect to dev/null example\n\necho \"This is going to the blackhole.\" >> /dev/null\nNote >> (append) or > (overwrite) will work for dev/null, although out of habit in other scenarios it is better to append when unsure using >>."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#redirect-std-error",
    "href": "notes/linux/bash_scripting.html#redirect-std-error",
    "title": "Cheatsheet",
    "section": "Redirect Std Error",
    "text": "Redirect Std Error\nls -l /bin/usr 2> ls-error.txt"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#redirect-std-out-err-into-one-file",
    "href": "notes/linux/bash_scripting.html#redirect-std-out-err-into-one-file",
    "title": "Cheatsheet",
    "section": "Redirect Std Out & Err into one file",
    "text": "Redirect Std Out & Err into one file\nls  -l /bin/sur > ls-output.txt 2>&1\nShortcut: use &\nls  -l /bin/sur &> ls-output.txt"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#dispose-std-err-output-devnull",
    "href": "notes/linux/bash_scripting.html#dispose-std-err-output-devnull",
    "title": "Cheatsheet",
    "section": "Dispose Std Err output /dev/null",
    "text": "Dispose Std Err output /dev/null\nls -l /bin/sur 2> /dev/null"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#brace-expansion",
    "href": "notes/linux/bash_scripting.html#brace-expansion",
    "title": "Cheatsheet",
    "section": "Brace Expansion",
    "text": "Brace Expansion\n> echo Hello-{Foo,Bar,Baz}-World                             \nHello-Foo-World Hello-Bar-World Hello-Baz-World"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#parameter-expansion-like-coalesce",
    "href": "notes/linux/bash_scripting.html#parameter-expansion-like-coalesce",
    "title": "Cheatsheet",
    "section": "Parameter Expansion, Like Coalesce",
    "text": "Parameter Expansion, Like Coalesce\n{parameter:-word}\nIf parameter is unset (i.e., does not exist) or is empty, this expansion results in the value of word. If parameter is not empty, the expansion results in the value of parameter."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#types-of-variables-1",
    "href": "notes/linux/bash_scripting.html#types-of-variables-1",
    "title": "Cheatsheet",
    "section": "Types of Variables",
    "text": "Types of Variables\n# declare int variable:\n> declare -i NEWVAR=10\n\n# inpsect type of NEWVAR\n> declare -p NEWVAR\ndeclare -i NEWVAR=\"10\"\n\n# declare readonly variable\n> declare -r READONLY=\"This is something we cannot overwrite\"\n\n# try to cancel READONLY type\n> declare +r READONLY\n### will result in an error\nVariables in bash are implicitly typed, the type will be inferred from the value you assign.\n\ndetermine the type of a variable: declare -p $MYVAR\ndeclare variable as integer: bash      declare -i NEWVAR=10\nIf you explicitly declare a variable as an int but assign it to a string, it will implicitly convert the value to 0."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#the-if-statement",
    "href": "notes/linux/bash_scripting.html#the-if-statement",
    "title": "Cheatsheet",
    "section": "The if statement",
    "text": "The if statement\n3\necho “Guess the Secret Number”\necho “======================“\necho “”\necho “Enter a Number Between 1 and 5”\nread GUESS\n\n\nif [ $GUESS -eq 3 ]\n    then\n        echo “You guessed the Correct Number!”\nfi\nTest if a file exists\nFILENAME=$1\necho “Testing for the existence of a file called $FILENAME”\n\nif [ -a $FILENAME ]\n    then\n        echo “$FILENAME does exist!”\nfi\n\n# negation operator \nif [! -a $FILENAME ]\n    then\n        echo “$FILENAME does not exist!”\nfi\n\n# test multiple expressions in if statement\n\nif [ -f $FILENAME ] && [ -R $FILENAME]\n    then\n        echo “File $FILENAME exists and is readable.”\nfi\n-a is the same as -f w.r.t. testing for the existence of a file."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#ifthenelse",
    "href": "notes/linux/bash_scripting.html#ifthenelse",
    "title": "Cheatsheet",
    "section": "If/Then/Else",
    "text": "If/Then/Else\necho “Enter a number between 1 and 3:”\nread VALUE\n\n# use semicolons for readability\nif [ “$VALUE” -eq “1” ]; then\n    echo “You entered $VALUE”\nfi\nUsing an OR statement:\n# another variation\nif [ “$VALUE” -eq “1” ] || [ “$VALUE” -eq “2” ] || [ “$VALUE” -eq “3” ]; then\n    echo “You entered $VALUE”\nelse\n    echo “You didn’t follow directions!”\nfi\nRedirect errors to /dev/null\nif [ “$VALUE” -eq “1” ] 2>/dev/null || [ “$VALUE” -eq “2” ] 2>/dev/null || [ “$VALUE” -eq “3” ] 2>/dev/null; then\n    echo “You entered $VALUE”\nelse\n    echo “You didn’t follow directions!”\nfi\n\nif [ “$VALUE” -eq “1” ] 2>/dev/null; then\n    echo “You entered #1”\nelif “ \"$VAL”E\" -e“ ”2\" ] 2>/dev/null; then\n    ech“ \"You entered ”2\"\nelif “ \"$VAL”E\" -e“ ”3\" ] 2>/dev/null; then\n    ech“ \"You entered ”3\"\nelse\n    ech“ \"You di’n't follow direction”!\"\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#file-expressions",
    "href": "notes/linux/bash_scripting.html#file-expressions",
    "title": "Cheatsheet",
    "section": "File Expressions",
    "text": "File Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#string-expressions",
    "href": "notes/linux/bash_scripting.html#string-expressions",
    "title": "Cheatsheet",
    "section": "String Expressions",
    "text": "String Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#integer-expressions",
    "href": "notes/linux/bash_scripting.html#integer-expressions",
    "title": "Cheatsheet",
    "section": "Integer Expressions",
    "text": "Integer Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#for-loop",
    "href": "notes/linux/bash_scripting.html#for-loop",
    "title": "Cheatsheet",
    "section": "For Loop",
    "text": "For Loop\n#!/bin/bash\necho “List all the shell scripts contents of the directory”\nSHELLSCRIPTS=`ls *.sh`\n\n# alternate using for loop\n\nfor FILE in *.sh; do\n    echo “$FILE”\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#case-statement",
    "href": "notes/linux/bash_scripting.html#case-statement",
    "title": "Cheatsheet",
    "section": "Case Statement",
    "text": "Case Statement\n#!/bin/bash\n\necho “1) Choice 2”\necho “2) Choice 2”\necho “3) Choice 3”\necho “Enter Choice:”\n\nread MENUCHOICE\n\ncase $MENUCHOICE in\n    1)\n        echo “You have choosen the first option”;;\n    2)\n        echo “You have chosen the second option”;;\n    3) \n        echo “You have selected the third option”;;\n    *)\n        echo “You have choosen unwisely”;;\n\nMatch Multiple Case Statements\nAllow many matches to occur"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#while-loop",
    "href": "notes/linux/bash_scripting.html#while-loop",
    "title": "Cheatsheet",
    "section": "While Loop",
    "text": "While Loop\n#!/bin/bash\n\necho “Enter number of times to display message:”\nread NUM\n\nCOUNT=1\n\n# -le means less than or equal to\nwhile [ $COUNT -le $NUM ]\ndo\n    echo “Hello World $COUNT”\n    COUNT=“`expr $COUNT + 1`”\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#asynchronous-execution-with-wait",
    "href": "notes/linux/bash_scripting.html#asynchronous-execution-with-wait",
    "title": "Cheatsheet",
    "section": "Asynchronous Execution with wait",
    "text": "Asynchronous Execution with wait\n\nThis is the most straightforward implementation of async I have ever seen. You basically decide when to block and wait for a process that you previously decided to run in a child process."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#short-circuit-expressions",
    "href": "notes/linux/bash_scripting.html#short-circuit-expressions",
    "title": "Cheatsheet",
    "section": "Short Circuit Expressions",
    "text": "Short Circuit Expressions\n\n&&: command1 && command2:\nonly run command2 if command1 is successful\n\n\n||: command1 || command2:\nonly run command2 if command1 fails"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#reading-files",
    "href": "notes/linux/bash_scripting.html#reading-files",
    "title": "Cheatsheet",
    "section": "Reading Files",
    "text": "Reading Files\necho “Enter a filename” \nread FILE\n\nwhile read -r SUPERHERO; do\n    echo “Superhero Name: $SUPERHERO”\ndone < “$FILE”"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#reading-files-with-loops",
    "href": "notes/linux/bash_scripting.html#reading-files-with-loops",
    "title": "Cheatsheet",
    "section": "Reading Files with loops",
    "text": "Reading Files with loops"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#file-descriptors",
    "href": "notes/linux/bash_scripting.html#file-descriptors",
    "title": "Cheatsheet",
    "section": "File Descriptors",
    "text": "File Descriptors\nUse a number >= 3 for file descriptors.\n0 - stdin 1 - stdout 2 - stderr\n/dev/null -> generic place where you can redirect streams into nothing.\n#!/bin/bash\n\necho “Enter file name: “\nread FILE\n\n# < means readonly,  > means write only,  <> means allow read & write\n# assign file descriptor to filename\nexec 5<>$FILE\n\nwhile read -r SUPERHERO; do\n    echo “Superhero Name: $SUPERHERO”\ndone <&5 #use & to reference the file descriptor\n\n# append to end of file.\necho \"File Was Read On: `date`\" >&5\n\n# close file descriptor\nexec 5>&-"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#delimiters-ifs",
    "href": "notes/linux/bash_scripting.html#delimiters-ifs",
    "title": "Cheatsheet",
    "section": "Delimiters (IFS)",
    "text": "Delimiters (IFS)\nIFS - Internal Field Seperator Default is a space\n# this will return a space\necho $IFS\necho \"Enter filename to parse: \"\nread FILE # spacedelim.txt\n\n# https://stackoverflow.com/questions/24337385/bash-preserve-string-with-spaces-input-on-command-line\n\nwhile read -r CPU MEM DISK; do\n    echo \"CPU: $CPU\"\n    echo \"Memory: $MEM\"\n    echo \"Disk: $DISK\"\ndone <\"$FILE\""
  },
  {
    "objectID": "notes/linux/bash_scripting.html#traps-and-signals",
    "href": "notes/linux/bash_scripting.html#traps-and-signals",
    "title": "Cheatsheet",
    "section": "Traps and Signals",
    "text": "Traps and Signals\nhttps://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html - cntrl+c = SIGINT - cntrl+z = SIGTSTP - kill command (without -9 flag) = SIGTERM - kill -9 = SIGKILL; this signal is not sent to the process, it is just killed.\nclear\n\n# first argument is what to exexute \ntrap 'echo \" - Please Press Q to Exit.\"' SIGINT SIGTERM SIGTSTP\n\n# cntrl+c = SIGINT\n# cntrl+z = SIGTSTP  (Suspend, send to background)\n\n\n\nwhile [ \"$CHOICE\" != \"Q\" ] && [ \"$CHOICE\" != \"q\" ]; do\n    echo \"Main Menu\"\n    echo \"=======\"\n    echo \"1) Choice One\"\n    echo \"2) Choice Two\"\n    echo \"3) Choice Three\"\n    echo \"Q) Quit\"\n    read CHOICE\n\n    clear\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#structure-of-functions-in-a-shell-script",
    "href": "notes/linux/bash_scripting.html#structure-of-functions-in-a-shell-script",
    "title": "Cheatsheet",
    "section": "structure of functions in a shell script",
    "text": "structure of functions in a shell script\nUnlike python, you must define your functions before you call them."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#scope",
    "href": "notes/linux/bash_scripting.html#scope",
    "title": "Cheatsheet",
    "section": "Scope",
    "text": "Scope\nsetting a variable within a function defines that variable globally after that function is called!!!\nGLOBALVAR=“Globally Visible”\n\n# sample function for function variable scope\nfuncExample () {\n    # local\n    LOCALVAR=“Locally Visible”\n\n    echo “From within the function, the variable’s value is set to $LOCALVAR …”\n}\n\n# script start\n\necho “this happens before the function call”\necho “”\necho “Local Variable = $LOCALVAR after the function call.”\necho “Global Variable = $GLOBALVAR (before the function call).”\n\nfuncExample\n\necho “this happens after the function call”\necho “Local Variable = $LOCALVAR after the function call.”\necho “Global Variable = $GLOBALVAR (before the function call).”\nOutput of above code:\n ./scope.sh\nthis happens before the function call\n\nLocal Variable =  after the function call.\nGlobal Variable = Globally Visible (before the function call).\nFrom within the function, the variable’s value is set to Locally Visible …\nthis happens after the function call\nLocal Variable = Locally Visible after the function call.\nGlobal Variable = Globally Visible (before the function call)."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#functions-with-parameters",
    "href": "notes/linux/bash_scripting.html#functions-with-parameters",
    "title": "Cheatsheet",
    "section": "Functions With Parameters",
    "text": "Functions With Parameters\n# global\nUSERNAME=$1\n\nfuncAgeInDays () {\n    echo “Hello $USERNAME, You are $1 Years old.”\n    echo “That makes you approx `expr 365 \\* $1` days old”\n}\n\n#script - start\nread -r -p “Enter your age:” AGE\n\n# pass in arguments like this\nfuncAgeInDays $AGE"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#nested-functions",
    "href": "notes/linux/bash_scripting.html#nested-functions",
    "title": "Cheatsheet",
    "section": "Nested Functions",
    "text": "Nested Functions\nAuthor of course uses this for organization purposes. When you call a function if it has nested functions the functions defined within will be exposed to the script also.\n# global\nGENDER=$1\n\nfuncHuman () {\n    ARMS=2\n    LEGS=2\n\n    funcMale () {\n        BEARD=1\n        echo “This man has $ARMS arms and $LEGS legs with $BEARD beard”\n    }\n\n    funcFemale () {\n        BEARD=0\n        echo “This woman has $ARMS arms and $LEGS legs with $BEARD beard”\n    }\n}\n\n# script start\nclear\n\n# determine the actual gender and display the characteristics.\nif  [ “$GENDER” == “male” ]; then\n    funcHuman\n    funcMale # this function is available after the parent function is called.\nelse\n    funcHuman\n    funcFemale\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#function-return-and-exit",
    "href": "notes/linux/bash_scripting.html#function-return-and-exit",
    "title": "Cheatsheet",
    "section": "Function Return and Exit",
    "text": "Function Return and Exit\nThis allows you to get arguments from the command line and then exit with a proper code and also use function returns inside scripts.\n# demo of return values and testing results\n\nYES=0\nNO=1\nFIRST=$1\nSECOND=$2\nTHIRD=$3\n\n# function definitions\n\nfuncCheckParams () {\n    # did we get three\n    # -z equivalent to isnull (in this case means not-null b/c of !)\n    if [ ! -z “$THIRD” ]; then\n        echo “We got three params”\n        return $YES\n    else\n        echo “We did not get three params”\n        return $NO\n    fi\n}\n\n# script start\n\nfuncCheckParams\n# the return value from the function gets stored in $?\nRETURN_VALS=$?\n\nif [ “$RETURN_VALS” -eq “$YES” ]; then\n    echo “We received three params and they are:”\n    echo “Param 1: $FIRST”\n    echo “Param 2: $SECOND”\n    echo “Param 3: $THIRD”\nelse\n    echo “Usage: funcreturn.sh [param1] [param2] [param3]”\n    exit 1\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#infobox",
    "href": "notes/linux/bash_scripting.html#infobox",
    "title": "Cheatsheet",
    "section": "Infobox",
    "text": "Infobox\nDissappears unless you sleep (see below). Does not come with any buttons.\n# globals\nINFOBOX=${INFOBOX=dialog}\nTITLE=“Default”\nMESSAGE=“Something to say”\nXCOORD=10\nYCOORD=20\n\nfuncDisplayInfoBox () {\n    $INFOBOX —title “$1” —infobox “$2” “$3” “$4”\n    sleep “$5”\n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#msgbox",
    "href": "notes/linux/bash_scripting.html#msgbox",
    "title": "Cheatsheet",
    "section": "Msgbox",
    "text": "Msgbox\nMsgbox - dissapears unless you sleep pass --msgbox argument, comes with default ok button and stays on screen.\n# global\nMSGBOX=${MSGBOX=dialog}\nTITLE=“Default”\nMESSAGE=“Some Message”\nXCOORD=10\nYCOORD=20\n\nfuncDisplayMsgBox () {\n    $MSGBOX —title “$1” —msgbox “$2” “$3” “$4”\n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#menus",
    "href": "notes/linux/bash_scripting.html#menus",
    "title": "Cheatsheet",
    "section": "Menus",
    "text": "Menus\nSee pdf notes/scripts"
  },
  {
    "objectID": "notes/linux/permprocdata.html",
    "href": "notes/linux/permprocdata.html",
    "title": "Processes, Permissions and Moving Data",
    "section": "",
    "text": ";"
  },
  {
    "objectID": "notes/linux/permprocdata.html#references",
    "href": "notes/linux/permprocdata.html#references",
    "title": "Processes, Permissions and Moving Data",
    "section": "References",
    "text": "References\nFiles associated with this tutorial can be found here."
  },
  {
    "objectID": "notes/linux/permprocdata.html#managing-processes-ps-kill-pkill",
    "href": "notes/linux/permprocdata.html#managing-processes-ps-kill-pkill",
    "title": "Processes, Permissions and Moving Data",
    "section": "Managing Processes (ps, kill, pkill)",
    "text": "Managing Processes (ps, kill, pkill)\n\nKill Single Process (ps, kill)\nA common scenario is that you might run a python script to train a model:\n$ python train.py\nLet’s say you want to kill this script for whatever reason. You might not always be able to type Cntrl + C to stop it, especially if this process is running in the background. (Aside: A way make a program run in the background is with a & for example:$ python train.py & )\nIn order to find this running program, you can use the command ps\n$ ps Gives you basic information (good enough most of the time)\nFlags:\n\n-e Allows you to see all running processes including from other users\n-f Allows you to see additional information about each process\n\nIn order to kill the process you will want to identify it’s PID for example, if the PID is 501 you can kill this process with the command:\n$ kill 501\n\n\nKilling Multiple Processes (pkill)\nIf you use process-based threading in python with a library like multi-processing, python will instantiate many processes for you. This is common thing to do in python for a task like data processing.\nLet’s consider the below example. When you run this in the background it will produce 8 processes:\nfrom multiprocessing import Pool\nfrom time import sleep\n\ndef f(x):\n    sleep(1000) # simulate some computation\n    return x*x\n\nif __name__ == '__main__':\n    with Pool(8) as p:\n        print(p.map(f, range(8)))\n\n$ python train_multi.py &\n\nAfter a few seconds, calling the command ps will yield something like this:\nPID TTY           TIME CMD\n 3982 ttys002    0:00.09 ...MacOS/Python train_multi.py\n 4219 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4220 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4221 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4222 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4223 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4224 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4225 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4226 ttys002    0:00.00 ...MacOS/Python train_multi.py\nYou can find all processes with the file train_multi.py with the pkill command and the -f flag:\n\n\n\nSee Parent / Child Processes (pstree)\npstree is also a helpful utility to see parent/child relationships between processes. You can install pstree on a mac with brew install pstree\nIn the above example, there are 8 sub-processes created by one python process. Running the command\n$ pstree -s train_multi.py\nWill show the process hierarchy. The -s flag allows you to filter parents and descendants of processes containing a string in their command. In the below example, PID 41592 will kill all the 8 child processes seen below\n\n\n\nKilling Process Options\n\nReminder: view processes with ps or top To show processes from all users ps aux\n\nTo restart pid 6996 kill -1 6996\nkill pid 6996 kill -9 6996\n\nYou can kill processes by name (which is also usually listed as the command that started the processes). killall will search for the string int he relevant process.\n\n\nBringing processes back into the foreground\nReminder you put processes in the background with & example is myscript.sh &\nYou can move processes back into the foreground with fg\nfg 1234 brings process 1234 back into the foreground."
  },
  {
    "objectID": "notes/linux/permprocdata.html#bundling-archiving-files-tar",
    "href": "notes/linux/permprocdata.html#bundling-archiving-files-tar",
    "title": "Processes, Permissions and Moving Data",
    "section": "Bundling & Archiving Files (tar)",
    "text": "Bundling & Archiving Files (tar)\nYou commonly want to package a bunch of files together, such as a collection of photos or CSVs, and optionally compress these with its directory structure intact. A common tool for this is tar . This is how you would bundle and compress a directory of CSV files:\n\n\nSending An Archive To A Remote Machine\nIt is often the case you want to send data to a remote machine. The below command creates a directory called data , compresses all files in a local folder named csv_data , with the exception of the sub-directory csv_data/intermediate_files without creating any temporary files locally:\nOptionally, create the directory on the remote machine:\n\nThen, stream the archive directly to remote. Note that providing a — instead of a destination filename allows tar to write to a stream (stdout) that can be sent directly to the remote server.\n\n\n\nMoving Files In Different Directories Into An Archive\nIf your files exist in sibling directories, rather than under one parent directory you can use find along with tar . Suppose you want to archive all csv files relative to a directory:\n\nWhen you archive files on the fly above with find you cannot compress the files until the archive is finished being built, therefore you have to compress the tar file with the gzip command:\n\n$ gzip data.tar\n\nTip: some people like to use locate with updatedb instead of find. There are tradeoffs so make sure you read the documentation carefully!\n\n\nUnpacking & Decompressing Archives\nYou can decompress and unpack a tar file, for example data.tar.gz with the following command:\n\n$ tar -xzvf data.tar.gz\n\nIf the data is not compressed, you can leave out the -z flag:\n\n$ tar -xvf data.tar"
  },
  {
    "objectID": "notes/linux/permprocdata.html#file-permissions",
    "href": "notes/linux/permprocdata.html#file-permissions",
    "title": "Processes, Permissions and Moving Data",
    "section": "File Permissions",
    "text": "File Permissions\nBefore we begin, we must introduce some nomenclature:\n\nIf you run the command ls -a you will see something similar to the below output for all of your files in the current directory.\n\nThe file permissions are shown in three-character groupings for three different groups (nine characters total). These three groups are the owner , group , and other users. In this case, the owner name is hamel and the group name is staff\nFor the owner, the file permissions are rwx which means that the owner has read r , write w , and execute x permissions.\nFor the group, the file permissions are r-x which means the group has read and execute permissions, but not write permissions. A group is a collection of users with common permissions.\nFinally, all other users have file permissions of r– which means only read permissions.\n\nChanging File Permissions\nThere are several ways to change file permissions.\nMethod 1: Using Characters and +, -\nRefer to the nomenclature above to follow along\n\nchmod o-r csvfiles.tar.gz\nRemoves - the ability of other users o to read r the file.\nchmod g+w csvfiles.tar.gz\nAdds + the ability of the group g to write w to the file.\nchmod u+x csvfiles.tar.gz\nAdds + the ability of the owner u to execute x the file.\nchomd a+x csvfiles.tar.gz\nAdds + the ability of all users a to execute x the file.\n\nMethod 2: using numbers\nThis method works by adding up the numbers corresponding to the permissions separately for each user group (owner, group, others). For example:\n\nchmod 777 csvfiles.tar.gz\nThis gives all users the ability to read (4), write( 2), and execute (1) files. In other words 4+2+1 = 7, for the owner, group and other users.\nchmod 732 csvfiles.tar.gz\nThis gives the owner the ability to read, write and execute ( 4+2+1=7), the group the ability to write and execute (2+1=3) and all other users only the ability to write (2).\n\n\n\nChanging Ownership\nYou can change the owner or group assigned to a file like this:\nchown newuser:newgroup file\nThe :newgroup is optional, if you do not specify that the group will stay the same."
  },
  {
    "objectID": "notes/linux/cookbook.html",
    "href": "notes/linux/cookbook.html",
    "title": "Cookbook",
    "section": "",
    "text": "You should browse the table of contents of this book and use the shell scripts contained within off the shelf if possible.\n\nGitHub: https://github.com/hamelsmu/wicked_cool_shell_scripts_2e/\nLink to book on GitHub: https://github.com/hamelsmu/wicked_cool_shell_scripts_2e/blob/master/WickedCoolShellScripts2E.pdf\nBook: https://nostarch.com/wcss2"
  },
  {
    "objectID": "notes/linux/cookbook.html#shift-and-pop-args-off-and-count-args",
    "href": "notes/linux/cookbook.html#shift-and-pop-args-off-and-count-args",
    "title": "Cookbook",
    "section": "shift and $# pop args off and count args",
    "text": "shift and $# pop args off and count args\nshift.sh\n#!/bin/bash\nwhile (( $# )); do\n    echo \"process args: $1\"\n    shift\ndone\nResults in:\n$ ./shift.sh foo bar bash                                                                             \nprocess args: foo\nprocess args: bar\nprocess args: bash\n\nUsing shift for CLI options:\n#!/bin/bash\n# newquota--A frontend to quota that works with full-word flags a la GNU\n\n# quota has three possible flags, -g, -v, and -q, but this script\n#   allows them to be '--group', '--verbose', and '--quiet' too:\n\nflags=\"\"\nrealquota=\"$(which quota)\"\n\nwhile [ $# -gt 0 ]\ndo\n  case $1\n  in\n    --help)  echo \"Usage: $0 [--group --verbose --quiet -gvq]\" >&2\n                       exit 1 ;;\n    --group )  flags=\"$flags -g\";       shift ;;\n    --verbose)  flags=\"$flags -v\";   shift ;;\n    --quiet)  flags=\"$flags -q\";       shift ;;\n    --)  shift;           break ;;\n    *)  break;          # done with 'while' loop!\n  esac\ndone\n\nexec $realquota $flags \"$@\""
  },
  {
    "objectID": "notes/linux/cookbook.html#collect-all-arguments",
    "href": "notes/linux/cookbook.html#collect-all-arguments",
    "title": "Cookbook",
    "section": "$* collect all arguments",
    "text": "$* collect all arguments\nshift2.sh\n#!/bin/bash\nfor var in $*; do\n    echo $var\ndone\nResults in:\n$ ./shift2.sh foo bar bash                                                                             \nprocess args: foo\nprocess args: bar\nprocess args: bash"
  },
  {
    "objectID": "notes/linux/cookbook.html#multi-option-case-statement",
    "href": "notes/linux/cookbook.html#multi-option-case-statement",
    "title": "Cookbook",
    "section": "Multi Option Case Statement",
    "text": "Multi Option Case Statement\nwhile read command args\ndo\n  case $command\n  in\n    quit|exit) exit 0                                  ;;\n    help|\\?)   show_help                               ;;\n    scale)     scale=$args                             ;;\n    *)         scriptbc -p $scale \"$command\" \"$args\"  ;;\n  esac\n\n  /bin/echo -n \"calc> \"\ndone\n\nAnother example of case statement\n  case $1 in\n    1 ) month=\"Jan\"    ;;  2 ) month=\"Feb\"    ;;\n    3 ) month=\"Mar\"    ;;  4 ) month=\"Apr\"    ;;\n    5 ) month=\"May\"    ;;  6 ) month=\"Jun\"    ;;\n    7 ) month=\"Jul\"    ;;  8 ) month=\"Aug\"    ;;\n    9 ) month=\"Sep\"    ;;  10) month=\"Oct\"    ;;\n    11) month=\"Nov\"    ;;  12) month=\"Dec\"    ;;\n    * ) echo \"$0: Unknown numeric month value $1\" >&2; exit 1\n  esac\n  return 0"
  },
  {
    "objectID": "notes/linux/cookbook.html#collecting-stdout-with--",
    "href": "notes/linux/cookbook.html#collecting-stdout-with--",
    "title": "Cookbook",
    "section": "Collecting stdout with -",
    "text": "Collecting stdout with -\necho \"Enter something: \" | cat -"
  },
  {
    "objectID": "notes/linux/cookbook.html#formatting-long-lines-fmt",
    "href": "notes/linux/cookbook.html#formatting-long-lines-fmt",
    "title": "Cookbook",
    "section": "Formatting Long Lines fmt",
    "text": "Formatting Long Lines fmt\nWill make lines no longer than 30 characters, not cutting off any words.\nfmt -w30 long_text.txt"
  },
  {
    "objectID": "notes/linux/cookbook.html#ifs---internal-field-seperator",
    "href": "notes/linux/cookbook.html#ifs---internal-field-seperator",
    "title": "Cookbook",
    "section": "IFS - Internal Field Seperator",
    "text": "IFS - Internal Field Seperator\nSets the internal delimiter\nifs_variable.sh\n#!/bin/bash\nIFS=\":\"\nvar='a:b-c~d'\nfor n in $var\ndo\n    echo \"$n\"\ndone\nResults in\n$ ./1/ifs_variable.sh\na\nb-c~d\n\nIFS in Great Expectations Action\nI’m using this in the Great Expectations Action to parse a list of arguments given as a string to an input\n# Loop through checkpoints\nSTATUS=0\nIFS=','\nfor c in $INPUT_CHECKPOINTS;do\n    echo \"\"\n    echo \"Validating Checkpoint: ${c}\"\n    if ! great_expectations checkpoint run $c; then\n        STATUS=1\n    fi\ndone\n\n\nIFS for iterating through $PATH\n#!/bin/bash\nIFS=\":\"\nfor directory in $PATH ; do\n   echo $directory\ndone\n\n\nIFS: Double vs. Single Quotes\nWith double quotes the outcome of the command expansion would be fed as one parameter to the source command. Without quotes it would be broken up into multiple parameters, depending on the value of IFS which contains space, TAB and newline by default.\nvar=\"some value\"\n\n# $var fed into cmd as one parameter\ncmd \"$var\"\n\n# $var is fed into cmd as two parameters\n#  delimted by the default IFS character, space\ncmd '$var'"
  },
  {
    "objectID": "notes/linux/cookbook.html#random",
    "href": "notes/linux/cookbook.html#random",
    "title": "Cookbook",
    "section": "$RANDOM",
    "text": "$RANDOM\necho $RANDOM will print out a random number"
  },
  {
    "objectID": "notes/linux/cookbook.html#debugging-shell-scripts--x",
    "href": "notes/linux/cookbook.html#debugging-shell-scripts--x",
    "title": "Cookbook",
    "section": "Debugging Shell Scripts -x",
    "text": "Debugging Shell Scripts -x\nDebug a script:\nbash -x myscript.sh\nOR, within a script:\nset -x # start debugging\n./myscript.sh\nset +x # stop debugging\nAll variables will be substituted and lines that are run will be printed to screen, showing the control flow of the program"
  },
  {
    "objectID": "notes/linux/cookbook.html#sourcing-files-with-.",
    "href": "notes/linux/cookbook.html#sourcing-files-with-.",
    "title": "Cookbook",
    "section": "Sourcing files with .",
    "text": "Sourcing files with .\nSo you can “import” scripts\n. myscript.sh\n# is equivalent to\nsource myscript.sh"
  },
  {
    "objectID": "notes/linux/cookbook.html#using-functions-to-set-exit-codes",
    "href": "notes/linux/cookbook.html#using-functions-to-set-exit-codes",
    "title": "Cookbook",
    "section": "Using functions to set exit codes",
    "text": "Using functions to set exit codes\n\nvalidAlphaNum()\n{\n  # Validate arg: returns 0 if all upper+lower+digits, 1 otherwise.\n  # Remove all unacceptable chars.\n  validchars=\"$(echo $1 | sed -e 's/[^[:alnum:]]//g')\"\n\n  if [ \"$validchars\" = \"$1\" ] ; then\n    return 0\n  else\n    return 1\n  fi\n}\n\nexit validAlphaNum"
  },
  {
    "objectID": "notes/linux/cookbook.html#know-if-someone-running-the-script-directly-with-bash_source",
    "href": "notes/linux/cookbook.html#know-if-someone-running-the-script-directly-with-bash_source",
    "title": "Cookbook",
    "section": "Know if someone running the script directly with $BASH_SOURCE",
    "text": "Know if someone running the script directly with $BASH_SOURCE\nThe variable $BASH_SOURCE can let you differentiate between when a script is run standalone vs when its invoked from another script:\nif [ \"$BASH_SOURCE\" = \"$0\" ]"
  },
  {
    "objectID": "notes/linux/cookbook.html#xargs",
    "href": "notes/linux/cookbook.html#xargs",
    "title": "Cookbook",
    "section": "xargs",
    "text": "xargs\nhttps://www.cyberciti.biz/faq/linux-unix-bsd-xargs-construct-argument-lists-utility/\n> echo 1 2 3 4 | xargs -n2 -I {} echo hello {} world                                                                                                                                                                                                                                                   \nhello 1 2 world\nhello 3 4 world"
  },
  {
    "objectID": "notes/linux/misc_utils.html",
    "href": "notes/linux/misc_utils.html",
    "title": "Misc Utilities",
    "section": "",
    "text": ";"
  },
  {
    "objectID": "notes/linux/misc_utils.html#history",
    "href": "notes/linux/misc_utils.html#history",
    "title": "Misc Utilities",
    "section": "History",
    "text": "History\n\nSee history with history command\nYou will get a number for each history item.\n\nYou can replay any number n with command !n\nHistory on OS X is stored in ~/.zsh_history\n\n!n refer to command number n in history when you call history"
  },
  {
    "objectID": "notes/linux/misc_utils.html#diff",
    "href": "notes/linux/misc_utils.html#diff",
    "title": "Misc Utilities",
    "section": "Diff",
    "text": "Diff\nYou can difff two files, you usually want to see a unified diff b/c that is easier to read\ndiff -u file1.txt file2.txt"
  },
  {
    "objectID": "notes/linux/misc_utils.html#here-documents",
    "href": "notes/linux/misc_utils.html#here-documents",
    "title": "Misc Utilities",
    "section": "Here Documents",
    "text": "Here Documents\nInstead of using echo, our script now uses cat and a here document. The string EOF (meaning end of file, a common convention) was selected as the token and marks the end of the embedded text. Note that the token must appear alone and that there must not be trailing spaces on the line.\n\nUnlike Echo, all double quotes and single quotes are escaped. Here is an example of the same thing at the command line.\n[me@linuxbox ~]$ foo=\"some text\"\n[me@linuxbox ~]$ cat << _EOF_\n> $foo\n> \"$foo\"\n> '$foo'\n> \\$foo\n> _EOF_ \nsome text \n\"some text\" \n'some text' \n$foo"
  },
  {
    "objectID": "notes/linux/misc_utils.html#mounting-devices",
    "href": "notes/linux/misc_utils.html#mounting-devices",
    "title": "Misc Utilities",
    "section": "Mounting devices",
    "text": "Mounting devices\nSometimes you need to mount these devices. Two common mount points are /mnt and /media. If you mount the device into an existing directory it will cover the contents of that directory making them invisible and unavailable.\nEx: mount device to /mnt\nmount /dev/sb1 /mnt\nEx: mount flash drive\nmount /dev/sdc1 /media\nYou can unmount a device with unmount:\nunmount /dev/sb1"
  },
  {
    "objectID": "notes/linux/misc_utils.html#getting-information-on-mounted-drives",
    "href": "notes/linux/misc_utils.html#getting-information-on-mounted-drives",
    "title": "Misc Utilities",
    "section": "Getting information on mounted drives",
    "text": "Getting information on mounted drives\ndf -h"
  },
  {
    "objectID": "notes/linux/misc_utils.html#permanently-deleting-files-with-shred",
    "href": "notes/linux/misc_utils.html#permanently-deleting-files-with-shred",
    "title": "Misc Utilities",
    "section": "Permanently deleting files with shred",
    "text": "Permanently deleting files with shred\nThis utility writes over files many times in order to erase things. Helpful for sensitive data."
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html",
    "href": "notes/web-scraping/browser-to-python.html",
    "title": "Browser requests to code",
    "section": "",
    "text": "I learned this from Zachary Blackwood’s 2022 NormConf Talk."
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "href": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "title": "Browser requests to code",
    "section": "Example: Get A List of Subway Restaurants With Python",
    "text": "Example: Get A List of Subway Restaurants With Python\n\nGo to https://www.subway.com/en-US/locator in Google Chrome\n\n\n\nOpen developer tools using Option + CMD + I\nGo the the network tab, and hit the clear button\n\n\n\nType in a zipcode and search. Look for a network request that seems like it is getting data, in this case GetLocations.ashx... looks super promising.\n\n\n\nRight click on that particular event and select Copy -> Copy as Curl\n\n\n\nGo to curlconverter.com and paste the curl command there.\n\n\nEnjoy your python code that uses this otherwise undocumented API :)"
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "href": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "title": "Browser requests to code",
    "section": "Bonus: Parse The Response",
    "text": "Bonus: Parse The Response\nYou can parse the response data in a hacky way.\n\n# run the code from curlconverter.com, which will give you a `response` object.\n\n>>> import json\n... response_string = response.text\n... json_string = response_string[response_string.index(\"(\") +1:response_string.index('\"AdditionalData\":')-1]+'}'\n... parsed_string = json.loads(json_string)\n... stores = parsed_string['ResultData']\n\n>>> stores\n[{'LocationId': {'StoreNumber': 21809, 'SatelliteNumber': 0},\n  'Address': {'Address1': '4888 NW Bethany Blvd',\n   'Address2': 'Suite K-1',\n   'Address3': 'Bethany Village Centre',\n   'City': 'Portland',\n   'StateProvCode': 'OR',\n   'PostalCode': '97229',\n   'CountryCode': 'US',\n   'CountryCode3': 'USA'},\n  'Geo': {'Latitude': 45.5548,\n   'Longitude': -122.8358,\n   'TimeZoneId': 'America/Los_Angeles',\n   'CurrentUtcOffset': 0},\n  'ListingNumber': 1,\n  'OrderingUrl': 'http://order.subway.com/Stores/Redirect.aspx?s=21809&sa=0&f=r&scc=US&spc=OR',\n  'CateringUrl': 'https://www.ezcater.com/catering/pvt/subway-portland-nw-bethany-blvd',\n  'ExtendedProperties': None},\n..."
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "href": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "title": "Browser requests to code",
    "section": "When to use this approach",
    "text": "When to use this approach\nThis is great for adhoc things, but you probably want to use a headless browser and actually scrape the HTML if you want to do this in a repeatable way. But many times you want to do a one-off scrape, this isn’t so bad!"
  },
  {
    "objectID": "notes/video_editing.html",
    "href": "notes/video_editing.html",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c–SNKiifuP8kEFh80wPu\nCMD + B -> “Blade” CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS"
  },
  {
    "objectID": "notes/video_editing.html#other-tools-to-look-into",
    "href": "notes/video_editing.html#other-tools-to-look-into",
    "title": "Video Editing",
    "section": "Other tools to look into",
    "text": "Other tools to look into\n\nDescript\nRunwayML\ncapcut - from Rajeev\nAdobe Premiere\nFrame - Video collaboration that you use for Upwork etc\nEpidemic Sound - Sound by mood (Sanyam)\nCayla - Artlist\nCayla - Premium Beat\n\nCayla recommmends 1080p / 24 FPS for Youtube"
  },
  {
    "objectID": "notes/quarto/listings-from-data.html",
    "href": "notes/quarto/listings-from-data.html",
    "title": "Listings from data",
    "section": "",
    "text": "You don’t need to have blog posts to create a listing on a Quarto page. For example, you can combine the following three yaml files:\nThis file specifies a list of blog posts that you can have elsewhere\nIn the front matter of any page (like index.qmd) you can reference blogs.yml and _metadata.yml like so:"
  },
  {
    "objectID": "notes/quarto/listings-from-data.html#results",
    "href": "notes/quarto/listings-from-data.html#results",
    "title": "Listings from data",
    "section": "Results",
    "text": "Results\nThis will generate a list of blog posts that you can see here on my page, this is in the table format. However, you can have pictures on your listing as well, which you can see from the Minimal Example."
  },
  {
    "objectID": "notes/quarto/listings-from-data.html#minimal-example",
    "href": "notes/quarto/listings-from-data.html#minimal-example",
    "title": "Listings from data",
    "section": "Minimal Example",
    "text": "Minimal Example\nHere is a minimal example of creating an index page of all your blog posts. It uses slightly different options than I did in the above example. You can see the code for that here."
  },
  {
    "objectID": "notes/quarto/listings-from-data.html#resources",
    "href": "notes/quarto/listings-from-data.html#resources",
    "title": "Listings from data",
    "section": "Resources",
    "text": "Resources\n\nQuarto listings\nQuarto shared metadata"
  },
  {
    "objectID": "notes/quarto/merging.html",
    "href": "notes/quarto/merging.html",
    "title": "Merge listings",
    "section": "",
    "text": "You can now merge listings by referencing multiple directories or files in the front matter. This allows you to create a single listing of all your external blogs you may write on other platforms, Medium, substack, your work blog, etc, with your own blogs.\nThe kind folks at Quarto have made an update in their latest pre-release that allows you to merge multiple listings like this:\nThe blog_data/blogs.yml is a listing from data while blog/posts is a Quarto posts directory of blog posts.\nSee source code for my blog for an example."
  },
  {
    "objectID": "notes/quarto/merging.html#resources",
    "href": "notes/quarto/merging.html#resources",
    "title": "Merge listings",
    "section": "Resources",
    "text": "Resources\n\nQuarto listings\nQuarto blog posts"
  },
  {
    "objectID": "notes/quarto/highlighting.html",
    "href": "notes/quarto/highlighting.html",
    "title": "Syntax Highlighting",
    "section": "",
    "text": "Syntax highlighting in Quarto follows the way pandoc handles syntax highlighting. There are two important concepts concerning syntax highlighting:"
  },
  {
    "objectID": "notes/quarto/highlighting.html#syntax-color-themes",
    "href": "notes/quarto/highlighting.html#syntax-color-themes",
    "title": "Syntax Highlighting",
    "section": "1. Syntax Color Themes",
    "text": "1. Syntax Color Themes\nSyntax color themes allow you to customize the colors shown in syntax highlighting. These are expressed with the highlight-style setting. You can change the syntax color theme in either your front matter or site-wide in _quarto.yml like this:\n\nFront Matter_quarto.yml\n\n\n---\nhighlight-style: custom.theme\n---\nIf you have both light and dark themes, you will likely want to set those separately like this:\n---\nhighlight-style:\n  light: custom-light.theme\n  dark: custom-dark.theme\n---\n\n\n\n\n_quarto.yml\n\nformat:\n  html:\n    theme: \n      light: assets/ek-theme-light.scss\n      dark: assets/ek-theme-dark.scss\n    highlight-style: \n      light: assets/ek-light.theme\n      dark: assets/ek-dark.theme\n\n\n\n\nThese color themes are defined by json files with the schema defined here. However, it is recommended that you choose one of the themes that quarto already provides and edit that. By default, Quarto uses the arrow-light theme. This means if you are happy with the way Quarto is highlighting syntax, you can just tweak this theme. Personally, my favorite theme is dracula. It is useful to look through these different themes to get a sense of the types of things you can change."
  },
  {
    "objectID": "notes/quarto/highlighting.html#syntax-definitions",
    "href": "notes/quarto/highlighting.html#syntax-definitions",
    "title": "Syntax Highlighting",
    "section": "2. Syntax Definitions",
    "text": "2. Syntax Definitions\nSyntax definitions define the rules by which syntax is highlighted. A rule is a string, character or regular expression against which to match the text being analyzed. This is helpful if you need to document a language that isn’t supported by Quarto out of the box. You can see the list of supported languages with this command:\nquarto pandoc --list-highlight-languages\nSyntax definitions are defined in xml files that follow this schema. Examples of syntax definitions for various languages can be found here.\nQuarto has additional example syntax definitions here which are useful to look at. Note how the name of the language and its file extensions are defined in the XML file.\nIn order to supply an additional syntax definition or override an existing one, set the syntax-definitions in your _quarto.yml file like this:\n\n\n_quarto.yml\n\nformat:\n  html:\n    syntax-definitions: \n    - new_language.xml\n\nAn example of defining a new language is illustrated below.\n\nExample\nSuppose you have a new programming language called Fomo that is just like Python, except you can define functions with fomo in addition to def. For example, consider this Python code:\ndef hello_world():\n    \"An example\"\n    pass\nUnfortunately, If you try to use the Python code fence for Fomo it looks like this:\nfomo hello_world():\n    \"An example\"\n    pass\nSince Fomo is almost identical to Python, you can start by copying the python syntax definition into a file named fomo.xml and edit the language name, style and extension fields like so:\n\n\nfomo.xml\n\n- <language name=\"Python\" version=\"26\" style=\"python\" indenter=\"python\" kateversion=\"5.0\" section=\"Scripts\" extensions=\"*.py;*.pyw;*.pyi;SConstruct;SConscript;*.FCMacro\" ...\n+ <language name=\"Fomo\" version=\"26\" style=\"fomo\" indenter=\"python\" kateversion=\"5.0\" section=\"Scripts\" extensions=\"*.fomo\" ...\n\nYou also have to add fomo to the list of defs like this:\n\n\nfomo.xml\n\n        <list name=\"defs\">\n            <item>class</item>\n            <item>def</item>\n+           <item>fomo</item>\n            <item>del</item>\n            <item>global</item>\n            <item>lambda</item>\n            <item>nonlocal</item>\n        </list>\n\nAfter that, you can add the Fomo syntax definition to your Quarto project with the syntax-definitions option like so:\n\n\n_quarto.yml\n\nformat:\n  html:\n    syntax-definitions: \n    - fomo.xml\n\nIn this case, fomo.xml is in the root of the Quarto project, but you can put it in a sub-folder as well.\nAfter doing this, you can use the ```fomo code fence, and your code will be highlighted correctly!\nfomo hello_world():\n    \"An example\"\n    pass"
  },
  {
    "objectID": "notes/jupyter/remote_browser.html",
    "href": "notes/jupyter/remote_browser.html",
    "title": "Remote Browser For Jupyter",
    "section": "",
    "text": "It is very common to connect to a remote Jupyter server with your local browser. However, if you lose connection with your remote server, logs printed to the screen may stop streaming. This is common when training deep learning models where training runs can last days or weeks where progress bars are printed to the screen in a notebook.\nTo avoid the issue with your browser loosing connection you can run the browser remotely on the same machine as the Jupyter server, even if your remote server does not have a desktop/GUI interface."
  },
  {
    "objectID": "notes/jupyter/remote_browser.html#fast.ai",
    "href": "notes/jupyter/remote_browser.html#fast.ai",
    "title": "Remote Browser For Jupyter",
    "section": "fast.ai",
    "text": "fast.ai\nThe below youtube link (at timestamp 1:58:33), from fastai Lesson 10 Part 2 (2018) will walk you through how to accomplish this."
  },
  {
    "objectID": "notes/jupyter/Best Way To Launch Jupyter On A Remote Server.html",
    "href": "notes/jupyter/Best Way To Launch Jupyter On A Remote Server.html",
    "title": "OpenData",
    "section": "",
    "text": "jupyter lab --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port 8081"
  },
  {
    "objectID": "notes/jupyter/shortcuts.html",
    "href": "notes/jupyter/shortcuts.html",
    "title": "My Jupyter Shortcuts",
    "section": "",
    "text": "People complain about “state” in Jupyter. This can be easily avoided by frequently restarting the kernel and running all cells from the top. Thankfully, you can set a hotkey that allows you to do this effortlessly. In Jupyter Lab, go to Settings then Advanced Settings Editor. Copy and paste the below json into the User Prefences pane. If you already have user-defined shortcuts, modify this appropriately.\n{\n    \"shortcuts\": [\n        {\n            \"args\": {},\n            \"command\": \"application:activate-next-tab\",\n            \"keys\": [\n                \"Ctrl Shift ]\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-next-tab-bar\",\n            \"keys\": [\n                \"Ctrl Shift .\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-previous-tab\",\n            \"keys\": [\n                \"Ctrl Shift [\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-previous-tab-bar\",\n            \"keys\": [\n                \"Ctrl Shift ,\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:close\",\n            \"keys\": [\n                \"Alt W\"\n            ],\n            \"selector\": \".jp-Activity\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:toggle-left-area\",\n            \"keys\": [\n                \"Accel B\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:toggle-mode\",\n            \"keys\": [\n                \"Accel Shift D\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"apputils:activate-command-palette\",\n            \"keys\": [\n                \"Accel Shift C\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"apputils:print\",\n            \"keys\": [\n                \"Accel P\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-console\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-CodeConsole-promptCell .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-file\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-FileEditor .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-notebook\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:linebreak\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='notebook'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:linebreak\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-forced\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='notebook'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-forced\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-unforced\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:continue\",\n            \"keys\": [\n                \"F9\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-console\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-CodeConsole\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-file\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-FileEditor\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-notebook\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-Notebook\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:next\",\n            \"keys\": [\n                \"F10\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:stepIn\",\n            \"keys\": [\n                \"F11\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:stepOut\",\n            \"keys\": [\n                \"Shift F11\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:terminate\",\n            \"keys\": [\n                \"Shift F9\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"docmanager:save\",\n            \"keys\": [\n                \"Accel S\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"docmanager:save-as\",\n            \"keys\": [\n                \"Accel Shift S\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:highlightNext\",\n            \"keys\": [\n                \"Accel G\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:highlightPrevious\",\n            \"keys\": [\n                \"Accel Shift G\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:start\",\n            \"keys\": [\n                \"Accel F\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"editmenu:redo\",\n            \"keys\": [\n                \"Accel Shift Z\"\n            ],\n            \"selector\": \"[data-jp-undoer]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"editmenu:undo\",\n            \"keys\": [\n                \"Accel Z\"\n            ],\n            \"selector\": \"[data-jp-undoer]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:copy\",\n            \"keys\": [\n                \"Accel C\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:create-main-launcher\",\n            \"keys\": [\n                \"Accel Shift L\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:cut\",\n            \"keys\": [\n                \"Accel X\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:delete\",\n            \"keys\": [\n                \"Delete\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:duplicate\",\n            \"keys\": [\n                \"Accel D\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:go-up\",\n            \"keys\": [\n                \"Backspace\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:paste\",\n            \"keys\": [\n                \"Accel V\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:rename\",\n            \"keys\": [\n                \"F2\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:toggle-main\",\n            \"keys\": [\n                \"Accel Shift F\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filemenu:close-and-cleanup\",\n            \"keys\": [\n                \"Ctrl Shift Q\"\n            ],\n            \"selector\": \".jp-Activity\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:flip-horizontal\",\n            \"keys\": [\n                \"H\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:flip-vertical\",\n            \"keys\": [\n                \"V\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:invert-colors\",\n            \"keys\": [\n                \"I\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:reset-image\",\n            \"keys\": [\n                \"0\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:rotate-clockwise\",\n            \"keys\": [\n                \"]\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:rotate-counterclockwise\",\n            \"keys\": [\n                \"[\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:zoom-in\",\n            \"keys\": [\n                \"=\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:zoom-out\",\n            \"keys\": [\n                \"-\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"inspector:open\",\n            \"keys\": [\n                \"Accel I\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"kernelmenu:interrupt\",\n            \"keys\": [\n                \"I\",\n                \"I\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"kernelmenu:restart\",\n            \"keys\": [\n                \"0\",\n                \"0\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"runmenu:restart-and-run-all\",\n            \"keys\": [\n                \"0\",\n                \"R\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:restart-and-run-to-selected\",\n            \"keys\": [\n                \"0\",\n                \"S\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-code\",\n            \"keys\": [\n                \"Y\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-1\",\n            \"keys\": [\n                \"1\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-2\",\n            \"keys\": [\n                \"2\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-3\",\n            \"keys\": [\n                \"3\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-4\",\n            \"keys\": [\n                \"4\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-5\",\n            \"keys\": [\n                \"5\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-6\",\n            \"keys\": [\n                \"6\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-markdown\",\n            \"keys\": [\n                \"M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-raw\",\n            \"keys\": [\n                \"R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:copy-cell\",\n            \"keys\": [\n                \"C\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:cut-cell\",\n            \"keys\": [\n                \"X\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:delete-cell\",\n            \"keys\": [\n                \"D\",\n                \"D\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-command-mode\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-command-mode\",\n            \"keys\": [\n                \"Ctrl M\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-edit-mode\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-above\",\n            \"keys\": [\n                \"Shift ArrowUp\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-above\",\n            \"keys\": [\n                \"Shift K\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-below\",\n            \"keys\": [\n                \"Shift ArrowDown\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-below\",\n            \"keys\": [\n                \"Shift J\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-bottom\",\n            \"keys\": [\n                \"Shift End\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-top\",\n            \"keys\": [\n                \"Shift Home\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:insert-cell-above\",\n            \"keys\": [\n                \"A\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:insert-cell-below\",\n            \"keys\": [\n                \"B\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cell-above\",\n            \"keys\": [\n                \"Ctrl Backspace\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cell-below\",\n            \"keys\": [\n                \"Ctrl Shift M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cells\",\n            \"keys\": [\n                \"Shift M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-down\",\n            \"keys\": [\n                \"ArrowDown\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-down\",\n            \"keys\": [\n                \"J\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-up\",\n            \"keys\": [\n                \"ArrowUp\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-up\",\n            \"keys\": [\n                \"K\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:paste-cell-below\",\n            \"keys\": [\n                \"V\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:redo-cell-action\",\n            \"keys\": [\n                \"Shift Z\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-insert-below\",\n            \"keys\": [\n                \"Alt Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-insert-below\",\n            \"keys\": [\n                \"Alt Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-select-next\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"runmenu:run\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \"[data-jp-code-runner]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"settingeditor:open\",\n            \"keys\": [\n                \"Accel ,\"\n            ],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"settingeditor:save\",\n            \"keys\": [\n                \"Accel S\"\n            ],\n            \"selector\": \".jp-SettingEditor\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tabsmenu:activate-previously-used-tab\",\n            \"keys\": [\n                \"Accel Shift '\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:dismiss\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \"body.jp-mod-tooltip .jp-Notebook\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:dismiss\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \"body.jp-mod-tooltip .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-console\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-CodeConsole-promptCell .jp-InputArea-editor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-file\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-FileEditor .jp-CodeMirrorEditor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-notebook\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode .jp-InputArea-editor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace):not(.jp-mod-completer-active)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-all-cell-line-numbers\",\n            \"keys\": [\n                \"Shift L\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-cell-line-numbers\",\n            \"keys\": [\n                \"L\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:split-cell-at-cursor\",\n            \"keys\": [\n                \"Ctrl Shift -\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-render-side-by-side\",\n            \"keys\": [\n                \"Ctrl Shift R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:select-all\",\n            \"keys\": [\n                \"Accel A\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-render-side-by-side-current\",\n            \"keys\": [\n                \"Shift R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:undo-cell-action\",\n            \"keys\": [\n                \"Z\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        }\n    ]\n}"
  },
  {
    "objectID": "notes/jupyter/Fix Jupyter CUDA cache.html",
    "href": "notes/jupyter/Fix Jupyter CUDA cache.html",
    "title": "Fix Jupyter CUDA cache",
    "section": "",
    "text": "[[CUDA]] [[Jupyter tip]]\napparently this is meant to work %config ZMQInteractiveShell.cache_size = 0 %reset -f out is meant to remove all stuff in the cache\nhttps://discord.com/channels/689892369998676007/766837559920951316/1037245359027658762"
  }
]