[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenData",
    "section": "",
    "text": "OpenData is a database of publicly available behavioral datasets. To browse the database, click on the links above or use the search bar at the top-right of this page. The goal of this project is simply to make it easier for researchers to find and use publicly available behavioral data as part of research."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "OpenData",
    "section": "üíº Get In Touch",
    "text": "üíº Get In Touch\nEmail me at yuanbopsy@gmail.com, or DM me on  or  if you‚Äôd like to chat!"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "OpenData",
    "section": "üìÆ Blog Posts",
    "text": "üìÆ Blog Posts\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n1/27/23\n\n\nAbir et al.¬†(2023)\n\n\n\n\n11/24/21\n\n\nAlgermissen et al.¬†(2021)\n\n\n\n\n4/4/16\n\n\nAlbrecht et al.¬†(2016)\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#subscribe",
    "href": "index.html#subscribe",
    "title": "OpenData",
    "section": "üì¨ Subscribe",
    "text": "üì¨ Subscribe\nSubscribe via  RSS or enter your email below:"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "üé§ Talks",
    "section": "",
    "text": "These are a list of talks I‚Äôve given:\n\nHow to evaluate ML Tooling: Guest Lecutre for Stanford CS 329S ML Systems Design, Feb 2022. Slides, Video\nTensorFlow World, 2019: ‚ÄúAutomating your developer workflow on GitHub with Tensorflow‚Äù. Slides, Link\nKubeCon 2018, ‚ÄúNatural Language Code Search With Kubeflow‚Äù. Slides, Video\nKDD, London August 2018: Hands on tutorial, ‚ÄúFeature Extraction and Summarization With Sequence to Sequence Learning‚Äù. Tutorial-site\nMl4all, Portland May 2018: ‚ÄúHow to Create Magical Data Products Using Sequence-to-Sequence Models‚Äù. Slides, Video"
  },
  {
    "objectID": "blog/posts/k8s/index.html",
    "href": "blog/posts/k8s/index.html",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "",
    "text": "K8s For Data Scientists Course\n\n\n\nIf you came here looking for the course, feel free to jump ahead to: K8s For Data Scientists.\nKubernetes, known as K8s, is an open-source system for deploying and managing containerized applications in the cloud. An increasing amount of modern web applications are deployed on K8s. If you are an ML engineer, it is increasingly likely that either the infrastructure you use to train, monitor, or orchestrate your models is deployed on K8s, or downstream applications that consume your models are running on K8s. However, K8s is a complex system that can be intimidating to learn.\nI agree with Chip Huyen that, in theory, Data Scientists shouldn‚Äôt need to learn K8s. However, the truth is: Even though you shouldn‚Äôt have to, it‚Äôs really beneficial if you do! I‚Äôve found that I‚Äôm often constrained by infrastructure and that infrastructure is increasingly hosted on Kubernetes.\nFor example, I‚Äôm rarely given access to a cloud provider‚Äôs console, and instead, I have access to a K8s cluster with some data tools already installed. When something goes awry, it‚Äôs beneficial to know enough about K8s to debug the issue. Additionally, familiarity with basic concepts allows me to have more productive conversations with my team about infrastructure.\nVicki Boykis seems to agree that the investment in learning this technology is worthwhile1:\nBelow, I outline several reasons why learning K8s is a good idea for machine learning engineers2."
  },
  {
    "objectID": "blog/posts/k8s/index.html#hosted-dataml-tools-are-not-always-an-option",
    "href": "blog/posts/k8s/index.html#hosted-dataml-tools-are-not-always-an-option",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "Hosted data/ML tools are not always an option",
    "text": "Hosted data/ML tools are not always an option\n\n\n\nA robot concierge helping a scientist\n\n\nLarge cloud providers offer their flavors of ML infrastructure as hosted solutions3. However, there is often a gap between these offerings and the needs of machine learning teams. For example, I‚Äôve seen the following tools deployed alongside or in place of hosted solutions:\n\nMetaflow\nKubeflow\nArgo\nJupyterHub\nDask\netc.\n\nWhen open source isn‚Äôt enough, third-party vendors are happy to install their software on your cloud. However, you often need basic infrastructure skills to enable this. These skills often intersect with Kubernetes. While you may not be responsible for deploying the infrastructure yourself, it is helpful to understand the basics of how things work so that you can do basic debugging and troubleshooting. For example, knowing where to find logs or an API/HTTPS endpoint can unblock you in many cases."
  },
  {
    "objectID": "blog/posts/k8s/index.html#nobody-is-coming-to-save-you",
    "href": "blog/posts/k8s/index.html#nobody-is-coming-to-save-you",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "Nobody is coming to save you",
    "text": "Nobody is coming to save you\n\n\n\nA super hero\n\n\nA typical first experience as a machine learning professional is that you don‚Äôt have the necessary tools to get started. This is incredibly frustrating, as making progress without the proper tools can be hard. This experience usually culminates in a conversation like this:\n\nML Eng: I‚Äôm excited to join ACME company! You‚Äôve hired me to optimize marketing spending with predictive models. The issue is that we don‚Äôt have the basic infrastructure or tools necessary for me to work efficiently.\nManager: I‚Äôm confused. Can‚Äôt you install the tools you need? Isn‚Äôt that what you are for? I was expecting that you would figure it out.\nML Eng: No, I don‚Äôt know how to set up and deploy infrastructure. We need a special infrastructure or DevOps person for that.\nManager: It will be hard to ask for more resources if we don‚Äôt know the expected return on investment. Can you do the ML project first, demonstrate some value, and then we can invest in infrastructure?\nML Eng: I need some minimum tools to experiment more quickly and develop a proof of concept. Also, I need tools that might help me collaborate better with my team‚Ä¶\n\nMy experience is that DevOps teams are chronically understaffed and overworked. While it usually isn‚Äôt advisable to deploy enterprise software yourself on Kubernetes for security concerns, having basic skills can lift a tremendous burden off your DevOps counterparts and make it tractable for them to help you.\nK8s are not a panacea for all infrastructure problems. You must operate within the constraints of your organization and existing software stack.4 However, with its growing popularity, it is increasingly likely that learning this technology will help you."
  },
  {
    "objectID": "blog/posts/k8s/index.html#ml-research-is-crowded.-compete-on-swe-skills.",
    "href": "blog/posts/k8s/index.html#ml-research-is-crowded.-compete-on-swe-skills.",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "ML research is crowded. Compete on SWE skills.",
    "text": "ML research is crowded. Compete on SWE skills.\n\n\n\nAn overcrowded room of scientists\n\n\nOne of the best ways to set yourself apart as a data scientist is through your skills. Traditional education often emphasizes learning the latest ML techniques. However, cutting-edge ML research is very competitive. It‚Äôs also an extremely crowded space.\nIn my experience, the bottleneck many teams face is not a lack of knowledge of cutting-edge ML techniques but software engineering skills and partners to help operationalize models. If you take some time to learn how to stand up tools and infrastructure, you will be invaluable to your team.\nMore importantly, deploying and integrating models into services and applications is critical to connecting ML to business problems. Learning K8s will help you do this."
  },
  {
    "objectID": "blog/posts/k8s/index.html#your-company-likely-already-runs-k8s",
    "href": "blog/posts/k8s/index.html#your-company-likely-already-runs-k8s",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "Your company likely already runs K8s",
    "text": "Your company likely already runs K8s\n\n\n\nA scientist shaking hands with someone who runs infrastructure\n\n\nJust as Python is the lingua franca of data science, K8s is becoming the lingua franca of cloud infrastructure. According to a 2021 Survey by CNCF, 96% of organizations are either using or evaluating Kubernetes. Furthermore, Stack Overflow‚Äôs 2022 Developer Survey shows that Docker and Kubernetes are the number one and two most loved and wanted tools, respectively. This is a strong indicator that K8s are here to stay.\nBasic proficiency with K8s will drastically increase your chances of garnering support for your desired tools in many organizations. Proficiency with K8s increases the likelihood that:\n\nYour DevOps counterparts will feel comfortable with the tools you want to deploy\nYou will have a shared language in which to talk to your application administrators\nYou will be more likely to attract people to help you with infra 5\n\nThese factors make it much more likely that you will get the tools that meet you where you are as opposed to something a software engineer without any data science experience thinks is a good idea (which I‚Äôve seen happen a lot!)."
  },
  {
    "objectID": "blog/posts/k8s/index.html#but-isnt-it-overkill",
    "href": "blog/posts/k8s/index.html#but-isnt-it-overkill",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "But isn‚Äôt it overkill?",
    "text": "But isn‚Äôt it overkill?\n\n\n\nCutting oranges with a chainsaw\n\n\nFor simple apps that you want to stand up quickly or prototype, K8s is overkill. Instead, I‚Äôm advocating knowledge of K8s as useful when working within the environments found in many companies. For example, hosting your data product on a single VM is often insufficient if you want to deploy production software. Many companies even have infrastructure that may block you from doing this with paved paths that only include Kubernetes.\nEven if you are not deploying any production software, K8s can be invaluable in allowing you to deploy the tools you need. In many cases using K8s can make tasks easier. Enterprises have necessarily invested resources in creating guardrails to control costs and security. Those guardrails are increasingly built around K8s patterns6. Understanding these concepts can make operating within the confines of your company‚Äôs cloud stack easier."
  },
  {
    "objectID": "blog/posts/k8s/index.html#you-dont-need-to-be-an-expert",
    "href": "blog/posts/k8s/index.html#you-dont-need-to-be-an-expert",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "You don‚Äôt need to be an expert",
    "text": "You don‚Äôt need to be an expert\n\n\n\nA student sitting at a desk in a library\n\n\nK8s are complicated, but you don‚Äôt need to become an expert to unlock great value as a Data Scientist. I‚Äôm not suggesting that data scientists become K8s administrators. K8s Administration is a very involved task and worthy of its own role. Unfortunately, nearly all educational material around K8s is focused on being an administrator, which is overkill for what most data scientists need."
  },
  {
    "objectID": "blog/posts/k8s/index.html#a-course",
    "href": "blog/posts/k8s/index.html#a-course",
    "title": "Why Should ML Engineers Learn Kubernetes?",
    "section": "A course?",
    "text": "A course?\nI haven‚Äôt yet found a good resource for people like data scientists to learn Kubernetes without wading through lots of irrelevant material geared towards administrators. So my colleagues and I are considering creating a free course with data scientists in mind. If this sounds interesting, you can sign up here."
  },
  {
    "objectID": "blog/secret.html",
    "href": "blog/secret.html",
    "title": "Hamel‚Äôs Blog",
    "section": "",
    "text": "This page is supposed to be secret!\n\n\n\nA listing of all my blog posts can be found here\n\n\n\n\n\n\n\n\n\n\n\n\nAbir et al.¬†(2023)\n\n\nHuman Exploration Strategically Balances Approaching and Avoiding Uncertainty\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\nAlgermissen et al.¬†(2021)\n\n\nStriatal BOLD and midfrontal theta power express motivation for action\n\n\n\n\n\n\n\n\n\nNov 24, 2021\n\n\n\n\n\n\n\n\nAlbrecht et al.¬†(2016)\n\n\nReduction of Pavlovian Bias in Schizophrenia: Enhanced Effects in Clozapine-Administered Patients\n\n\n\n\n\n\n\n\n\nApr 4, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guest-blog.html",
    "href": "guest-blog.html",
    "title": "Guest Blogs",
    "section": "",
    "text": "nbdev + Quarto: A new secret weapon for productivity, the fastai blog, July 2022.\nNotebooks in production with Metaflow Introduces a new Metaflow feature that allows users to use notebooks in production ML workflows.\nPython Concurrency: The Tricky Bits: An exploration of threads, processes, and coroutines in Python, with interesting examples that illuminate the differences between each.\nghapi, a new third-party Python client for the GitHub API by Jeremy Howard & Hamel Husain, GitHub Repo.\nNbdev: A literate programming environment that democratizes software engineering best practices by Hamel Husain, Jeremy Howard, The GitHub Blog.\nfastcore: An Underrated Python Library by Hamel Husain, Jeremy Howard, GitHub Repo.\nData Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes: An end-to-end example of deploying a machine learning product using Jupyter, Papermill, Tekton, GitOps and Kubeflow. by Jeremy Lewi, Hamel_Husain, The Kubeflow Blog.\nIntroducing fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. by Jeremy Howard & Hamel Husain, GitHub Repo\nGitHub Actions: Providing Data Scientists With New Superpowers by Jeremy Howard & Hamel Husain.\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search: by Miltiadis Allamanis, Marc Brockschmidt, Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit GitHub Repo\nHow To Create Natural Language Semantic Search for Arbitrary Objects With Deep¬†Learning. (Related: GitHub engineering blog article, Live demo)\nHow To Create Magical Data Products Using Sequence-to-Sequence Models\nHow to Automate Tasks on GitHub With Machine Learning for Fun and Profit\nHow Docker Can Make You A More Effective Data Scientist\nAutomated Machine Learning, A Paradigm Shift That Accelerates Data Scientst Productivity At Airbnb"
  },
  {
    "objectID": "notes/how-to-learn/index.html",
    "href": "notes/how-to-learn/index.html",
    "title": "How to learn",
    "section": "",
    "text": "I read the book Mindshift and it was unituitively so good that I decided to take this class. As a parent, I learned a bunch of things that I think will be beneficial to my children‚Äôs education.\nNotes from class Learning how to learn. These notes are for me and may not make sense for others."
  },
  {
    "objectID": "notes/how-to-learn/index.html#focused-vs-diffused-mode",
    "href": "notes/how-to-learn/index.html#focused-vs-diffused-mode",
    "title": "How to learn",
    "section": "Focused vs Diffused Mode",
    "text": "Focused vs Diffused Mode\nYou can not access focus and diffused mode simultaneously.\nPeople have tried to access diffuse mode of thinking by bringing themselves to the point of sleep and waking up just as they fall asleep. For example, Salvador Dali - holding keys in your hand, and let the sound of keys falling the ground wake you up.\nExercise, going for a walk good way to access diffuse thinking. You must take notes right away b/c diffuse thoughts may evaporate very fast."
  },
  {
    "objectID": "notes/how-to-learn/index.html#procrastination-memory-and-sleep",
    "href": "notes/how-to-learn/index.html#procrastination-memory-and-sleep",
    "title": "How to learn",
    "section": "Procrastination Memory and Sleep",
    "text": "Procrastination Memory and Sleep\nThey advocate the Pomodoro technique to combating procrastination. Its like HITT.\nPeriodic relaxation (every ~ 30 minutes) is important for accessing your diffuse mode. ‚ÄúIts important for the mortar to dry‚Äù.\nSpaced repetition (like Anki) is important for building memory. i\nGo over what you want to learn about right before you go to sleep, this will substantially improve the chances you will dream about it and form new connections about the subject.\nExercise can help create new neurons in your hippocampus (new neurons can be created there in adulthood) and help them survive longer."
  },
  {
    "objectID": "notes/how-to-learn/index.html#writing-tips-diffuse-mode",
    "href": "notes/how-to-learn/index.html#writing-tips-diffuse-mode",
    "title": "How to learn",
    "section": "Writing Tips Diffuse Mode",
    "text": "Writing Tips Diffuse Mode\nDiffuse mode is very important for writing. Editing is like focus mode and creating ideas is diffuse mode. Some rules of thumb: - Do not outline, make a mind map - Do not edit while you are writing (this is really hard to do -> turn off monitor and just write).https://writeordie.com - app that forces you to stay in diffuse mode. You really cannot look at the screen. - Repeating again, do not look at screen while you are writing! Only when editing."
  },
  {
    "objectID": "notes/how-to-learn/index.html#chunking",
    "href": "notes/how-to-learn/index.html#chunking",
    "title": "How to learn",
    "section": "Chunking",
    "text": "Chunking\n‚ÄúTying your shoes‚Äù. Best chunks are subconscious. Spoken language is the best example of chunking. You have to practice to build chunks, you cannot just observe. You have to perform the task yourself.\nYou should scan a chapter before you read it: section headings, pictures, etc. This can help you build chunks."
  },
  {
    "objectID": "notes/how-to-learn/index.html#illusions-of-competence",
    "href": "notes/how-to-learn/index.html#illusions-of-competence",
    "title": "How to learn",
    "section": "Illusions of competence",
    "text": "Illusions of competence\nRight after you read something, look away and repeat to yourself what you recall. You can also draw a concept map. The recall process actually improves memory.\nRecall is better than re-reading. Re-reading is effective when you let time pass so you get spaced repetition. You need to test yourself to make sure you are competent. Recall is a form of testing.\nRecall outside your place of study to strengthen your memory. This is because you can get queues from where you are studying."
  },
  {
    "objectID": "notes/how-to-learn/index.html#deliberate-practice",
    "href": "notes/how-to-learn/index.html#deliberate-practice",
    "title": "How to learn",
    "section": "Deliberate Practice",
    "text": "Deliberate Practice\nFocus on the bits that you find difficult. Interleaving is important, meaning learning different subjects or even sections within one subject at once. Thomas S. Khun discovered that two types of people tend to make scientific breakthroughs: (1) young people (2) those who are trained in another discipline."
  },
  {
    "objectID": "notes/how-to-learn/index.html#procrastination-and-memory",
    "href": "notes/how-to-learn/index.html#procrastination-and-memory",
    "title": "How to learn",
    "section": "Procrastination and Memory",
    "text": "Procrastination and Memory\nYou have already learned about the Pomodoro technique. There are other techniques.\nFocus on the process, not the product. Don‚Äôt focus on completing the homework, focus on the process that leads you to complete the homework. Process is the small chunks of time to chip away at the task. This is the idea behind the Pomodoro. Your only goal is to finish the Pomodoro, for example."
  },
  {
    "objectID": "notes/how-to-learn/index.html#juggling-life-and-learning",
    "href": "notes/how-to-learn/index.html#juggling-life-and-learning",
    "title": "How to learn",
    "section": "Juggling Life and Learning",
    "text": "Juggling Life and Learning\nYou should make to-do list the night before for the next day and write it down. This will allow your subconscious to work on how it will conquer that task. Furthermore, writing it down will allow you to free it from working memory.\nPlan your quitting time is important."
  },
  {
    "objectID": "notes/concurrency.html",
    "href": "notes/concurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "Understand the world of Python concurrency: threads, processes, coroutines and asynchronous programming with a realistic examples.\nSee this blog article."
  },
  {
    "objectID": "notes/pandoc/index.html",
    "href": "notes/pandoc/index.html",
    "title": "pandoc filters",
    "section": "",
    "text": "Two python packages\nThe tutorial on pandoc filters can help you get oriented to the general idea. If rolling your own filters, you probably want to use the JSON filters. Furthermore you can understand the pandoc AST by using the -t native flag (examples of this are shown later)."
  },
  {
    "objectID": "notes/pandoc/index.html#the-minimal-notebook",
    "href": "notes/pandoc/index.html#the-minimal-notebook",
    "title": "pandoc filters",
    "section": "The minimal notebook",
    "text": "The minimal notebook\nHere is minimal notebook we are working with:\njson title=\"minimal.ipynb\" {  \"cells\": [   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## A minimal notebook\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"<MyTag></MyTag>\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": 1,    \"metadata\": {},    \"outputs\": [     {      \"name\": \"stdout\",      \"output_type\": \"stream\",      \"text\": [       \"2\\n\"      ]     }    ],    \"source\": [     \"# Do some arithmetic\\n\",     \"print(1+1)\"    ]   }  ],  \"metadata\": {   \"interpreter\": {    \"hash\": \"42fd40e048e0585f88ec242f050f7ef0895cf845a8dd1159352394e5826cd102\"   },   \"kernelspec\": {    \"display_name\": \"Python 3.9.7 ('base')\",    \"language\": \"python\",    \"name\": \"python3\"   },   \"language_info\": {    \"codemirror_mode\": {     \"name\": \"ipython\",     \"version\": 3    },    \"file_extension\": \".py\",    \"mimetype\": \"text/x-python\",    \"name\": \"python\",    \"nbconvert_exporter\": \"python\",    \"pygments_lexer\": \"ipython3\",    \"version\": \"3.9.7\"   }  },  \"nbformat\": 4,  \"nbformat_minor\": 4 }"
  },
  {
    "objectID": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-pandoc",
    "href": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-pandoc",
    "title": "pandoc filters",
    "section": "Minimal ipynb to md converstion with pandoc",
    "text": "Minimal ipynb to md converstion with pandoc\n$ pandoc --to gfm minimal.ipynb\n<div class=\"cell markdown\">\n\n## A minimal notebook\n\n</div>\n\n<div class=\"cell markdown\">\n\n<MyTag></MyTag>\n\n</div>\n\n<div class=\"cell code\" execution_count=\"1\">\n\n``` python\n# Do some arithmetic\nprint(1+1)\n```\n\n<div class=\"output stream stdout\">\n\n    2\n\n</div>\n\n</div>"
  },
  {
    "objectID": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-quarto",
    "href": "notes/pandoc/index.html#minimal-ipynb-to-md-converstion-with-quarto",
    "title": "pandoc filters",
    "section": "Minimal ipynb to md converstion with quarto",
    "text": "Minimal ipynb to md converstion with quarto\n$ quarto render minimal.ipynb --to gfm\npandoc\n  to: gfm+footnotes+tex_math_dollars-yaml_metadata_block\n  output-file: minimal.md\n  standalone: true\n  default-image-extension: png\n  filters:\n    - crossref\n\nOutput created: minimal.md\nThis creates\n\n## A minimal notebook\n\n<MyTag></MyTag>\n\n``` python\n# Do some arithmetic\nprint(1+1)\n```\n\n    2\nRunning Pandoc With those Extensions\nrunning pandoc with --standalone --to gfm+footnotes+tex_math_dollars-yaml_metadata_block still adds the divs and looks different than quarto. Somewhere, maybe quarto is removing the divs. We can see the Div elements in the AST when we explore panflute in the sections below."
  },
  {
    "objectID": "notes/pandoc/index.html#how-to-use-panflute",
    "href": "notes/pandoc/index.html#how-to-use-panflute",
    "title": "pandoc filters",
    "section": "How to use panflute",
    "text": "How to use panflute\nThe examples are helpful.\nThis filter places CodeOutput blocks around code as well as changes the codefence to have file=script.py in order to hack the code fence.\n#!/Users/hamel/opt/anaconda3/bin/python\n#flute.py\nfrom typing import Text\nfrom panflute import *\nfrom logging import warning\n\n\ndef increase_header_level(elem, doc):\n    if type(elem) == CodeBlock and type(elem.parent.prev) == CodeBlock:\n        return ([RawBlock(\"<CodeOutput>\"), elem, RawBlock(\"</CodeOutput>\")])\n    elif type(elem) == CodeBlock:\n        elem.classes = ['file=script.py']\n\n\ndef main(doc=None):\n    return run_filter(increase_header_level, doc=doc)\n\n\nif __name__ == \"__main__\":\n    main()\nThis is how we can use this filter and see the rendered output:\n$ pandoc --to gfm minimal.ipynb --filter \"flute.py\"\n<div class=\"cell markdown\">\n\n## A minimal notebook\n\n</div>\n\n<div class=\"cell markdown\">\n\n<MyTag></MyTag>\n\n</div>\n\n<div class=\"cell code\" execution_count=\"1\">\n\n``` file=script.py\n# Do some arithmetic\nprint(1+1)\n```\n\n<div class=\"output stream stdout\">\n\n<CodeOutput>\n\n    2\n\n</CodeOutput>\n\n</div>\n\n</div>\nNote: we could probably replace the inner div with the output class with <CodeOutput> tag\nJust for completeness, this is the schema of the minimal notebook using the --to native flag prior to applying the filter:\n$pandoc --to native minimal.ipynb\n[ Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Header\n        2\n        ( \"a-minimal-notebook\" , [] , [] )\n        [ Str \"A\" , Space , Str \"minimal\" , Space , Str \"notebook\" ]\n    ]\n, Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Para\n        [ RawInline (Format \"html\") \"<MyTag>\"\n        , RawInline (Format \"html\") \"</MyTag>\"\n        ]\n    ]\n, Div\n    ( \"\"\n    , [ \"cell\" , \"code\" ]\n    , [ ( \"execution_count\" , \"1\" ) ]\n    )\n    [ CodeBlock\n        ( \"\" , [ \"python\" ] , [] )\n        \"# Do some arithmetic\\nprint(1+1)\"\n    , Div\n        ( \"\" , [ \"output\" , \"stream\" , \"stdout\" ] , [] )\n        [ CodeBlock ( \"\" , [] , [] ) \"2\\n\" ]\n    ]\n]\nAnd after applying the filter:\n$pandoc --to native minimal.ipynb --filter flute.py\n[ Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Header\n        2\n        ( \"a-minimal-notebook\" , [] , [] )\n        [ Str \"A\" , Space , Str \"minimal\" , Space , Str \"notebook\" ]\n    ]\n, Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Para\n        [ RawInline (Format \"html\") \"<MyTag>\"\n        , RawInline (Format \"html\") \"</MyTag>\"\n        ]\n    ]\n, Div\n    ( \"\"\n    , [ \"cell\" , \"code\" ]\n    , [ ( \"execution_count\" , \"1\" ) ]\n    )\n    [ CodeBlock\n        ( \"\" , [ \"file=script.py\" ] , [] )\n        \"# Do some arithmetic\\nprint(1+1)\"\n    , Div\n        ( \"\" , [ \"output\" , \"stream\" , \"stdout\" ] , [] )\n        [ RawBlock (Format \"html\") \"<CodeOutput>\"\n        , CodeBlock ( \"\" , [] , [] ) \"2\\n\"\n        , RawBlock (Format \"html\") \"</CodeOutput>\"\n        ]\n    ]\n]"
  },
  {
    "objectID": "notes/docker/index.html",
    "href": "notes/docker/index.html",
    "title": "Docker",
    "section": "",
    "text": "Notes from the book Docker In Action\n;\n\nChapter 1\n\nDocker containers are faster than VMs to start, partly because they do NOT offer any hardware virtualization.\n\nVMs provide hardware abstractions so you can run operating systems.\n\nDocker uses Linux namespaces and cgropus\n\nHamel: I don‚Äôt know what this is\n\n\n\n\nChapter 2\n\nGetting help:\n\ndocker help cp\ndocker help run\n\nLinking containers: docker run --link\n\nthis is apparently deprecated per the docs\nOpens a secure tunnel between two containers automatically\nAlso exposes environment variables and other things (see the docs)\n\ndocker cp copy files from a container to local filesystem\nDetach an interactive container:\n\nHold down Control and press P then Q\n\nGet logs docker logs <container name>\n\nHamel: This is like kubectl logs\n\nRun a new command in a running container docker exec\n\ndocker exec <container_name> ps will run the ps command and emit that to stdout\n\nRename a container with docker rename <current_name> <new_name>\ndocker exec run additional processes in an already running container\ndocker create is the same as docker run except that the container is created in a stopped state.\ndocker run --read-only allows you to run a container in a read only state, which you only need to do in special circumstances (you probably never need to use this). You can make exceptions to the read only constraint with the -v flag:\n\n\n\nOverride the entrypoint using the --entrypoint flag (this is discussed in part 2 of the book).\n\n\n\nInjecting environment variables\nWith the --env or -e flags.\nA nice trick to see all the environment variables in a docker container is to use the Unix command env\n\nSetting multiple environment variables: use \\ for multiline like this:\ndocker create \\\n  --env WORDPRESS_DB_HOST=<my database hostname> \\\n  --env WORDPRESS_DB_USER=site_admin \\\n  --env WORDPRESS_DB_PASSWORD=MeowMix42 \\ \nwordpress:4\n\n\nAutomatically restarting containers\nDocker uses an exponential backoff strategy - double the previous time waiting until restarting.\ndocker run -d --restart always ...\nSee these restart policies\n\nno\non-failure[:max-retries]\nalways\nunless-stopped\n\n\n\nRemoving containers vs.¬†images\nContainers are the actual instantiation of an image, just like how an object is an instantion of an instance of a class.\ndocker rm: remove a container docker rmi: remove an image\n\n\n\nChapter 3\n\nTwo ways to publish an image\n\nBuild locally, push image to registry\nMake a Dockerfile and use DockerHub‚Äôs build system. This is preferred and considered to be safer, and DockerHub will mark your image as trusted if you do this because it is the only way to provide transparency to what is in your image.\n\nSearch dockerhub by keyword , sorted descending by stars\n\ndocker search <keyword>\nexample: docker search postgres\n\nUsing Alternative registries\n\ndocker pull quay.io/dockerinaction/ch3_hello_registry:latest\n\n\n\nImages as files\nYou can transport, save and load images as files! (You don‚Äôt have to push them to a registry).\n\nYou can then load the image:\ndocker load -i myfile.tar\n\n\n\nChapter 4 Persistent Storage &. Shared State with Volumes\n-v and --volume are aliases\n--volumes-from=\"<container-name>\" Mount all volumes from the given container\n\nDifferent kind of Volumes\n\nBind mount - this is what you always use\nDocker managed volume (2 kinds)\n\nAnonymous\nNamed volume (a special case of Anonymous)\n\n\nUse volumes | Docker Documentation - Named vs.¬†Anonymous volumes: article - Hamel: maybe? You might use named volumes to persist data between containers.\n\n\nTo persist data with named volumes\nNamed volume is a kind of anonymous volume where the mount point is managed by Docker. Example of how you used a named volume:\n\nStart container with a named volume: docker run --name myDatabaseWithVolume -v appdata:/var/lib/mysql  mysql save a table in the mysql database\nStart a new container with the same named volume docker run --name myDatabaseWithVolume2 -v appdata:/var/lib/mysql mysql You should be able to see the same table you created in the last container b/c data has been persisted.\n\n\n\nSee where Docker anonymous volumes store information\nUnlike a bind mount, where you explicitly name the host location, docker will manage the storage location of anonymous volumes. But how do you know where the files are stored on the host?\nYou can use docker inspect command filtered for the Volumes key to find the storage location on the host.\nCreate a container with an anonymous volume. docker run -v /some/location --name cass-shared alpine\ndocker inspect -f \"{{json .Volumes}}\" cass-shared\nThis will output a json blob which will show the mount points.\n\n\nOther things you didn‚Äôt know about volumes\n\nwhen you mount a volume, it overrides any files already at that location\n\nYou can mount specific files which avoid this\nif you specify a host directory that doesn‚Äôt exist Docker will create it for you\n\nexception: If you are mounting a file instead of a directory and it doesn‚Äôt exist on the host, Docker will throw an error\n\n\nyou can mount a volume as read only -v /source:/destination:ro\n\nsee docs (there is this optional third argument for volumes)\n\n\n\n\nThe volumes-from flag\nAllows you to share volumes with another container. When you use this flag, the same volumes are mounted into your container at the same location.\n\nVolumes are copied transitively, so this will automatically mount volumes that are also mounted this way from another container.\nCaveats - You cannot mount a shared volume to a different location within a container. This is a limitation of --volumes-from - If you have a collision in the destination mount point among several volumes-from only one volume will survive, which you can ascertain from docker inpsect - see above for how to use docker inspect - You cannot change the write permission of the volume, you inherit whatever the permission is in the source container.\n\n\nCleaning up volumes\n-v flag\ndocker rm -v will delete any managed volumes referenced by the target container\nHowever, if you delete all containers but forget a -v flag you will be left with an orphaned volume. This is bad b/c it takes up disk space until cleaned up. You have to run complicated cleanup steps to get rid of orphans.\nSolution: There is none, its a good habit to use -v anytime you call docker rm\nHamel: this means that- - Don‚Äôt use managed volumes unless you really need it - If you do use them, try to include makefiles that include -v as a part of things\n\n\nAdvanced Volume Stuff\n\nYou can have a volume container p.¬†72 so that you can reference --volume-from from all your containers.\n\nData-paced volume containers, you can pre-load volume containers with data p.¬†73\nYou can change the behavior of currently running containers by mounting configuration files and application in volumes. In a way, Hamel\n\n\n\n\nChapter 5 Single Host Networking\n\nTerminology:\n\nprotocols: tcp, http\ninterfaces: IP addresses\nports: you know what this means\n\nCustomary ports:\n\nHTTP: 80\nMySQL: 3306\nMemcached: 11211\n\n\n\n\nDiscuss advanced networking and creating a network using the docker network command. Hamel: I don‚Äôt see an immediate use for this.\n\nSpecial container networks:\n\nhost\n\ndocker run --network host allows you to pretend like the host is your local machine, and you can expose any port and that will bind to the host.\n\nnone\n\ndocker run --network none closes all connection to the outside world. This is useful for security.\n\n\n\n\nexposing ports\n-p 8080 This binds port 8080 to a random port on the host! you can find the port that the container is bound to by docker port <image name> example: docker run -p 8080 --name listener alpine docker port listener\nThis will give you output that looks like container --> host (which is reverse the other nomenclature of host:container\n-p 8080:8080 this binds the container‚Äôs port to the host‚Äôs port 8080\n-p 0.0.0.0:8080:8080/tcp same as above but specifies the interface and the tcp protocol.\nSyntax is -p <host-interface>:<host-port>:<target-port>/<protocol>\n\n\n\nChapter 6 Isolation\n\nLimit resources: Memory, CPU,\n\n-m or --memory\n\nnumber, where unit = b, k, m or g\nmemory limits are not reservations, just caps\n\n--cpu-shares\n\nis a weight you set that is used to calculate % of CPU usage allowed\n% is calculated as weight / (sum of all weights)\nonly enforced when there is contention for a CPU\n\n--cpuset-cpus : limits process to a specific CPU\n\ndocker run -d --cpuset-cpus 0 Restricts to CPU number 0\nCan specify a list or 0,1,2 or a range 0-2\n\n--device\n\nmount your webcam: docker run --device /dev/video0:/dev/video0\n\nShared memory : Hamel this was too advanced for me\n\n\n\nRunning as a user\n\nYou can only inspect the default run-as User by creating or pulling the image\n\nsee p.¬†113\n\nChange run-as user\n\ndocker run --user nobody\nThe user has to exist in the image when doing this or you will get an error. The user will not be created automatically for you.\nSee available users:\n\ndocker run --rm busybox:latest awk -F: '$0=$1' /etc/passwd\n\n\n\n\n\nPrivileged Containers: TRY NOT TO DO THIS\n\nThis is how you run Docker-in-Docker\nPriviliged containers have root privileges on the host.\n\n--privilged on docker create or docker run\n\n\n\n\nChapter 7 packaging software\nAside: cleaning up your docker environment\ndocker image prune -a and docker container prune\n\nRecovering changes to a stopped container\nI always thought you have to commit changes in order to preserve changes to an image you made in a container. This is not true (although committing changes is a good idea).\nAny changes you make to a container is saved even if the container is exited\nTo recover changes to a container\n\nFind the container (if you didn‚Äôt name it with docker run --name it will be named for you), using docker ps -a\nStart the container using docker start -ai <container_name> the -ai flags mean to attach and run interactively\nNow you are in the container you can verify that everything you installed is still there!\n\nNote: if you run your container initially with docker run --rm this automatically removes your container upon exit, so this might not be recommended as your changes are not recoverable if you forget to commit\n\n\n\nSeeing changes to a container from the base image\ndocker diff <container name> will output a long list of of file changes: - A: file added - D: file deleted - C: file changed\n\n\nOther tricks\nYou can override the entry point to the container permanently by using the --entrypoint flag: docker run --entrypoint\n\n\nUnderstanding Images & Layers\n\nfiles are stored in a Union file system, so they are stored in specific layers. The file system you are seeing as an end user are a union of all the layers. Each time a change is made to a union file system, that change is recorded on a new layer on top of all of the others. The ‚Äúunion‚Äù of all of those layers, or top-down view, is what the container (and user) sees when accessing the file system.\n\nThis means if you are not careful you can bloat the file system by making a bunch of unnecessary changes to add/delete files.\n\ndocker commit commits the top-layer changes to an image, meaning all the files changes are saved.\n\nSee image size with\ndocker images. Even though you remove a file, the image size will increase! This is because of the Union File System\nSee size of all layers\ndocker history <image name>\nflatten an image This is kind of complicated, you can do this by exporting and importing the filesystem into a base image See pg. 140. BUT there is an experimental feature called docker build --squash -t <image> .You can enable experimental features by following these instructions: dockerd Docker Documentation. For Mac, you can turn on experimental features by setting experimental: true in `settings> Command Line > enable experimental\n\n\n\nChapter 8 Build Automation\n\nuse .dockerignore to prevent certain files from being copied\nYou can set multiple environment variables at once in Dockerfile\nYou can use environment variables in the LABEL command\n\nThe metadata makes it clear that the environment variable substitution works. You can use this form of substitution in the ENV, ADD, COPY, WORKDIR, VOLUME, EXPOSE, and USER instructions.\n\n\nENV APPROOT \"/app\" APP \"mailer.sh\" VERSION \"0.6\"\nLABEL base.name \"Mailer Archetype\" base.version \"${VERSION}\"\n\nview metadata using the command docker inspect <image name>\n\n\nENTRYPOINT something arugment vs.¬†ENTRYPOINT [‚Äúsomething‚Äù, ‚Äúargument‚Äù]\nTLDR; use the ugly list approach\nThere are two instruction forms shell form and exec form docker - Dockerfile CMD shell versus exec form - Stack Overflow\nThe ENTRYPOINT instruction has two forms: the shell form and an exec form. The shell form looks like a shell command with whitespace-delimited arguments. The exec form is a string array where the first value is the command to execute and the remaining values are arguments. .\nMost importantly, if the shell form is used for ENTRYPOINT, then all other arguments provided by the CMD instruction or at runtime as extra arguments to docker run will be ignored. This makes the shell form of ENTRYPOINT less flexible.\nOther commands can use the exec form too! You must use the exec form when any of the arguments contain a whitespace:\nFROM dockerinaction/mailer-base:0.6 \nCOPY [\"./log-impl\", \"${APPROOT}\"] \nRUN chmod a+x ${APPROOT}/${APP} && \\ chown example:example /var/log \nUSER example:example \nVOLUME [\"/var/log\"]  # each value in this array will be created as a new volume definition\nCMD [\"/var/log/mailer.log\"]\nNote: you usually don‚Äôt want to specify a volume at build time.\n\n\nCMD vs.¬†ENTRYPOINT (You should really try to always use both!)\nCMD is actually an argument list for the ENTRYPOINT.\n\nLogically when you run a container it runs as <default shell program> ENTRYPOINT CMD\nYou can override the ENTRYPOINT with docker run --entrypoint, and you can override commands by just passing commands to docker run : docker run <image name> <command>\n\nFROM ubuntu\n\nENTRYPOINT [ \"ls\" ]\nCMD [\"-lah\"]\nAs you can see using ENTRYPOINT as well as CMD separately provides your downstream users with the most flexibility.\n\n\nCOPY vs ADD\nUse COPY. ADD has additional functionality like ability to download from urls and decompress files, which proved opaque over time and you shouldn‚Äôt use it.\n\n\nONBUILD\nThe ONBUILD instruction defines instructions to execute if the resulting image is used as a base for another build. those ONBUILD instructions are executed after the FROM instruction and before the next instruction in a Dockerfile.\nFROM busybox:latest \nWORKDIR /app RUN touch /app/base-evidence \nONBUILD RUN ls -al /app\n\n\nOther Stuff\n\nYou should always validate the presence of required environment variables in a startup shell script like entrypoint.sh\n\n\n\nDocker Digests\nReference the exact SHA of a Container which is the only way to guarantee the image you are referencing has not changed. @ symbol followed by the digest.\nHamel: doesn‚Äôt look like a good way to find history of digests, but you can see the current SHA when you use docker pull , you can see the SHA as well if you call docker images --digests\nFROM debian@sha256:d5e87cfcb730...\n\n\n\nChapter 10 (skipped Ch 9)\n\nYou can run your own customized registry. Simplest version can be hosted from a Docker Container!\n\n# start a local registry on port 5000\ndocker run -d --name personal_registry\n \\ -p 5000:5000 --restart=always \n \\ registry:2\n\n# push an image to the registry (using the same image that created the registry for convenience)\ndocker tag registry:2 localhost:5000/distribution:2 \ndocker push localhost:5000/distribution:2\nNote that docker push syntax is actually docker push <registry url>/org/repo\nThis chapter discusses many more things which are skipped: - Centralized registries - Enhancements - Durable blog storage - Integrating through notifications\n\n\nChapter 11 Docker Compose\nDocker compose for fastpages:\nversion: \"3\"\nservices:\n  fastpages: &fastpages\n    working_dir: /data\n    environment:\n        - INPUT_BOOL_SAVE_MARKDOWN=false\n    build:\n      context: ./_action_files\n      dockerfile: ./Dockerfile\n    image: fastpages-dev\n    logging:\n      driver: json-file\n      options:\n        max-size: 50m\n    stdin_open: true\n    tty: true\n    volumes:\n      - .:/data/\n\n  converter:\n    <<: *fastpages\n    command: /fastpages/action_entrypoint.sh\n\n  watcher:\n    <<: *fastpages\n    command: watchmedo shell-command --command /fastpages/action_entrypoint.sh --pattern *.ipynb --recursive --drop\n\n  jekyll:\n    working_dir: /data\n    image: hamelsmu/fastpages-jekyll\n    restart: unless-stopped\n    ports:\n      - \"4000:4000\"\n    volumes:\n      - .:/data/\n    command: >\n     bash -c \"gem install bundler\n     && jekyll serve --trace --strict_front_matter\"\nThe above uses YAML anchors: YAML anchors - Atlassian Documentation\nStart a particular service: docker-compose up <service name> Rebuild a service docker-compose build <service name>\nYou can express dependencies with depends_on which is useful for compose to know which services to restart or start in a specified order.\nSee examples of Docker Compose files on p 243\n\nScaling Up w/Docker Compose\nThat‚Äôs right you don‚Äôt need docker swarm. This example uses ch11_coffee_api/docker-compose.yml at master ¬∑ dockerinaction/ch11_coffee_api ¬∑ GitHub\n\nGet list of containers that are currently providing the service.\n\ndocker-compose ps coffee\n          Name                 Command       State            Ports\n----------------------------------------------------------------------------\nch11_coffee_api_coffee_1   ./entrypoint.sh   Up      0.0.0.0:32768->3000/tcp\n\nScale it up with docker-compose up --scale\n\ndocker-compose up --scale coffee=5\nWhen you run docker-compose ps coffee:\ndocker-compose ps coffee                                                                                                                        ÓÇ≤ ‚úî\n          Name                 Command       State            Ports\n----------------------------------------------------------------------------\nch11_coffee_api_coffee_1   ./entrypoint.sh   Up      0.0.0.0:32768->3000/tcp\nch11_coffee_api_coffee_2   ./entrypoint.sh   Up      0.0.0.0:32769->3000/tcp\nch11_coffee_api_coffee_3   ./entrypoint.sh   Up      0.0.0.0:32771->3000/tcp\nch11_coffee_api_coffee_4   ./entrypoint.sh   Up      0.0.0.0:32770->3000/tcp\nch11_coffee_api_coffee_5   ./entrypoint.sh   Up      0.0.0.0:32772->3000/tcp\nNote that the coffee service binds to port 0 on your host, which is an ephemeral port, which just means that your host machine assigns the service to a random port. This is required if you plan on using docker compose up --scale\nThe service was bound to port 0 on the host with\ncoffee:\n  build: ./coffee\n  user: 777:777\n  restart: always\n  expose:\n    - 3000\n  ports:\n    - \"0:3000\"\n...\n\nLoad balancer\n\nProblem with this kind of scaling is you don‚Äôt know the ports in advance , and you don‚Äôt want to hit these individual endpoints, you need a load balancer. This blog post shows you how to luse NGINX as a load balancer.\nYou will need something like this in your compose file\n  nginx:\n    image: nginx:latest\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - pspdfkit\n    ports:\n      - \"4000:4000\"\n\n\nTemplating Docker Compose Files\nYou can read about this here: Share Compose configurations between files and projects | Docker Documentation, allows you to override certain things from a base compose file.\n\n\n\nChapter 12 Clusters w/Machine & Swarm\nHamel: I skipped this completely"
  },
  {
    "objectID": "notes/serving/fastapi/index.html",
    "href": "notes/serving/fastapi/index.html",
    "title": "FastAPI",
    "section": "",
    "text": "FastAPI is a web framework for Python. People like to use this framework for serving prototypes of ML models."
  },
  {
    "objectID": "notes/serving/fastapi/index.html#impressions",
    "href": "notes/serving/fastapi/index.html#impressions",
    "title": "FastAPI",
    "section": "Impressions",
    "text": "Impressions\n\nModel serving frameworks (TF Serving, TorchServe, etc) are probably the way to go for production / enterprise deployments, especially for larger models. They offer more features, and latency will be more predictable (even if slower). I think that for smaller models (< 200MB) FastAPI is fine.\nIt is super easy to get started with FastAPI.\nI was able to confirm Sayak‚Äôs Benchmark where FastAPI is faster than TF Serving, but also less consistent overall. FastAPI is also more likely to fail, although I haven‚Äôt been able to cause that. In my experiments FastAPI was much faster for this small model, but this could change with larger models.\nMemory is consumed linearly as you increase the number of Uvicorn workers. Model serving frameworks like TF-Serving seem to work more efficiently. You should be careful to set the environment variable TF_FORCE_GPU_ALLOW_GROWTH=true if you are running inference on GPUs. I think in many cases you would be doing inference on CPUs, so this might not be relevant most of the time.\nFastAPI seems like it could be really nice on smaller models and scoped hardware where there is only one worker per node and you load balance across nodes (because you aren‚Äôt replicating the model with each worker).\nDebugging FastAPI is amazing, as its pure python and you get a nice docs page at http://<IP>/docs that lets you test out your endpoints right on the page! The documentation for FastPI is also amazing.\nIf you want the request parameters to be sent in the body (as you often do with ML b/c you want to send data to be scored), you have to use Pydantic. This is very opinionated, but easy enough to use."
  },
  {
    "objectID": "notes/serving/fastapi/index.html#load-model-make-predictions",
    "href": "notes/serving/fastapi/index.html#load-model-make-predictions",
    "title": "FastAPI",
    "section": "Load Model & Make Predictions",
    "text": "Load Model & Make Predictions\nGoing to use the model trained in the TF Serving tutorial. Furthermore, we are going to load this from the SavedModel format.\n\n# this cell is exported to a script\n\nfrom fastapi import FastAPI, status\nfrom pydantic import BaseModel\nfrom typing import List\nimport tensorflow as tf\nimport numpy as np\n\ndef load_model(model_path='/home/hamel/hamel/notes/serving/tfserving/model/1'):\n    \"Load the SavedModel Object.\"\n    sm = tf.saved_model.load(model_path)\n    return sm.signatures[\"serving_default\"] # this is the default signature when you save a model\n\n\n# this cell is exported to a script\n\ndef pred(model: tf.saved_model, data:np.ndarray, pred_layer_nm='dense_3'):\n    \"\"\"\n    Make a prediction from a SavedModel Object.  `pred_layer_nm` is the last layer that emits logits.\n    \n    https://www.tensorflow.org/guide/saved_model\n    \"\"\"\n    data = tf.convert_to_tensor(data, dtype='int32')\n    preds = model(data)\n    return preds[pred_layer_nm].numpy().tolist()\n\n\nTest Data\n\n_, (x_val, _) = tf.keras.datasets.imdb.load_data(num_words=20000)\nx_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=200)[:2, :]\n\n\n\nMake a prediction\n\nmodel = load_model()\npred(model, x_val[:2, :])\n\n[[0.8761785626411438, 0.12382148206233978],\n [0.0009457750129513443, 0.9990542531013489]]"
  },
  {
    "objectID": "notes/serving/fastapi/index.html#build-the-fastapi-app",
    "href": "notes/serving/fastapi/index.html#build-the-fastapi-app",
    "title": "FastAPI",
    "section": "Build The FastApi App",
    "text": "Build The FastApi App\n\n# this cell is exported to a script\n\napp = FastAPI()\n\nitems = {}\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"Load the model on startup https://fastapi.tiangolo.com/advanced/events/\"\n    items['model'] = load_model()\n\n\n@app.get(\"/\")\ndef health(status_code=status.HTTP_200_OK):\n    \"A health-check endpoint\"\n    return 'Ok'\n\nWe want to send the data for prediction in the Request Body (not with path parameters). According the docs:\n\nFastAPI will recognize that the function parameters that match path parameters should be taken from the path, and that function parameters that are declared to be Pydantic models should be taken from the request body.\n\n\n# this cell is exported to a script\n\nclass Sentence(BaseModel):\n    tokens: List[List[int]]\n\n@app.post(\"/predict\")\ndef predict(data:Sentence, status_code=status.HTTP_200_OK):\n    preds = pred(items['model'], data.tokens)\n    return preds"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html",
    "href": "notes/serving/tfserving/gpu.html",
    "title": "GPUs & Batching",
    "section": "",
    "text": "Warning\n\n\n\nI was not able to simulate a situation where dynamic batching is better than not batching. Apparently it can take time and lots of experiments to get right. Follow this guide for more information. This is a topic I may revisit in the future.\n\n\nAccording to the docs:\n\nModel Server has the ability to batch requests in a variety of settings in order to realize better throughput. The scheduling for this batching is done globally for all models and model versions on the server to ensure the best possible utilization of the underlying resources no matter how many models or model versions are currently being served by the server. You can enable this by using the --enable_batching flag and control it with the --batching_parameters_file.\n\nThis is an example batching parameters file:\n\n%%writefile batch-config.cfg\nmax_batch_size { value: 1000 }\nbatch_timeout_micros { value: 1000 }\nmax_enqueued_batches { value: 16 }\nnum_batch_threads { value: 16 }\n\nOverwriting batch-config.cfg\n\n\n\n\n\n\n\n\nGuidance on batch configuration\n\n\n\nGuidance for these config files are here there is no ‚Äúright answer‚Äù. For GPUs, the guidance is this:\nGPU: One Approach\nIf your model uses a GPU device for part or all of your its inference work, consider the following approach:\n\nSet num_batch_threads to the number of CPU cores.\nTemporarily set batch_timeout_micros to a really high value while you tune max_batch_size to achieve the desired balance between throughput and average latency. Consider values in the hundreds or thousands.\nFor online serving, tune batch_timeout_micros to rein in tail latency. The idea is that batches normally get filled to max_batch_size, but occasionally when there is a lapse in incoming requests, to avoid introducing a latency spike it makes sense to process whatever‚Äôs in the queue even if it represents an underfull batch. The best value for batch_timeout_micros is typically a few milliseconds, and depends on your context and goals. Zero is a value to consider; it works well for some workloads. (For bulk processing jobs, choose a large value, perhaps a few seconds, to ensure good throughput but not wait too long for the final (and likely underfull) batch.)"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#test-the-server",
    "href": "notes/serving/tfserving/gpu.html#test-the-server",
    "title": "GPUs & Batching",
    "section": "Test the server",
    "text": "Test the server\nThe model we are going to serve is generated in this note.\nI‚Äôm going to start two TF Serving instances, one thats regular CPU and one that does batching on GPU. I‚Äôm running both commands from the /home/hamel/tf-serving/ directory.\n\nCPU Version\ndocker run \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model/,target=/models/model \\\n--net=host -t tensorflow/serving --grpc_max_threads=1000\n\n\n\n\n\n\nNote\n\n\n\n--net=host binds all ports to the host, which is convenient for testing\n\n\nTest the CPU version:\n\n! curl http://localhost:8501/v1/models/model\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"1\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\n\n\nGPU Version\n\n\nPre-requisites\nYou must install nvidia-docker first\n\n\nDocker Command\nYou can pass additional arguments like --enable_batching to the docker run ... command just like you would if you were running tfserving locally.\nNote that we need the --gpus all flag to enable GPUs with nvidia-Docker. Furthermore, use the latest-gpu tag to enable GPUs as well as the --port and --rest_api_port so that it doesn‚Äôt conflict with the other tf serving instance I have running:\ndocker run --gpus all \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models \\\n--net=host -t tensorflow/serving:latest-gpu --enable_batching \\\n--batching_parameters_file=/models/batch-config.cfg --port=8505 \\\n--rest_api_port=8506 --grpc_max_threads=1000\n\n\n\n\n\n\n--grpc_max_threads flag\n\n\n\nI found that in non-batch mode I can easily overwhelm the server with gRPC requests. I wasn‚Äôt able to overwhelm the server over REST. Setting --grpc_max_threads=1000 takes care of this.\n\n\n\n\n\n\n\n\nOther flags\n\n\n\nThere are lots of flags. Hannes uses these additional ones, and they seem to make things a bit faster.\n--enable_model_warmup  \\\n--tensorflow_intra_op_parallelism=4 \\\n--tensorflow_inter_op_parallelism=4\n\n\n\n\n\n\n\n\nUnderstanding the volume mount\n\n\n\nOn the host, the config file is located at /home/hamel/hamel/notes/serving/tfserving/batch-config.cfg and the model is located at /home/hamel/hamel/notes/serving/tfserving/model/\nThe Docker file will try to import the model like this:\n# Set where models should be stored in the container\nENV MODEL_BASE_PATH=/models\nRUN mkdir -p ${MODEL_BASE_PATH}\n\n# The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=model\n\n# Create a script that runs the model server so we can use environment variables\n# while also passing in arguments from the docker command line\nRUN echo '#!/bin/bash \\n\\n\\\ntensorflow_model_server --port=8500 --rest_api_port=8501 \\\n--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n&& chmod +x /usr/bin/tf_serving_entrypoint.sh\nBy default it will try to get models from ${MODEL_BASE_PATH}/${MODEL_NAME} which is /models/model. So when we mount /home/hamel/hamel/notes/serving/tfserving from the host to /models in the container.\nIn the container:\n\nThe model files will be available at models/model as expected\nThe config file will be available at models/batch-config.cfg\n\n\n\nTest the TF-Serving GPU api:\n\n! curl http://localhost:8506/v1/models/model\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"1\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#prepare-the-data",
    "href": "notes/serving/tfserving/gpu.html#prepare-the-data",
    "title": "GPUs & Batching",
    "section": "Prepare the data",
    "text": "Prepare the data\n\nfrom tensorflow import keras\n\nvocab_size = 20000  # Only consider the top 20k words\nmaxlen = 200  # Only consider the first 200 words of each movie review\n\n_, (x_val, _) = keras.datasets.imdb.load_data(num_words=vocab_size)\nx_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n\nsample_data = x_val[:5, :]\ndata = [sample_data] * 10000"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#the-prediction-code",
    "href": "notes/serving/tfserving/gpu.html#the-prediction-code",
    "title": "GPUs & Batching",
    "section": "The prediction code",
    "text": "The prediction code\n\nimport json, requests\nimport numpy as np\n\nfrom fastcore.parallel import parallel\nfrom functools import partial\nparallel_pred = partial(parallel, threadpool=True, n_workers=500)\n\n\ndef predict_rest(data, port):\n    json_data = json.dumps(\n    {\"signature_name\": \"serving_default\", \"instances\": data.tolist()}\n    )\n    url = f\"http://localhost:{port}/v1/models/model:predict\"\n\n    json_response = requests.post(url, data=json_data)\n    response = json.loads(json_response.text)\n    rest_outputs = np.array(response[\"predictions\"])\n    return rest_outputs\n\n\nrest_outputs = predict_rest(sample_data, '8501')\nrest_outputs\n\narray([[0.89650154, 0.10349847],\n       [0.00330466, 0.9966954 ],\n       [0.13089457, 0.8691054 ],\n       [0.49083445, 0.50916553],\n       [0.0377177 , 0.96228224]])\n\n\n\ngRPC\nThis is the code that will be used to make gRPC prediction requests. For more discussion about gRPC, see this note\n\nimport grpc\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n# Create a channel that will be connected to the gRPC port of the container\n\n\n\ndef predict_grpc(data, input_name='input_1', port='8505'):\n    \n    options = [('grpc.max_receive_message_length', 100 * 1024 * 1024)]\n    channel = grpc.insecure_channel(f\"localhost:{port}\", options=options) # the gRPC port for the GPU server was set at 8505\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n    # Create a gRPC request made for prediction\n    request = predict_pb2.PredictRequest()\n\n    # Set the name of the model, for this use case it is \"model\"\n    request.model_spec.name = \"model\"\n\n    # Set which signature is used to format the gRPC query\n    # here the default one \"serving_default\"\n    request.model_spec.signature_name = \"serving_default\"\n\n    # Set the input as the data\n    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data))\n\n    # Send the gRPC request to the TF Server\n    result = stub.Predict(request)\n    return result"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#cpu-server",
    "href": "notes/serving/tfserving/gpu.html#cpu-server",
    "title": "GPUs & Batching",
    "section": "CPU Server",
    "text": "CPU Server\nThe CPU server is running on port 8501.\n\nREST CPU\nThe REST API endpoint on the CPU-bound server.\n\ncpu_pred = partial(predict_rest, port = '8501')\n\n\n%%time\nresults = parallel_pred(cpu_pred, data)\n\nCPU times: user 27.7 s, sys: 5.56 s, total: 33.3 s\nWall time: 26 s\n\n\n\n\ngrpc CPU\nThis is using the same CPU-bound TF Serving server, but is hitting the gRPC endpoint.\n\npredict_grpc_cpu = partial(predict_grpc, port='8500')\n\n\n%%time\nresults = parallel_pred(predict_grpc_cpu, data)\n\nCPU times: user 7.5 s, sys: 2.33 s, total: 9.84 s\nWall time: 7.63 s"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#gpu-server-with-batching",
    "href": "notes/serving/tfserving/gpu.html#gpu-server-with-batching",
    "title": "GPUs & Batching",
    "section": "GPU Server with batching",
    "text": "GPU Server with batching\nThe GPU server is running on port 8506 (we already started it above).\n\nREST\n\ngpu_pred = partial(predict_rest, port = '8506')\n\n\n%%time\nresults = parallel_pred(gpu_pred, data)\n\nCPU times: user 27.1 s, sys: 3.44 s, total: 30.6 s\nWall time: 27 s\n\n\n\n\ngRPC with batch\nThis is much faster than the REST endpoint! This is also much faster than the CPU version on this specific example. However, the batching part doesn‚Äôt appear to be providing any speedup at all, because the non-batch gRPC version is almost the same speed (if not a little bit faster).\n\n%%time\nresult = parallel(predict_grpc, data)\n\nCPU times: user 2.71 s, sys: 551 ms, total: 3.26 s\nWall time: 6.6 s"
  },
  {
    "objectID": "notes/serving/tfserving/gpu.html#gpu-server-without-batching",
    "href": "notes/serving/tfserving/gpu.html#gpu-server-without-batching",
    "title": "GPUs & Batching",
    "section": "GPU server without batching",
    "text": "GPU server without batching\ndocker run --gpus all --mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models --net=host -t tensorflow/serving:latest-gpu --port=8507 --rest_api_port=8508\n\nREST\n\ngpu_pred_no_batch = partial(predict_rest, port = '8508')\n\n\n%%time\nresults = parallel_pred(gpu_pred_no_batch, data)\n\nCPU times: user 26.9 s, sys: 3.61 s, total: 30.5 s\nWall time: 25.7 s\n\n\n\n\ngRPC without batching\nWhen I initially did this I got an error that said ‚ÄúResources Exhausted‚Äù. I was able to solve this by increasing the threads with the flag --grpc_max_threads=1000 when running the Docker container.\n\npredict_grpc_no_batch = partial(predict_grpc, port='8507')\n\n\n%%time\nresult = parallel_pred(predict_grpc_no_batch, data)\n\nCPU times: user 5.06 s, sys: 1.42 s, total: 6.48 s\nWall time: 6.65 s"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html",
    "href": "notes/serving/tfserving/tf-serving-basics.html",
    "title": "Basics",
    "section": "",
    "text": "These notes use code from here and this tutorial on tf serving."
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#create-the-model",
    "href": "notes/serving/tfserving/tf-serving-basics.html#create-the-model",
    "title": "Basics",
    "section": "Create The Model",
    "text": "Create The Model\n\n\n\n\n\n\nNote\n\n\n\nI didn‚Äôt want to use an existing model file from a tfserving tutorial, so I‚Äôm creating a new model from scratch.\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nfrom train import get_model\n\nvocab_size = 20000  # Only consider the top 20k words\nmaxlen = 200  # Only consider the first 200 words of each movie review\nembed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\n\n(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\nprint(len(x_train), \"Training sequences\")\nprint(len(x_val), \"Validation sequences\")\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n\n25000 Training sequences\n25000 Validation sequences\n\n\n\n\n\n\n\n\nImportant\n\n\n\nget_model is defined here\n\n\n\nmodel = get_model(maxlen=maxlen, vocab_size=vocab_size, \n                  embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should be careful to specify dtype properly for the input layer, so that the tfserving api validation will work properly. Like this:\ninputs = layers.Input(shape=(maxlen,), dtype='int32')\n\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 200)]             0         \n                                                                 \n token_and_position_embeddin  (None, 200, 32)          646400    \n g (TokenAndPositionEmbeddin                                     \n g)                                                              \n                                                                 \n transformer_block (Transfor  (None, 200, 32)          10656     \n merBlock)                                                       \n                                                                 \n global_average_pooling1d (G  (None, 32)               0         \n lobalAveragePooling1D)                                          \n                                                                 \n dropout_2 (Dropout)         (None, 32)                0         \n                                                                 \n dense_2 (Dense)             (None, 20)                660       \n                                                                 \n dropout_3 (Dropout)         (None, 20)                0         \n                                                                 \n dense_3 (Dense)             (None, 2)                 42        \n                                                                 \n=================================================================\nTotal params: 657,758\nTrainable params: 657,758\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nTrain Model\n\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n)\n\nEpoch 1/2\n782/782 [==============================] - 49s 58ms/step - loss: 0.3977 - accuracy: 0.8056 - val_loss: 0.2856 - val_accuracy: 0.8767\nEpoch 2/2\n782/782 [==============================] - 19s 24ms/step - loss: 0.1962 - accuracy: 0.9258 - val_loss: 0.3261 - val_accuracy: 0.8608\n\n\n\n\nSave Model\nYou can serialize your tensorflow models to a SavedModel format using tf.saved_model.save(...). This format is documented here. We are saving two versions of the model in order to discuss features of how TF Serving can serve multiple model versions.\n\n!rm -rf ./model\n\n\ndef save_model(model_version, model_dir=\"./model\"):\n\n    model_export_path = f\"{model_dir}/{model_version}\"\n\n    tf.saved_model.save(\n        model,\n        export_dir=model_export_path,\n    )\n\n    print(f\"SavedModel files: {os.listdir(model_export_path)}\")\n\nsave_model(model_version=1)\nsave_model(model_version=2)\n\nWARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ./model/1/assets\n\n\nINFO:tensorflow:Assets written to: ./model/1/assets\n\n\nSavedModel files: ['fingerprint.pb', 'variables', 'assets', 'saved_model.pb']\n\n\nWARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ./model/2/assets\n\n\nINFO:tensorflow:Assets written to: ./model/2/assets\n\n\nSavedModel files: ['fingerprint.pb', 'variables', 'assets', 'saved_model.pb']\n\n\nModel versioning is done by saving your model into a directory with an integer. By default, the directory with the highest integer will be served. You can change this with config files.\n\n!ls model/\n\n1  2"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#validate-the-api-schema",
    "href": "notes/serving/tfserving/tf-serving-basics.html#validate-the-api-schema",
    "title": "Basics",
    "section": "Validate the API Schema",
    "text": "Validate the API Schema\nThe output of the below command will show the input schema and shape, as well as the output shape of the API we will create with tfserving.\nThie below flags are mostly boilerplate. I don‚Äôt know what signature really means just yet.\n\n!saved_model_cli show --dir ./model/2 --tag_set serve --signature_def serving_default\n\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['input_1'] tensor_info:\n      dtype: DT_INT32\n      shape: (-1, 200)\n      name: serving_default_input_1:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['dense_3'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 2)\n      name: StatefulPartitionedCall:0\nMethod name is: tensorflow/serving/predict"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#launch-the-docker-container",
    "href": "notes/serving/tfserving/tf-serving-basics.html#launch-the-docker-container",
    "title": "Basics",
    "section": "Launch the docker container",
    "text": "Launch the docker container\nThe TFServing docs really want you to use docker. But you can use the CLI tensorflow_model_server instead, which is what is packaged in the Docker container. This is what their docs say:\n\nThe easiest and most straight-forward way of using TensorFlow Serving is with Docker images. We highly recommend this route unless you have specific needs that are not addressed by running in a container.\n\n\nTIP: This is also the easiest way to get TensorFlow Serving working with GPU support.\n\nIt worth looking at The Dockerfile for TFServing:\nENV MODEL_BASE_PATH=/models\nRUN mkdir -p ${MODEL_BASE_PATH}\n\n# The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=model\n\n\nRUN echo '#!/bin/bash \\n\\n\\\ntensorflow_model_server --port=8500 --rest_api_port=8501 \\\n--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n\nthis means that it is looking in /models/model by default. We can consider this when mounting the local model directory into the container.\nSuppose my local model is located at /home/hamel/hamel/notes/serving/tfserving/model. This is how you would run the Docker container:\ndocker run -p 8500:8500 \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model,target=/models/model \\\n--net=host -t tensorflow/serving\n\nTFServing on a GPU\nSee the note on using GPUs in TF Serving.\nHowever, it probably only makes sense to enable the GPU if you are going to enable batching, or if a single prediction are GPU intensive (like Stable Diffusion)"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#testing-the-api",
    "href": "notes/serving/tfserving/tf-serving-basics.html#testing-the-api",
    "title": "Basics",
    "section": "Testing the API",
    "text": "Testing the API\nAccording to the documentation we can see the status of our model like this:\nGET http://host:port/v1/models/${MODEL_NAME}, which for us is:\ncurl https://localhost:8501/v1/models/model\n\n! curl http://localhost:8501/v1/models/model\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"2\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\nNote how this shows the highest version number by default. You can access different model versions through different endpoints and supplying the right config files."
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#model-versioning",
    "href": "notes/serving/tfserving/tf-serving-basics.html#model-versioning",
    "title": "Basics",
    "section": "Model Versioning",
    "text": "Model Versioning\nModels that you save into the directory have a version number, for example our model is saved at home/hamel/hamel/notes/serving/tfserving/model with directories with versions 1 and 2.\n\n!ls /home/hamel/hamel/notes/serving/tfserving/model\n\n1  2\n\n\nBy default, TF Serving will always serve the model with the highest version number. However, you can change that with a model server config. You can also serve multiple versions of a model, add labels to models, etc. This is probably one of the most useful aspects of TF Serving. Here are some configs that allow you to serve multiple versions at the same time:\n\n%%writefile ./model/models.config\n\n\nmodel_config_list {\n config {\n    name: 'model'\n    base_path: '/models/model/'\n    model_platform: 'tensorflow'\n    model_version_policy: {all: {}}\n        }\n}\n\nOverwriting ./model/models.config\n\n\nIf you wanted to specify specific models to serve, you could name the versions instead of specifying all like this:\n\n%%writefile ./model/models-specific.config\n\nmodel_config_list {\n config {\n    name: 'model'\n    base_path: '/models/model/'\n    model_platform: 'tensorflow'\n    model_version_policy {\n      specific {\n        versions: 1\n        versions: 2\n      }\n    }\n  }\n}\n\nOverwriting ./model/models-specific.config\n\n\nTo read the config files, we need to pass these additional flags when running the container:\ndocker run \\\n--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model,target=/models/model \\\n--net=host \\\n-t tensorflow/serving \\\n--model_config_file=/models/model/models-specific.config \\\n--model_config_file_poll_wait_seconds=60 \nThe flag --model_config_file_poll_wait_seconds=60 tells the server to check for a new config file at the path every 60 seconds. This is optional but likely a good idea so you can change your config file without rebooting the server.\nTo access a specific version of the model, you would make a request to\nhttp://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict. For example, for version 1 the endpoint would be http://localhost:8501/v1/models/model/versions/1:predict.\nIf you did not care about the version, and just wanted the highest version we can use the general endpoint without the version which will serve the highest version by default:\nhttp://localhost:8501/v1/models/model:predict\nWe can test that all of these version is avialable to serve like so:\n\n! curl http://localhost:8501/v1/models/model/versions/2\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"2\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\n\n! curl http://localhost:8501/v1/models/model/versions/1\n\n{\n \"model_version_status\": [\n  {\n   \"version\": \"1\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTF Serving doesn‚Äôt make all versions available by default, only the latest one (with the highest number). You have to supply a config file if you want multiple versions to be made available at once. You probably should use labels to make URLs consistent in production scenarios."
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#rest",
    "href": "notes/serving/tfserving/tf-serving-basics.html#rest",
    "title": "Basics",
    "section": "REST",
    "text": "REST\nTime to make a prediction request. We will first try the REST API, which says the api endpoint is as follows: Note that v1 is just a hardcoded thing that has to do with the version of tfServing, not the version of the model:\nPOST http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict\n\nimport json, requests\nimport numpy as np\n\nsample_data = x_val[:2, :]\n\ndata = json.dumps(\n    {\"signature_name\": \"serving_default\", \"instances\": sample_data.tolist()}\n)\nurl = \"http://localhost:8501/v1/models/model:predict\" # this would be \"http://localhost:8501/v1/models/model/versions/1:predict\" for version 1\n\n\ndef predict_rest(json_data, url):\n    json_response = requests.post(url, data=json_data)\n    response = json.loads(json_response.text)\n    rest_outputs = np.array(response[\"predictions\"])\n    return rest_outputs\n\nrest_outputs = predict_rest(data, url)\n\n\nrest_outputs\n\narray([[0.94086391, 0.05913605],\n       [0.00317052, 0.99682945]])\n\n\n\nmodel_outputs = model.predict(sample_data)\n\n1/1 [==============================] - 0s 210ms/step\n\n\nLet‚Äôs compare this to our model‚Äôs output. It‚Äôs close enough :)\n\nassert np.allclose(rest_outputs, model_outputs, rtol=1e-4)"
  },
  {
    "objectID": "notes/serving/tfserving/tf-serving-basics.html#grpc",
    "href": "notes/serving/tfserving/tf-serving-basics.html#grpc",
    "title": "Basics",
    "section": "gRPC",
    "text": "gRPC\n\nThe payload format for grpc uses Protocol Buffers which are compressed better than JSON, which might make latency lower. This makes a difference for higher payload sizes, like images.\n\ngRPC has some kind of bi-directional streaming whereas REST is just a response/request model. I don‚Äôt know what this means.\ngRPC uses a newer HTTP protocol than REST. I don‚Äôt know what this means.\n\n\nimport grpc\n\n# Create a channel that will be connected to the gRPC port of the container\nchannel = grpc.insecure_channel(\"localhost:8500\")\n\n\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\nstub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n\n\n# Get the serving_input key\nloaded_model = tf.saved_model.load(model_export_path)\ninput_name = list(\n    loaded_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\n)[0]\n\n\ninput_name\n\n'input_1'\n\n\n\ndef predict_grpc(data, input_name, stub):\n    # Create a gRPC request made for prediction\n    request = predict_pb2.PredictRequest()\n\n    # Set the name of the model, for this use case it is \"model\"\n    request.model_spec.name = \"model\"\n\n    # Set which signature is used to format the gRPC query\n    # here the default one \"serving_default\"\n    request.model_spec.signature_name = \"serving_default\"\n\n    # Set the input as the data\n    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data))\n\n    # Send the gRPC request to the TF Server\n    result = stub.Predict(request)\n    return result\n\nsample_data = tf.convert_to_tensor(x_val[:2, :], dtype='int32')\n\ngrpc_outputs = predict_grpc(sample_data, input_name, stub)\n\n\nInspect the gRPC response\nWe can see all the fields that the gRPC response has. In this situation, the name of the final layer of our model will be the key that containst the predictions, which is dense_3 in this case.\n\ngrpc_outputs\n\noutputs {\n  key: \"dense_3\"\n  value {\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: 2\n      }\n      dim {\n        size: 2\n      }\n    }\n    float_val: 0.9408639073371887\n    float_val: 0.059136051684617996\n    float_val: 0.0031705177389085293\n    float_val: 0.9968294501304626\n  }\n}\nmodel_spec {\n  name: \"model\"\n  version {\n    value: 2\n  }\n  signature_name: \"serving_default\"\n}\n\n\nWe can also get the name of the last layer of the model like this:\n\nloaded_model.signatures[\"serving_default\"].structured_outputs\n\n{'dense_3': TensorSpec(shape=(None, 2), dtype=tf.float32, name='dense_3')}\n\n\n\n\nReshaping the Response\n\nshape = [x.size for x in grpc_outputs.outputs['dense_3'].tensor_shape.dim]\n\ngrpc_preds = np.reshape(grpc_outputs.outputs['dense_3'].float_val, shape)\ngrpc_preds\n\narray([[0.94086391, 0.05913605],\n       [0.00317052, 0.99682945]])\n\n\nThe predictions are close enough. I am not sure why they wouldn‚Äôt be exactly the same.\n\nassert np.allclose(model_outputs, grpc_preds,rtol=1e-4)"
  },
  {
    "objectID": "notes/serving/torchserve/basic-torchserve.html",
    "href": "notes/serving/torchserve/basic-torchserve.html",
    "title": "Basics",
    "section": "",
    "text": "The key to understanding TorchServe is to first understand torch-model-archiver which packages model artifacts into a single model archive file (.mar). torch-model-archive needs the following inputs:\n\n\nNeed a model checkpoint file\n\n\n\nNeed a model definition file and a state_dict file.\n\n\n\nThe CLI produces a .mar file. Below is an example of archiving an eager mode model.\n\n!torch-model-archiver --model-name densenet161 \\\n    --version 1.0 \\\n    --model-file ./_serve/examples/image_classifier/densenet_161/model.py \\\n    --serialized-file densenet161-8d451a50.pth \\\n    --export-path model_store \\\n    --extra-files ./_serve/examples/image_classifier/index_to_name.json \\\n    --handler image_classifier \\\n    -f\n\nWARNING - Overwriting model_store/densenet161.mar ...\n\n\nThis is the model file:\n\n\n_serve/examples/image_classifier/densenet_161/model.py\n\n\n\nOptions for model archiver:\n\n! torch-model-archiver --help\n\nusage: torch-model-archiver [-h] --model-name MODEL_NAME\n                            [--serialized-file SERIALIZED_FILE]\n                            [--model-file MODEL_FILE] --handler HANDLER\n                            [--extra-files EXTRA_FILES]\n                            [--runtime {python,python2,python3}]\n                            [--export-path EXPORT_PATH]\n                            [--archive-format {tgz,no-archive,default}] [-f]\n                            -v VERSION [-r REQUIREMENTS_FILE]\n\nTorch Model Archiver Tool\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --model-name MODEL_NAME\n                        Exported model name. Exported file will be named as\n                        model-name.mar and saved in current working directory if no --export-path is\n                        specified, else it will be saved under the export path\n  --serialized-file SERIALIZED_FILE\n                        Path to .pt or .pth file containing state_dict in case of eager mode\n                        or an executable ScriptModule in case of TorchScript or TensorRT\n                        or a .onnx file in the case of ORT.\n  --model-file MODEL_FILE\n                        Path to python file containing model architecture.\n                        This parameter is mandatory for eager mode models.\n                        The model architecture file must contain only one\n                        class definition extended from torch.nn.modules.\n  --handler HANDLER     TorchServe's default handler name\n                         or Handler path to handle custom inference logic.\n  --extra-files EXTRA_FILES\n                        Comma separated path to extra dependency files.\n  --runtime {python,python2,python3}\n                        The runtime specifies which language to run your inference code on.\n                        The default runtime is \"python\".\n  --export-path EXPORT_PATH\n                        Path where the exported .mar file will be saved. This is an optional\n                        parameter. If --export-path is not specified, the file will be saved in the\n                        current working directory. \n  --archive-format {tgz,no-archive,default}\n                        The format in which the model artifacts are archived.\n                        \"tgz\": This creates the model-archive in <model-name>.tar.gz format.\n                        If platform hosting TorchServe requires model-artifacts to be in \".tar.gz\"\n                        use this option.\n                        \"no-archive\": This option creates an non-archived version of model artifacts\n                        at \"export-path/{model-name}\" location. As a result of this choice, \n                        MANIFEST file will be created at \"export-path/{model-name}\" location\n                        without archiving these model files\n                        \"default\": This creates the model-archive in <model-name>.mar format.\n                        This is the default archiving format. Models archived in this format\n                        will be readily hostable on native TorchServe.\n  -f, --force           When the -f or --force flag is specified, an existing .mar file with same\n                        name as that provided in --model-name in the path specified by --export-path\n                        will overwritten\n  -v VERSION, --version VERSION\n                        Model's version\n  -r REQUIREMENTS_FILE, --requirements-file REQUIREMENTS_FILE\n                        Path to a requirements.txt containing model specific python dependency\n                         packages.\n\n\n\n\n\nTorchServe has the following handlers built-in that do post and pre-processing:\n\nimage_classifier\nobject_detector\ntext_classifier\nimage_segmenter\n\nYou can implement your own custom handler by following these docs. Most of the time you only need to subclass BaseHandler and override preprocess and/or postprocess.\n\n\nFrom the docs:\n\nimage_classifier, text_classifier and object_detector can all automatically map from numeric classes (0,1,2‚Ä¶) to friendly strings. To do this, simply include in your model archive a file, index_to_name.json, that contains a mapping of class number (as a string) to friendly name (also as a string)."
  },
  {
    "objectID": "notes/serving/torchserve/basic-torchserve.html#serving",
    "href": "notes/serving/torchserve/basic-torchserve.html#serving",
    "title": "Basics",
    "section": "Serving",
    "text": "Serving\nAfter archiving you can start the modeling server:\ntorchserve --start --ncs \\\n    --model-store model_store \\\n    --models densenet161.mar\nTorchServe uses default ports 8080 / 8081 / 8082 for REST based inference, management & metrics APIs and 7070 / 7071 for gRPC APIs.\n\n!torchserve --help\n\nusage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG]\n                  [--model-store MODEL_STORE]\n                  [--workflow-store WORKFLOW_STORE]\n                  [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\n                  [--log-config LOG_CONFIG] [--foreground]\n                  [--no-config-snapshots] [--plugins-path PLUGINS_PATH]\n\nTorchserve\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         Return TorchServe Version\n  --start               Start the model-server\n  --stop                Stop the model-server\n  --ts-config TS_CONFIG\n                        Configuration file for model server\n  --model-store MODEL_STORE\n                        Model store location from where local or default\n                        models can be loaded\n  --workflow-store WORKFLOW_STORE\n                        Workflow store location from where local or default\n                        workflows can be loaded\n  --models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]\n                        Models to be loaded using [model_name=]model_location\n                        format. Location can be a HTTP URL or a model archive\n                        file in MODEL_STORE.\n  --log-config LOG_CONFIG\n                        Log4j configuration file for model server\n  --foreground          Run the model server in foreground. If this option is\n                        disabled, the model server will run in the background.\n  --no-config-snapshots, --ncs\n                        Prevents to server from storing config snapshot files.\n  --plugins-path PLUGINS_PATH, --ppath PLUGINS_PATH\n                        plugin jars to be included in torchserve class path\n\n\n\n!curl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  7341  100  7341    0     0   108k      0 --:--:-- --:--:-- --:--:--  108k\n\n\n\n!curl http://127.0.0.1:8080/predictions/densenet161 -T kitten_small.jpg\n\n{\n  \"tabby\": 0.4783327877521515,\n  \"lynx\": 0.19989627599716187,\n  \"tiger_cat\": 0.1682717651128769,\n  \"tiger\": 0.061949197202920914,\n  \"Egyptian_cat\": 0.05116736516356468\n}\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI wouldn‚Äôt recommend installing torchserve and running it on a VM. It‚Äôs probably easier to use Docker.\ndocker pull pytorch/torchserve\n\n\n\nDocker\nSee these docs. We have to mount the necessary files and run the same commands. We also have to expose all the ports, etc.\n\n\n\n\n\n\nImportant\n\n\n\nNote that you have to supply the torchserve command, which implies you can run other things (but I don‚Äôt know what those are).\n\n\ndocker run --rm -it --gpus '\"device=0\"' \\\n    -p 8080:8080 \\\n    -p 8081:8081 \\\n    -p 8082:8082 \\\n    -p 7070:7070 \\\n    -p 7071:7071 \\\n    --mount type=bind,source=/home/hamel/hamel/notes/serving/torchserve/model_store,target=/tmp/models \\\n    pytorch/torchserve:latest-gpu \\\n    torchserve \\\n    --model-store /tmp/models \\\n    --models densenet161.mar\n\n!curl http://127.0.0.1:8080/predictions/densenet161 -T kitten_small.jpg\n\n{\n  \"tabby\": 0.4783327877521515,\n  \"lynx\": 0.19989627599716187,\n  \"tiger_cat\": 0.1682717651128769,\n  \"tiger\": 0.061949197202920914,\n  \"Egyptian_cat\": 0.05116736516356468\n}"
  },
  {
    "objectID": "notes/serving/torchserve/basic-torchserve.html#other-notes",
    "href": "notes/serving/torchserve/basic-torchserve.html#other-notes",
    "title": "Basics",
    "section": "Other Notes",
    "text": "Other Notes\nI found these articles to be very important:\n\nSource code for BaseHandler.\nPerformance guide: Concurrency and number of workers.\nconfig.properties example 1 and example 2 of how you can pass configuration files"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html",
    "href": "notes/serving/torchserve/hf.html",
    "title": "Serving Your Own Model",
    "section": "",
    "text": "Before we try to load models into Torch Serve, I‚Äôm going to download two different HuggingFace models and make sure I can do inference in a notebook.\n\n\nGPT-2 looks archaic compared to GPT-3\n\nfrom transformers import pipeline\npipe = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n\n\npreds = pipe([\"How do you use Torch Serve for model inference?\", \n              \"The quick brown fox jumps over the lazy\"])\npreds\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n[[{'generated_text': 'How do you use Torch Serve for model inference? Or just use Http to help. Or as a framework, where the actual code runs as expected, like something like JRuby on top of the project? Or maybe you use some way to get'}],\n [{'generated_text': 'The quick brown fox jumps over the lazy wolf, then hops over the hoot and follows him.'}]]\n\n\n\n\n\nThis definitely requires a GPU\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\n\nrepo_id = \"stabilityai/stable-diffusion-2\"\npipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=\"fp16\")\n\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda:0\")\n\n\n\n\n\nprompt = \"A Butterly in space\"\nimage = pipe(prompt, num_inference_steps=25)\nimg = image.images[0]\nimg\n\n\n\n\n\n\n\nYou can convert PIL to JSON serializable structures like this:\n\nimport numpy as np\nimg_as_list = np.array(img).tolist()"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#gpt-handler",
    "href": "notes/serving/torchserve/hf.html#gpt-handler",
    "title": "Serving Your Own Model",
    "section": "GPT Handler",
    "text": "GPT Handler\n\n%%writefile gpt_handler.py\n#gpt_handler.py\nimport logging\nimport torch\nfrom transformers import pipeline\nfrom ts.torch_handler.base_handler import BaseHandler\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Starting GPT Handler\")\n\nclass GptHandler(BaseHandler):\n    def __init__(self):\n        self.initialized = False\n        \n    def initialize(self, ctx):\n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        \n        self.device = torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\"))\n            if torch.cuda.is_available() and properties.get(\"gpu_id\") is not None\n            else \"cpu\"\n        )\n        \n        # you might normaly get the model from disk, but we don't have to in this case.\n        self.pipe = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n        self.initialized = True\n        \n    def preprocess(self, data): \n        text = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n        logging.info(f'Here is the text: {text}')\n        sentences = text.decode('utf-8')\n        return sentences\n    \n    def inference(self, data): return self.pipe(data)\n    \n    def postprocess(self, data): return data\n\nOverwriting gpt_handler.py"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#diffusion-handler",
    "href": "notes/serving/torchserve/hf.html#diffusion-handler",
    "title": "Serving Your Own Model",
    "section": "Diffusion Handler",
    "text": "Diffusion Handler\n\n%%writefile diffusion_handler.py\n#diffusion_handler.py\nimport logging\nimport torch\nimport numpy as np\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom ts.torch_handler.base_handler import BaseHandler\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Starting Diffusion Handler\")\n\nclass DiffusionHandler(BaseHandler):\n    def __init__(self):\n        self.initialized = False\n        \n    def initialize(self, ctx):\n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        \n        self.device = torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\"))\n            if torch.cuda.is_available() and properties.get(\"gpu_id\") is not None\n            else \"cpu\"\n        )\n        \n        repo_id = \"stabilityai/stable-diffusion-2\"\n        self.pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=\"fp16\")\n        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n        self.pipe = self.pipe.to(self.device)\n        self.initialized = True\n        \n    def preprocess(self, data): \n        text = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n        prompt = text.decode('utf-8')\n        return prompt\n    \n    def inference(self, data): \n        image = self.pipe(data, num_inference_steps=25)\n        img = image.images[0]\n        return np.array(img)\n    \n    def postprocess(self, data): \n        return [data.tolist()]\n\nOverwriting diffusion_handler.py"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#create-the-model-archive",
    "href": "notes/serving/torchserve/hf.html#create-the-model-archive",
    "title": "Serving Your Own Model",
    "section": "Create the model archive",
    "text": "Create the model archive\n\n!mkdir -p model_store\n\n! torch-model-archiver \\\n--export-path model_store \\\n--model-name \"gpt\" --version 1.0 \\\n--handler \"./gpt_handler.py\" \\\n--force\n\n! torch-model-archiver \\\n--export-path model_store \\\n--model-name \"diffusion\" --version 1.0 \\\n--handler \"./diffusion_handler.py\" \\\n--force\n\nWARNING - Overwriting model_store/gpt.mar ...\nWARNING - Overwriting model_store/diffusion.mar ..."
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#set-config.properties",
    "href": "notes/serving/torchserve/hf.html#set-config.properties",
    "title": "Serving Your Own Model",
    "section": "Set config.properties",
    "text": "Set config.properties\nThe diffusion response is bigger than the allowable default so we must pass a config. An example is here. I don‚Äôt know why I have to set the different ports like this, since these are the defaults (If I do not set these, things do not work properly).\n\n%%writefile config/config.properties\ninference_address=http://0.0.0.0:8080\nmanagement_address=http://0.0.0.0:8081\nmetrics_address=http://0.0.0.0:8082\nload_models=all\nmax_response_size=655350000\n\nOverwriting config/config.properties"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#create-a-dockerfile",
    "href": "notes/serving/torchserve/hf.html#create-a-dockerfile",
    "title": "Serving Your Own Model",
    "section": "Create a Dockerfile",
    "text": "Create a Dockerfile\nWe can get ideas from their Dockerfile\n\n%%writefile Dockerfile.gpt\n\nFROM pytorch/torchserve:latest-gpu\nRUN python -m pip install transformers diffusers\n\nENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\nCMD [\"serve\"]\n\nOverwriting Dockerfile.gpt\n\n\nBuild the Dockerfile\n\n! docker build -f Dockerfile.gpt . -t pytorch/torchserve:gpu-hf;\n\nSending build context to Docker daemon  334.6MB\nStep 1/4 : FROM pytorch/torchserve:latest-gpu\n ---> 046086392ab2\nStep 2/4 : RUN python -m pip install transformers diffusers\n ---> Using cache\n ---> 13135ca5603f\nStep 3/4 : ENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\n ---> Using cache\n ---> 6910f9182230\nStep 4/4 : CMD [\"serve\"]\n ---> Using cache\n ---> bbed6fd312c2\nSuccessfully built bbed6fd312c2\nSuccessfully tagged pytorch/torchserve:gpu-hf"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#list-models",
    "href": "notes/serving/torchserve/hf.html#list-models",
    "title": "Serving Your Own Model",
    "section": "List Models",
    "text": "List Models\n\n!curl http://127.0.0.1:8081/models\n\n{\n  \"models\": [\n    {\n      \"modelName\": \"diffusion\",\n      \"modelUrl\": \"diffusion.mar\"\n    },\n    {\n      \"modelName\": \"gpt\",\n      \"modelUrl\": \"gpt.mar\"\n    }\n  ]\n}"
  },
  {
    "objectID": "notes/serving/torchserve/hf.html#make-predictions",
    "href": "notes/serving/torchserve/hf.html#make-predictions",
    "title": "Serving Your Own Model",
    "section": "Make Predictions",
    "text": "Make Predictions\n\nGPT\nWith curl\n\n! echo \"The quick brown fox jumps over the lazy\" > gpt.txt\n! cat gpt.txt\n\nThe quick brown fox jumps over the lazy\n\n\n\n!curl http://127.0.0.1:8080/predictions/gpt -T gpt.txt\n\n{\n  \"generated_text\": \"The quick brown fox jumps over the lazy\\nI do find some funny gifs to do.\\nI used to have cats but I never find one\\nI used to have dogs. But I couldn't really find a cute dog but now I enjoy\"\n}\n\n\nWith requests\n\nimport requests\nresp = requests.post('http://127.0.0.1:8080/predictions/gpt',\n                     data={'data': \"The quick brown fox jumps over the lazy\"})\nresp.text\n\n'{\\n  \"generated_text\": \"The quick brown fox jumps over the lazy blonde to win, the latter has to wait for the rest to come on, and she also needs a hug and a hug. The adorable feline can barely contain itself, but the kitten is quite responsive,\"\\n}'\n\n\n\n\nDiffusion\n\nimport requests\nimport json\nfrom PIL import Image\n\nresp = requests.post('http://127.0.0.1:8080/predictions/diffusion',\n                     data={'data': \"A butterfly in space with glasses.\"})\n\n\ndimg = np.array(json.loads(resp.text), dtype='uint8')\nImage.fromarray(dimg)"
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html",
    "href": "notes/k8s/19-Pod-Lifecycle.html",
    "title": "Pod restart vs.¬†replacement",
    "section": "",
    "text": "If you google Pod restart vs replacement, virutally every article conflates the two, but the distinction is very important!\nA good way to test if some event causes a restart vs a replacment is to see if the UID for the pod remains the same or not before vs.¬†after the event:\nA pod with the same UID is guaranteed to be running on the same node, since it has only been restarted.\nThis article on Pod lifecycle is helpful."
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html#what-causes-a-restart-vs-replacement",
    "href": "notes/k8s/19-Pod-Lifecycle.html#what-causes-a-restart-vs-replacement",
    "title": "Pod restart vs.¬†replacement",
    "section": "What causes a restart vs replacement",
    "text": "What causes a restart vs replacement\n\nrestart:\n\nfailed liveness probe (I confirmed with the UID that this restarts the Pod).\nWhen a container exits the pod will be restarted according to the restartPolicy in the podspec.\n\nreplacement:\n\nkubectl rollout restart Yes! It replaces the pod, I checked and the UID changes! Don‚Äôt get foooled by the word ‚Äúrestart‚Äù\ndeleting the resource (ex: kubectl delete deploy/...)\nscaling the resource to zero (ex: kubectl scale deployment ...)\nIf you change the podspec.\n\n\nIf unsure do some experiments!"
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html#forcing-a-container-to-exit",
    "href": "notes/k8s/19-Pod-Lifecycle.html#forcing-a-container-to-exit",
    "title": "Pod restart vs.¬†replacement",
    "section": "Forcing a container to exit",
    "text": "Forcing a container to exit\nYou can force a container to exit with the following command. This might be useful for testing:\nkl exec -it {pod name} -- killall5\nThis will cause the pod to restart the container, not replace it."
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html#storage-implications",
    "href": "notes/k8s/19-Pod-Lifecycle.html#storage-implications",
    "title": "Pod restart vs.¬†replacement",
    "section": "Storage Implications",
    "text": "Storage Implications\nStorage that exists at the Pod-level, like emptyDir will survive a Pod restart, but NOT a pod replacement:\n...\nspec:\n containers:\n   - name: myimage\n     image: repo/image\n     volumeMounts:\n      - name: data                 # Mounts a volume called data\n         mountPath: /data          # into the /data directory\n volumes:\n   - name: data                    # This is the data volume spec,\n     emptyDir: {}                  # which is the EmptyDir type.\nAny data stored in an EmptyDir volume remains in the Pod between restarts, so Pod‚Äôs that are restarted can access data written by their predecessors. An EmptyDir volume can be a reasonable source for a local cache because if the app crashes, then the replacement container will still have the cached files."
  },
  {
    "objectID": "notes/k8s/23-Logging.html",
    "href": "notes/k8s/23-Logging.html",
    "title": "Logging",
    "section": "",
    "text": "From Chapter 13\nK8s stores log entries in a directory on each node. If you want to combine all the logs, you need a system to do this, which can also be done on K8s. This is referred to as a ‚Äúlog collector‚Äù\nHow to see logs from all pods for an app?\nThe --all-containers gets all the logs from all the containers in the pod (remember that pods can have more than one container, if the pod has more than one container you have to specify the container name with -c <container name>). The -l app=timecheck is a label selector. The -n kiamol-ch13-dev is the namespace. The --tail 1 is to only show the last line of the log.\nAs a practical matter, it‚Äôs hard to use container logs directly, and you need a log collector."
  },
  {
    "objectID": "notes/k8s/23-Logging.html#logs-on-nodes",
    "href": "notes/k8s/23-Logging.html#logs-on-nodes",
    "title": "Logging",
    "section": "Logs on Nodes",
    "text": "Logs on Nodes\nLogs are stored in /var/log/containers on each node. We can use a HostPath volume mount to see what this looks like:\n...\n      containers:\n      - name: sleep\n        image: kiamol/ch03-sleep\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\nSince we mounted the node directory, we can see the logs at the path where they are stored on the node: /var/log/containers. The naming convention is <pod name>_<namespace>_<container name>-<container id>.log. The container ID is the first 12 characters of the container ID. The container ID is the same as the container ID in the docker ps output. These log files are in a JSON format, and can be parsed with jq.\n% kubectl exec -it deploy/sleep -- ls var/log/containers\ncoredns-95db45d46-ckb68_kube-system_coredns-d6e70a7dabdc81bcd18d83595ae92f577036912cbf5ccb36fbf46cd95476ba0f.log\ncoredns-95db45d46-pnm2p_kube-system_coredns-723c2ba38c0511eb3582a8ffdf852195ae47385fc5aaddf6444888d23265b1a9.log\netcd-docker-desktop_kube-system_etcd-e9bf1a84556e483e44aae7a6596daa125d7b46c2df442f7d26b45a32de62af07.log\nkube-apiserver-docker-desktop_kube-system_kube-apiserver-fc1afa677baa0aacfd6f9f9c5a675748ba2a383f17df4e99aaa8673116aa5a2e.log\nkube-controller-manager-docker-desktop_kube-system_kube-controller-manager-2d3eba90c4496a5256e7f3e29a7fecf4dc5922db3364b216023806921b156fc7.log\n...\nThe contents of one of thes json files look like this:\n{\n  \"log\": \"2020-05-18T14:56:00.000000Z\\tinfo\\tEpoch 0 starting\\n\",\n  \"stream\": \"stdout\",\n  \"time\": \"2020-05-18T14:56:00.000000000Z\"\n}\nThey are really like jsonl files where each line is its own record."
  },
  {
    "objectID": "notes/k8s/23-Logging.html#efk-stack",
    "href": "notes/k8s/23-Logging.html#efk-stack",
    "title": "Logging",
    "section": "EFK Stack",
    "text": "EFK Stack\nWe will use the EFK Stack: Elasticsearch, Fluentd, Kibana. This is a common logging stack for K8s.\n\nLog Collector: Fluentd\n\n\n\n\n\n\nNote\n\n\n\nWe didn‚Äôt go deep into FluentD or Fluent-Bit. This may not be that important in the large scheme as in the cloud you have toosl that help with this.\n\n\nFluent Bit is a lightweight version of FluentD. It uses a DaemonSet to run a pod on each node, which uses a HostPath volume mount to access the log files.\nIt reads the log files from the node and sends them to Elasticsearch. It can also parse the logs and add metadata to them. It can also send the logs to other destinations, such as Splunk or AWS CloudWatch.\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  namespace: kiamol-ch13-logging\n  labels:\n    kiamol: ch13\nspec:\n  selector:\n    matchLabels:\n      app: fluent-bit\n  template:\n    metadata:\n      labels:\n        app: fluent-bit\n    spec:\n      serviceAccountName: fluent-bit\n      containers:\n      - name: fluent-bit\n        image: fluent/fluent-bit:1.8.11\n        volumeMounts:\n        - name: fluent-bit-config\n          mountPath: /fluent-bit/etc/\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n      volumes:\n      - name: fluent-bit-config\n        configMap:\n          name: fluent-bit-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\nYou should create fluent-bit DaemonSet in a different namespace because you typically want it to run as a shared service used by all the applications running on the cluster. It doesn‚Äôt matter that its running in a different namespace because it is reading from each node so it doesn‚Äôt matter.\nThere are different pieces you can configure with FluentD\n\nWe‚Äôre currently running a simple configuration with three stages:\n\nthe input stage reads log files\nthe parser stage deconstructs the JSON log entries,\nand the output stage writes each log as a separate line to the standard output stream in the Fluent Bit container.\n\nBelow is part of a Fluent Bit configuration file:\nkind: ConfigMap\n...\n[INPUT]\n   Name              tail         # Reads from the end of a file\n   Tag               kube.*       # Uses a prefix for the tag\n   Path              /var/log/containers/timecheck*.log\n   Parser            docker       # Parses the JSON container logs\n   Refresh_Interval  10        # Sets the frequency to check the file list\n\n[OUTPUT]\n   Name            stdout         # Writes to standard out\n   Format          json_lines     # Formats each log as a line\n   Match           kube.*         # Writes logs with a kube tag prefix\nFluent Bit uses tags to identify the source of a log entry. The tag is added at the input stage and can be used to route logs to other stages. In this configuration, the log file name is used as the tag, prefixed with kube. The match rule routes all the kube tagged entries to the output stage so every log is printed out, but the input stage reads only the timecheck log files, so those are the only log entries you see.\nWhen you apply the configuration (revisit this chapter to see all the details), you can see the logs in the Fluent Bit container:\nkubectl logs  -l app=fluent-bit -n kiamol-ch13-logging --tail 2\nThis will show you logs from all namespaces, as fluentd is reading the logs from all nodes.\n\nRouting Output\nIf you look at the file fluentbit/update/fluentbit-config-match.yaml you will see a boilerplate config that is generic that will work with any cluster. (You just have to change the namespace and labels). You end up with tag that is kube.<namespace_name>.<container_name>.<pod_name>.<docker_id>- and then the log file name. Based on this, you can route output like so:\n```(.yml file=‚Äúfluentbit/update/fluentbit-config-match-multiple.yaml‚Äù) [OUTPUT] Name stdout # The standard out plugin will Format json_lines # print only log entries where Match kube.kiamol-ch13-test.* # the namespace is test.\n[OUTPUT] Name counter # The counter prints a count of Match kube.kiamol-ch13-dev.* # logs from the dev namespace.\n\nThis shows you that you can have different kinds of logs for different namespaces, for example the `dev` namespace is counting the lines of logs.  The counter is a plugin that is built into FluentD.\n\n### Elasticsearch\n\nElasticsearch is a database, where each datum can have different fields, there is no fixed schema.  You access it via a REST API.  Kibana is a web interface for Elasticsearch. Fluent Bit has an Elasticsearch output plugin that creates a document for each log entry using the Elasticsearch REST API. The plugin needs to be configured with the domain name of the Elasticsearch server, and you can optionally specify the index where documents should be created.  Note that if there are logs that don't match an output rule, they will be discarded:\n\n```{.yml filename=\"fluentbit-config-elasticsearch.yaml\"}\n[OUTPUT]\n   Name       es                            # Logs from the test namespace\n   Match      kube.kiamol-ch13-test.*       # are routed to Elasticsearch\n   Host       elasticsearch                 # and created as documents in \n   Index      test                          # the \"test\" index.\n\n[OUTPUT]\n   Name       es                            # System logs are created in\n   Match      kube.kube-system.*            # the \"sys\" index in the same\n   Host       elasticsearch                 # Elasticsearch server.\n   Index      sys\nWe deploy Elasticsearch and Kibana and get the endpoint of Kibana.\nI skipped the rest of this b/c I‚Äôm hoping cloud services provide a good logging stack for me. Also, you will likely want to use plugins for FluentD."
  },
  {
    "objectID": "notes/k8s/29-preemption.html",
    "href": "notes/k8s/29-preemption.html",
    "title": "Preemption",
    "section": "",
    "text": "Sometimes when a node is working too hard, K8s will preempt (evict) pods to allow the node to recover. K8s will also taint the node, so new pods will run there and remove the taint when the pressure eases.\nYou want to make sure your least important workloads are evicted during premption. That‚Äôs why you need to set the priority class for your pods. The higher the priority, the less likely it is to be evicted.\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000\nglobalDefault: false # should the value of this priority class be used for pods without a priorityClassName?\ndescription: \"This priority class should be used for XYZ service pods only.\"\nSetting globalDefault to true or false is important. Usually should be false.\nHow to set priority class for a pod:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  priorityClassName: high-priority"
  },
  {
    "objectID": "notes/k8s/13-JobsCron.html",
    "href": "notes/k8s/13-JobsCron.html",
    "title": "Jobs & CronJobs",
    "section": "",
    "text": "Thes are useful! You can run ad-hoc jobs to completion, or schedule something to run! Lots of DS workloads are like this.\n\nJobs aren‚Äôt just for stateful apps; they‚Äôre a great way to bring a standard approach to any batch-processing problems, where you can hand off all the scheduling and monitoring and retry logic to the cluster. You can run any container image in the Pod for a Job, but it should start a process that ends; otherwise, your jobs will keep running forever.\n\napiVersion: batch/v1\nkind: Job                           # Job is the object type.\nmetadata:\n name: pi-job\nspec:\n template:\n   spec:                           # The standard Pod spec\n     containers:\n       - name: pi                  # The container should run and exit.\n         image: kiamol/ch05-pi     \n         command: [\"dotnet\", \"Pi.Web.dll\", \"-m\", \"console\", \"-dp\", \"50\"]\n     restartPolicy: Never          # If the container fails, replace the Pod.\n\n\nRun the above and get logs\nkl apply -f pi/pi-job.yaml  # this is the filename  for the above\nkl logs jobs/pi-job\n\n\n\nits like a Pod standard spec, but there is an additional required field restartPolicy.\n\n\n\ncompletions: how many times should the job run.\nparallelism: How many Pods to run in parallel with multiple completions set.\n\n\n\n\n\nParallel Jobs with a work queue: - do not specify .spec.completions, default to .spec.parallelism. - the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue. - each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done. - when any Pod from the Job terminates with success, no new Pods are created. - once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success. - once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.\n\nhttps://kubernetes.io/docs/concepts/workloads/controllers/job/\nArgo is basically a wrapper on Jobs."
  },
  {
    "objectID": "notes/k8s/13-JobsCron.html#cronjob",
    "href": "notes/k8s/13-JobsCron.html#cronjob",
    "title": "Jobs & CronJobs",
    "section": "CronJob",
    "text": "CronJob\nJust adds a few lines to the Job YAML:\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n name: todo-db-backup\nspec:\n schedule: \"*/2 * * * *\"          # Creates a Job every 2 minutes\n concurrencyPolicy: Forbid        # Prevents overlap so a new Job won‚Äôt be\n jobTemplate:                     # created if the previous one is running\n   spec:\n     # job template...\nhttps://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#cronjobspec-v1-batch\n\nCronJob Cleanup\n\nCronJobs don‚Äôt perform an automatic cleanup for Pods and Jobs.\n\nCronJobs don‚Äôt follow the standard controller model, with a label selector to identify the Jobs it owns. You can add your own labels in the Job template for the CronJob, but if you don‚Äôt do that, you need to identify Jobs where the owner reference is the CronJob\n\n\nTLDR; Clean up lingering pods afer you are done, and organize everything with labels!\n\n\nPausing CronJobs\n\nYou can also move CronJobs to a suspended state, which means the object spec still exists in the cluster, but it doesn‚Äôt run until the CronJob is activated again"
  },
  {
    "objectID": "notes/k8s/99-Random.html",
    "href": "notes/k8s/99-Random.html",
    "title": "Random TILs",
    "section": "",
    "text": "--show labels will show you all the labels!\nA podsec can be configured to connect to the Kubernetes API server its running on (so for example, you can use Kubectl commands). I didn‚Äôt try to do this yet."
  },
  {
    "objectID": "notes/k8s/99-Random.html#abbreviations",
    "href": "notes/k8s/99-Random.html#abbreviations",
    "title": "Random TILs",
    "section": "Abbreviations",
    "text": "Abbreviations\nYou can get a list of the short abbrevations for resources like this:\n$ kl api-resources --sort-by name\n\nNAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND\napiservices                                    apiregistration.k8s.io/v1              false        APIService\nbindings                                       v1                                     true         Binding\ncertificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest\nclusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding\nclusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole\ncomponentstatuses                 cs           v1                                     false        ComponentStatus\nconfigmaps                        cm           v1                                     true         ConfigMap\ncontrollerrevisions                            apps/v1                                true         ControllerRevision\ncronjobs                          cj           batch/v1                               true         CronJob\ncsidrivers                                     storage.k8s.io/v1                      false        CSIDriver\ncsinodes                                       storage.k8s.io/v1                      false        CSINode\ncsistoragecapacities                           storage.k8s.io/v1                      true         CSIStorageCapacity\ncustomresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition\ndaemonsets                        ds           apps/v1                                true         DaemonSet\ndeployments                       deploy       apps/v1                                true         Deployment\nendpoints                         ep           v1                                     true         Endpoints\nendpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice\nevents                            ev           v1                                     true         Event\nevents                            ev           events.k8s.io/v1                       true         Event\nflowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema\nhorizontalpodautoscalers          hpa          autoscaling/v2                         true         HorizontalPodAutoscaler\ningressclasses                                 networking.k8s.io/v1                   false        IngressClass\ningresses                         ing          networking.k8s.io/v1                   true         Ingress\njobs                                           batch/v1                               true         Job\nleases                                         coordination.k8s.io/v1                 true         Lease\nlimitranges                       limits       v1                                     true         LimitRange\nlocalsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview\nmutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration\nnamespaces                        ns           v1                                     false        Namespace\nnetworkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy\nnodes                             no           v1                                     false        Node\npersistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim\npersistentvolumes                 pv           v1                                     false        PersistentVolume\npoddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget\npods                              po           v1                                     true         Pod\npodtemplates                                   v1                                     true         PodTemplate\npriorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass\nprioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration\nreplicasets                       rs           apps/v1                                true         ReplicaSet\nreplicationcontrollers            rc           v1                                     true         ReplicationController\nresourcequotas                    quota        v1                                     true         ResourceQuota\nrolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding\nroles                                          rbac.authorization.k8s.io/v1           true         Role\nruntimeclasses                                 node.k8s.io/v1                         false        RuntimeClass\nsecrets                                        v1                                     true         Secret\nselfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview\nselfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview\nserviceaccounts                   sa           v1                                     true         ServiceAccount\nservices                          svc          v1                                     true         Service\nstatefulsets                      sts          apps/v1                                true         StatefulSet\nstorageclasses                    sc           storage.k8s.io/v1                      false        StorageClass\nsubjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview\ntokenreviews                                   authentication.k8s.io/v1               false        TokenReview\nvalidatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration\nvolumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment"
  },
  {
    "objectID": "notes/k8s/99-Random.html#dbs-on-k8s",
    "href": "notes/k8s/99-Random.html#dbs-on-k8s",
    "title": "Random TILs",
    "section": "DBs on K8s",
    "text": "DBs on K8s\nDon‚Äôt do it. Use a managed DB from your cloud provider instead."
  },
  {
    "objectID": "notes/k8s/99-Random.html#further-reading",
    "href": "notes/k8s/99-Random.html#further-reading",
    "title": "Random TILs",
    "section": "Further Reading",
    "text": "Further Reading\nJeremy Lewi recommends KubeBuilder. I used it to understand Kinds, Resources, Groups and Versions."
  },
  {
    "objectID": "notes/k8s/99-Random.html#creating-a-custom-controller-with-python",
    "href": "notes/k8s/99-Random.html#creating-a-custom-controller-with-python",
    "title": "Random TILs",
    "section": "Creating A custom controller with python",
    "text": "Creating A custom controller with python\n\nExample of writing your own operator/controller with python using kopf: repo\nThe official python client for K8s: repo\nA K8s slackbot written in python.\nImplementing a custom controller in python"
  },
  {
    "objectID": "notes/k8s/22-Resource-Limits.html",
    "href": "notes/k8s/22-Resource-Limits.html",
    "title": "Resource Limits",
    "section": "",
    "text": "By default, Pods have no resource limits, which means they can use as much CPU and memory as the node has available. To prevent one Pod from monopolizing all available resources, you should set resource limits for Pods. You should set both a memory limit and a CPU limit. If a Pod uses more than its resource limit, it is restarted (new container). If it continues to fail, it goes into a CrashLoopBackoff just like with liveness probes.\nYou can specify resource limits in the spec.containers[].resources.limits section of a Pod configuration. The following example sets a memory limit of 50 MiB for a container:\n\n\nvmemory-allocator-with-limit.yaml\n\nspec:                       # The Pod spec in the Deployment\n containers:\n   - image: kiamol/ch12-memory-allocator\n     resources:\n       limits:             # Resource limits constrain compute power\n         memory: 50Mi      # for the container; this limits RAM to 50 MB.\n\nWhen you create this Pod, the container is automatically assigned a memory limit of 256 MiB. It will restart when it hits that limit"
  },
  {
    "objectID": "notes/k8s/22-Resource-Limits.html#namespace-quotas",
    "href": "notes/k8s/22-Resource-Limits.html#namespace-quotas",
    "title": "Resource Limits",
    "section": "Namespace Quotas",
    "text": "Namespace Quotas\nYou can also set resource limits at the namespace level. This is useful if you want to prevent a single user from monopolizing all the resources on a cluster. You can set a quota for the total amount of CPU and memory that can be used by all Pods in a namespace.\n\n\nnamespace-with-quota/02-memory-quota.yaml\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: memory-quota\n  namespace: kiamol-ch12-memory\nspec:\n  hard:\n    limits.memory: 150Mi\n\n\n\n\n\n\n\nImportant\n\n\n\nBecause resource quotas are proactive, Pods won‚Äôt be created if the limits they specify exceed what‚Äôs available in the quota. If there‚Äôs a quota in place, then every Pod spec needs to include a resource section so Kubernetes can compare what the spec needs to what‚Äôs currently available in the namespace.\n\n\nNow, if a pod specifies a limit that is greater than the namespace quota, it will not be created. For example:\n\n\n03-memory-allocator.yaml\n\napiVersion: apps/v1\nkind: Deployment\n...\n    spec:\n      containers:\n        - name: api\n          image: kiamol/ch12-memory-allocator\n          resources:\n            limits:\n              memory: 200Mi"
  },
  {
    "objectID": "notes/k8s/22-Resource-Limits.html#cpu-limits",
    "href": "notes/k8s/22-Resource-Limits.html#cpu-limits",
    "title": "Resource Limits",
    "section": "CPU Limits",
    "text": "CPU Limits\nCPU limits to containers and quotas, but they work in a slightly different way. Containers with a CPU limit run with a fixed amount of processing power, and they can use as much of that CPU as they like‚Äîthey aren‚Äôt replaced if they hit the limit\nYou can use multiples to give your app container access to many cores or divide a single core into ‚Äúmillicores,‚Äù where one millicore is one-thousandth of a core\n\n\nweb-with-cpu-limit.yaml\n\nspec:\n containers:\n   - image: kiamol/ch05-pi\n     command: [\"dotnet\", \"Pi.Web.dll\", \"-m\", \"web\"]\n     resources:\n       limits:\n            cpu: 250m    # 250 millicores limits the container to 0.25 cores.\n\nInclude both CPU and memory limits in your Pod specs."
  },
  {
    "objectID": "notes/k8s/22-Resource-Limits.html#resource-constraint-failures",
    "href": "notes/k8s/22-Resource-Limits.html#resource-constraint-failures",
    "title": "Resource Limits",
    "section": "Resource Constraint Failures",
    "text": "Resource Constraint Failures\nWhat if you have a namespace quota of 500m, but you have two replicas with a limit of 300m each? It will only deploy one pod. YOu can see it like this:\n# after deploying your app with two repolicas\n% kubectl get deploy -n kiamol-ch12-cpu                                                   \n\nNAME     READY   UP-TO-DATE   AVAILABLE   AGE\npi-web   1/2     1            1           44s\n\n# Debug what happened (It will tell you it ran out of quota)\n% kubectl describe replicaset -n kiamol-ch12-cpu\n\n# Get resource quota\n% kl get resourcequota -n kiamol-ch12-memory"
  },
  {
    "objectID": "notes/k8s/22-Resource-Limits.html#checking-available-resources",
    "href": "notes/k8s/22-Resource-Limits.html#checking-available-resources",
    "title": "Resource Limits",
    "section": "Checking Available Resources",
    "text": "Checking Available Resources\nYou can check cpu available on a node like so:\nkubectl get nodes -o jsonpath='{.items[].status.allocatable.cpu}'\n\n\n\n\n\n\nImportant\n\n\n\nYou can‚Äôt use 100% of the node‚Äôs CPU because Kubernetes system components allocate CPU themselves.\n\n\nAlso, when you scale things up, it is helpful to check the ReplicaSet to see if it is scaling up. If it is not, it is likely because it is hitting a resource limit.\nkubectl get rs -l app=pi-web --all-namespaces"
  },
  {
    "objectID": "notes/k8s/22-Resource-Limits.html#dont-use-cpu-limits",
    "href": "notes/k8s/22-Resource-Limits.html#dont-use-cpu-limits",
    "title": "Resource Limits",
    "section": "Don‚Äôt Use CPU Limits?",
    "text": "Don‚Äôt Use CPU Limits?\nThis blog post is controversial, Michal thinks it‚Äôs bad advice. I tend to agree with Michal."
  },
  {
    "objectID": "notes/k8s/22-Resource-Limits.html#requests-vs.-limits",
    "href": "notes/k8s/22-Resource-Limits.html#requests-vs.-limits",
    "title": "Resource Limits",
    "section": "Requests vs.¬†Limits",
    "text": "Requests vs.¬†Limits\n\nNo CPU limit\nIf you do not specify a CPU limit for a Container, then one of these situations applies:\n\nThe Container has no upper bound on the CPU resources it can use. The Container could use all of the CPU resources available on the Node where it is running.\nThe Container is running in a namespace that has a default CPU limit, and the Container is automatically assigned the default limit. Cluster administrators can use a LimitRange to specify a default value for the CPU limit.\n\n\n\nLimits create requests by default\nIf you specify a CPU limit for a Container but do not specify a CPU request, Kubernetes automatically assigns a CPU request that matches the limit. Similarly, if a Container specifies its own memory limit but does not specify a memory request, Kubernetes automatically assigns a memory request that matches the limit."
  },
  {
    "objectID": "notes/k8s/security/26-network-security.html",
    "href": "notes/k8s/security/26-network-security.html",
    "title": "Network Security",
    "section": "",
    "text": "Tip\n\n\n\n#Ignore\nYou can probably ignore this section"
  },
  {
    "objectID": "notes/k8s/security/26-network-security.html#network-policies",
    "href": "notes/k8s/security/26-network-security.html#network-policies",
    "title": "Network Security",
    "section": "Network Policies",
    "text": "Network Policies\nK8s have a flat networking model, which means that all pods can communicate with each other. To prevent this, we can use network policies. Network policies are like firewall rules that allow or deny traffic to pods. Network policies are applied to pods using labels. This can be used to block incoming and outgoing traffic.\nOutgoing traffic is referred to as egress and incoming traffic is referred to as ingress, which should not be confused with the ingress resource.\nFor example, the below network policy will only allow traffic to pods labeled app: apod-api from pods labeled app: apod-web:\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n name: apod-api\nspec:\n podSelector:             # This is the Pod where the rule applies.\n   matchLabels:\n     app: apod-api       \n ingress:                 # Rules default to deny, so this rule\n - from:                  # denies all ingress except where the \n   - podSelector:         # source of the traffic is a Pod with \n       matchLabels:       # the apod-web label.\n         app: apod-web\n   ports:                 # This restriction is by port.\n   - port: api            # The port is named in the API Pod spec.\nHowever this doesn‚Äôt do anything! Just like you need an ingress controller, you need something in your cluster‚Äôs networking system to enforce this. This involves installing various plugins, which your DevOps team should be doing. Furthermore, different cloud platforms make this easier or harder."
  },
  {
    "objectID": "notes/k8s/security/30-rbac.html",
    "href": "notes/k8s/security/30-rbac.html",
    "title": "RBAC",
    "section": "",
    "text": "You don‚Äôt want everyone to have admin priviliges on a K8s cluster. For example kl delete ns --all or kl delete deploy --all -A (deletes all deployments in all namespaces)."
  },
  {
    "objectID": "notes/k8s/security/30-rbac.html#terminology",
    "href": "notes/k8s/security/30-rbac.html#terminology",
    "title": "RBAC",
    "section": "Terminology:",
    "text": "Terminology:\n\nsubject: a user, system account or group.\nRole: a defined set of permissions that you will apply to subject(s).\nRoleBinding: config where you assign roles to subject(s).\n\nRBAC grants permissoins to perform actions on resources. You set a permissions in a role, and apply the role to one or more subject.\n\nGroup, Versions, Kinds and Resources\nA resource is described by the triple (group, version, kind) (GVK for short)\nSee this page\nIt is important to understand groups, versions, kinds, and resources for RBAC.\n\nKinds: each (group, version) contains one or more api types called Kinds. These are guaranteed to be compatible across versions\nResource: A use of Kind in the API. There is often a one-to-one mapping between Kinds and Resources. For instance, the pods resource corresponds to the Pod Kind. You can see the correspondence with the command kl api-resources --sort-by name.\n\nHowever the same Kind may be returned by multiple resources. For example, the Pod Kind is returned by the pods and pods/log resources Notice that resources are always lowercase, and by convention are the lowercase form of the Kind.\n\napiGroup: A collection of related functionality. Each group has one or more versions. This allows us to change how an API works over time. The api groups are referenced here.\n\nTo lookup the apiGroup for a resource, you can use the command kl api-resources --sort-by name, and ignoring the version name. For example, the pods resource is part of the core apiGroup which is the empty string in the spec (see below). You can also look at the reference docs, for example PriorityClass is part of the scheduling.k8s.io apiGroup, which is indicated here.\n\n\nExample of how to lookup the apiGroup and resource name for kind: PriorityClass:\n\n```bash\n$  kl api-resources --sort-by name\n\nNAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND\n...\npriorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass\nThis tells us that the PriorityClass Kind is part of the scheduling.k8s.io apiGroup, and the resource name is priorityclasses.\n\n\nRole vs ClusterRole\nSome resources are namespaces, some are cluster wide.\n\nRole and RoleBinding work on namepsaced objects.\nClusterRole and ClusterRoleBinding work on the whole cluster\n\nThere are lots of built in ClusterRoles which you can see with:\nkl get clusterrole"
  },
  {
    "objectID": "notes/k8s/security/30-rbac.html#authentication",
    "href": "notes/k8s/security/30-rbac.html#authentication",
    "title": "RBAC",
    "section": "Authentication",
    "text": "Authentication\nYou don‚Äôt login to K8s cluster with a username. K8s does not authenticate end users ‚Äì it relies on external identity providers. Cloud platforms will provide this user identification and authentication layer for you. For example, GKE can use Google accounts."
  },
  {
    "objectID": "notes/k8s/security/30-rbac.html#defining-roles",
    "href": "notes/k8s/security/30-rbac.html#defining-roles",
    "title": "RBAC",
    "section": "Defining Roles",
    "text": "Defining Roles\nYou can define your own Role and ClusterRole objects. In addition to apiGroups andresources which we discussed above, you will also need to know about verbs.\nYou can see all the api request verbs here, which are: get, list, create, update, patch, watch, delete, and deletecollection.\nThe apiGroups referenced here is helpful to see the correspondence between the resource and the apiGroup. Below is an example Role and ClusterRole (which are not related):\napiVersion: rbac.authorization.k8s.io/v1 # this is the Group/Version used in the Binding\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"] #pods are part of the Core: \n  verbs: [\"get\", \"watch\", \"list\"]\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  # \"namespace\" omitted since ClusterRoles are not namespaced\n  name: secret-reader\nrules:\n- apiGroups: [\"\"] # secrets are part of Core https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#secret-v1-core\n  resources: [\"secrets\"] # The name of the resource for accessing Secret objects is \"secrets\"\n  verbs: [\"get\", \"watch\", \"list\"]\n\nRoleBinding\nBelow is an example of a RoleBinding that grants the pod-reader role to the user jane in the namespace default. As a reminder, a RoleBinding assigns subjects to a Role, and a Role defines permissions. Thats why the below example doesn‚Äôt define any permissions. That is the associated Role‚Äôs job.\napiVersion: rbac.authorization.k8s.io/v1\n# This role binding allows \"jane\" to read pods in the \"default\" namespace.\n# You need to already have a Role named \"pod-reader\" in that namespace.\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n# You can specify more than one \"subject\"\n- kind: User\n  name: jane # \"name\" is case sensitive\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  # \"roleRef\" specifies the binding to a Role / ClusterRole\n  kind: Role #this must be Role or ClusterRole\n  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to\n  apiGroup: rbac.authorization.k8s.io\nA RoleBinding can also reference a ClusterRole to grant the permissions defined in that ClusterRole to resources inside the RoleBinding‚Äôs namespace. This kind of reference lets you define a set of common roles across your cluster, then reuse them within multiple namespaces which we saw above with RoleBinding to a ClusterRole.\n\nRoleBindings w/ existing ClusterRoles\nAn easy path is to use an existing built in ClusterRole and assign it to a subject. For example, we can assign the view role to a specific user:\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n name: reader-view\n namespace: default                    # The scope of the binding\nsubjects:\n- kind: User\n  name: reader@kiamol.net               # The subject is the new user\n  apiGroup: rbac.authorization.k8s.io   # The version of the K8s API used to specify the subject\nroleRef:\n kind: ClusterRole\n name: view                      # Gives them the view role from the built-in ClusterRole\n apiGroup: rbac.authorization.k8s.io   # The version of the K8s API used to specify the roleRef\nBy RoleBinding to an existing ClusterRole, you dont have to bother with creating a Role! Its a nice shortcut when you can get away with it.\nSeveral important notes:\n\nWe have to assign the RoleBinding to a namespace, because RoleBinding is scoped to a namespace, even though we are referencing an existing ClusterRole. Yes, you can create a RoleBinding to a ClusterRole you aren‚Äôt limited to only a Role!\n\nThe apiGroup is the version of the K8s api you are using to specify that object. For practical purposes, just accept the value rbac.authorization.k8s.io as boilerplate. Also if you leave this out, defaults values will probably be just fine.\n\nFrom the docs: APIGroup holds the API group of the referenced subject. Defaults to ‚Äú‚Äù for ServiceAccount subjects. Defaults to ‚Äúrbac.authorization.k8s.io‚Äù for User and Group subjects.\n\n\n\n\n\n\n\n\nYou must have a RoleBinding\n\n\n\nRoles are additive or grant-only (you can‚Äôt deny permissions). This means hat everything starts with no permissions and you must add them. What this also means that if you create a Role (in this case the reader-view) but no RoleBinding then your subject will have no permissions to do anything!\n\n\nAnother example of a RoleBinding to a ClusterRole:\napiVersion: rbac.authorization.k8s.io/v1\n# This role binding allows \"dave\" to read secrets in the \"development\" namespace.\n# You need to already have a ClusterRole named \"secret-reader\".\nkind: RoleBinding\nmetadata:\n  name: read-secrets\n  #\n  # The namespace of the RoleBinding determines where the permissions are granted.\n  # This only grants permissions within the \"development\" namespace.\n  namespace: development\nsubjects:\n- kind: User\n  name: dave # Name is case sensitive\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n\n\n\nClusterRoleBinding\nTo grant permissions across a whole cluster, you can use a ClusterRoleBinding. The following ClusterRoleBinding allows any user in the group ‚Äúmanager‚Äù to read secrets in any namespace.\napiVersion: rbac.authorization.k8s.io/v1\n# This cluster role binding allows anyone in the \"manager\" group to read secrets in any namespace.\nkind: ClusterRoleBinding\nmetadata:\n  name: read-secrets-global\nsubjects:\n- kind: Group\n  name: manager # Name is case sensitive\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n\n\n\n\n\n\nImportant\n\n\n\nAfter you create a binding, you cannot change the Role or ClusterRole that it refers to. If you try to change a binding‚Äôs roleRef, you get a validation error. If you do want to change the roleRef for a binding, you need to remove the binding object and create a replacement.\n\n\n\n\nA More complex example\nBelow is an example of a custom Role with a RoleBinding:\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n name: system-pod-reader\n namespace: kube-system        # Scoped to the system namespace\nrules:\n- apiGroups: [\"\"]               # The API group of the object spec\n resources: [\"pods\"]           # Pods are in the core group, which\n verbs: [\"get\", \"list\"]        # is identified with an empty string.\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n name: kube-explorer-system\n namespace: kube-system         # Needs to match the role\nsubjects:\n- kind: ServiceAccount\n  name: kube-explorer            # The subject can be in a\n  namespace: default             # different namespace.\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: Role\n name: system-pod-reader"
  },
  {
    "objectID": "notes/k8s/security/30-rbac.html#referring-to-resources",
    "href": "notes/k8s/security/30-rbac.html#referring-to-resources",
    "title": "RBAC",
    "section": "Referring To Resources",
    "text": "Referring To Resources\nThere is something called a subresource, for examplethere are pods and also the subresource pods/log, which allows you to get the logs for a pod. You should try to lookup the subresource at the time you are trying to accomplish something and not try to memorize every subresource. this is how you might specify a subresource:\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-and-pod-logs-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"pods/log\"] # this is applying the rules to both the resource AND the subresource\n  verbs: [\"get\", \"list\"]\nYou can also use glob patterns like * to match all resources, verbs, etc:\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: example.com-superuser  # DO NOT USE THIS ROLE, IT IS JUST AN EXAMPLE\nrules:\n- apiGroups: [\"example.com\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]"
  },
  {
    "objectID": "notes/k8s/security/30-rbac.html#referring-to-subjects",
    "href": "notes/k8s/security/30-rbac.html#referring-to-subjects",
    "title": "RBAC",
    "section": "Referring to Subjects",
    "text": "Referring to Subjects\nSubjects can be groups, users, or ServiceAccounts. ServiceAccounts have names prefixed with system:serviceaccount:, and belong to groups that have names prefixed with system:serviceaccounts:.\nHere is how you would refer to a User, Group and ServiceAccount:\n# For a user named alice@example.com:\nsubjects:\n- kind: User\n  name: \"alice@example.com\"\n  apiGroup: rbac.authorization.k8s.io\n\n# For a group named frontend-admins:\n- kind: Group\n  name: \"frontend-admins\"\n  apiGroup: rbac.authorization.k8s.io\n\n# For the default service account in the \"kube-system\" namespace:\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: kube-system\n\n# For all service accounts in the \"qa\" namespace:\nsubjects:\n- kind: Group\n  name: system:serviceaccounts:qa\n  apiGroup: rbac.authorization.k8s.io\n\n# For all service accounts in any namespace:\nsubjects:\n- kind: Group\n  name: system:serviceaccounts\n  apiGroup: rbac.authorization.k8s.io"
  },
  {
    "objectID": "notes/k8s/security/30-rbac.html#service-accounts",
    "href": "notes/k8s/security/30-rbac.html#service-accounts",
    "title": "RBAC",
    "section": "Service Accounts",
    "text": "Service Accounts\nEvery namespace has a default service account created automatically. Service accounts are for securing apps that use the Kubernetes API like Prometheus, which needs to get a list of pods. Any Pods that do not specify a service account are automatically assigned to the default service account, which has no permissions. However, you usually don‚Äôt want to mess with the default service account, because that is the ‚Äúdefault‚Äù for all pods in the namespace! Instead, you should create a dedicated service account per app.\nYou can see service accounts like this:\n#create a namespace, which also creates a new service account\n$ kl create ns foobar\n\n# see the service accounts in the namespace. one of the service accounts will be named \"default\"\n$ kl get serviceaccounts -n foobar                                                                                                              \nNAME      SECRETS   AGE\ndefault   0         10m\n\n\nCreating Your Own Service Accounts\nkubectl apply -f - <<EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: build-robot\nEOF\nRemember that unless otherwise specified, every Pod runs as the default service account in its namespace. This means by default, Pods cannot access the Kubernetes API. You can change this by specifying a service account in the Pod spec that has the correct permissions.\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: build-robot\nautomountServiceAccountToken: false\n...\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  serviceAccountName: build-robot\n  automountServiceAccountToken: false\n...\nSetting automountServiceAccountToken to false will prevent the service account token from being mounted into the Pod. Otherwise, this is mounted at /var/run/secrets/kubernetes.io/serviceaccount/token. This setting in the Pod spec takes precedence over the setting in the ServiceAccount.\n\n\n\n\n\n\nNote\n\n\n\nYou have to update or create a role binding to assign this build-robot service account to a particular role for it to be usable."
  },
  {
    "objectID": "notes/k8s/security/29-cluster-updates.html",
    "href": "notes/k8s/security/29-cluster-updates.html",
    "title": "Updating a K8s Cluster",
    "section": "",
    "text": "Make sure you use readiness checks\nHandle SIGTERM in your applications so they can gracefully shut down. Use a terminationGracePeriodSeconds configuration to give your application time to shut down.\nWhen nodes are deleted/updated, this will cause the pods to be rescheduled. The edge case here is when all replicas for an application or on a single node ‚Äì to avoid tihs you can use Pod Disruption Budgets (PDBs) to ensure that at least one replica is always available. Alternatively, you can use minAvailable to ensure that at least one replica is always available (recommended)."
  },
  {
    "objectID": "notes/k8s/security/28-workloads.html",
    "href": "notes/k8s/security/28-workloads.html",
    "title": "Webhooks",
    "section": "",
    "text": "Tip\n\n\n\nML people should don‚Äôt need to worry about creating their own webhooks. It is just useful to know what they are, since some of the terms/concepts can show up in error messages.\nWebhooks provide a way for you to validate if a K8s cluster should run an object. Some of these are built in ‚Äì like the ResourceQuota, which stops workloads from running if they exceed quotas. Webhooks that block workloads are called ValidatingAdmissionWebhook.\nThere is also the MutatingAdmissionWebhook which can modify objects before they are created. This is useful for things like injecting sidecars into pods or adding labels to objects. It‚Äôs hard to tell if a resource has been mutated as there is no indication when you run kubectl describe. This is one very undesirable feature of webhooks, as they are hard to debug and are not visible. There is something called an Open Policy Agent (OPA) that can be used to mitigate these issues. Example of confusion caused by a Mutating webhook:\nWebhooks can be expressed in arbitrary code like python or javascript. These can run inside or outside the cluster but must be served on HTTPS.\nYou have to document the rules of webhooks outside the cluster, as their rules are not discoverable by kubectl.\nWebhooks are used to enforce rules like:"
  },
  {
    "objectID": "notes/k8s/security/28-workloads.html#webhook-errors",
    "href": "notes/k8s/security/28-workloads.html#webhook-errors",
    "title": "Webhooks",
    "section": "Webhook Errors",
    "text": "Webhook Errors\nThis is what webhook error messages might look like when you encounter them:"
  },
  {
    "objectID": "notes/k8s/security/27-container-security.html",
    "href": "notes/k8s/security/27-container-security.html",
    "title": "Securing Containers",
    "section": "",
    "text": "Containers usually run as root, which is dangerous b/c if they can break out of the container, they can do anything on the host. You can run containers as a different user, but some apps work only if they‚Äôre running as root. There are ways to specify this via a security context like so, but it will require work to make sure things aren‚Äôt broken, which they often will when you move away from root:\nkind: Deployment\n...\nspec:                     # This is the Pod spec in the Deployment.\n securityContext:        # These controls apply to all Pod containers.\n   runAsNonRoot: true    # Runs as a non-root user\n   runAsUser: 65534      # Runs as the ‚Äúunknown‚Äù user\nAnother thing that you usually want to do is make sure the Kubernetes API token is not mounted into the container, which is only necessary for apps that actually need to use the Kubernetes API (which is rare). You can prevent this token from being exposed like this:\n\n\n\n\n\n\nNote\n\n\n\nThe Kubernetes API token is located at /run/secrets/kubernetes.io/serviceaccount/token which you can see if you run\nkubectl exec -it <pod> -- cat /run/secrets/kubernetes.io/serviceaccount/token\n\n\nkind: Deployment\n...\nspec:    \n automountServiceAccountToken: false      # Removes the API token\nThere are many other settings you can specify. For example a readOnlyRootFilesystem setting will prevents people from downloading scripts or libraries. But these will require you to extensively test your apps to make sure this doesn‚Äôt break anything (and it often will)."
  },
  {
    "objectID": "notes/k8s/Open Questions.html",
    "href": "notes/k8s/Open Questions.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "What is kl wait --for=condition=Ready pod -l app=something ? Is it possible to customize the Ready condition?\n\nUnderstand how nginx works\n\nWTF is going on in Chapter 5 w/proxy setup and caching\n\nUnderstand how caddy works\n\nWhen you are consuming a queue with a Job and completions, can you end the job when the queue is exhausted?"
  },
  {
    "objectID": "notes/k8s/14-RolloutsRollbacks.html",
    "href": "notes/k8s/14-RolloutsRollbacks.html",
    "title": "Rollouts",
    "section": "",
    "text": "Rollouts happen when you create a deployment or update a podspec.\nOnly triggered by change to the podspec, not other changes to a Deployment You can see rollout history like this\nYou can get details of a revision with the revision flag:\nIt‚Äôs helpful to include informational labels with version numbers."
  },
  {
    "objectID": "notes/k8s/14-RolloutsRollbacks.html#to-a-specific-version",
    "href": "notes/k8s/14-RolloutsRollbacks.html#to-a-specific-version",
    "title": "Rollouts",
    "section": "To a specific version",
    "text": "To a specific version\nYou can rollback to a specific revision\nkubectl rollout undo deploy/vweb --to-revision=2"
  },
  {
    "objectID": "notes/k8s/25-Ingress.html",
    "href": "notes/k8s/25-Ingress.html",
    "title": "Ingress",
    "section": "",
    "text": "Load balancers are an easy way to expose apps, however Ingress is yet a different way that is more suited to HTTP requests. Here are some key differences:\n\nLoad Balancers can accept any kind of traffic (TCP, UDP, etc).\nCloud Kubernetes platforms like AKS and EKS are highly available multinode clusters. Deploying a Kubernetes LoadBalancer Service creates an actual load balancer in your cloud, which spans all the nodes in your cluster‚Äîthe cloud load balancer sends incoming traffic to one of the nodes and then Kubernetes routes it to a Pod. You‚Äôll get a different IP address for each LoadBalancer Service, and it will be a public address, accessible from the internet. What this means is that a LoadBalancer will spin up additional resources on your cloud provider. This doesn‚Äôt have to be the case with Ingress.\nCloud providers often charge based on ‚Äúload balancing rules‚Äù, which roughly translates into how many load balancing external IP addresses are assigned. By using an ingress to combine several services into one, rather than each being exposed with it‚Äôs own IP, you can likely save money.\n\n\n\n\n\n\n\nDocker Desktop\n\n\n\nDocker Desktop‚Äôs local Kubernetes runs on a single machine and integrates with the network stack so LoadBalancer Services are available at the localhost address."
  },
  {
    "objectID": "notes/k8s/25-Ingress.html#ingress",
    "href": "notes/k8s/25-Ingress.html#ingress",
    "title": "Ingress",
    "section": "Ingress",
    "text": "Ingress\nFor HTTPS, Ingress is the preferred way to expose your apps. You can have multiple services exposed on one host, whereas you cannot do that with a LoadBalancer. Ingress allows you to route requests based on the URL path. Ingress allows you to make changes to your services without changing the address to the end user (ex: if you want different url paths to now go to different services).\n\nIngress Controller\nAn Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer.\nThe Ingress Resource doesn‚Äôt do anything by itself; you need an ingress controller which does the work of routing requests. The ingress controller often contains a reverse proxy like Nginx. When you set this up on cloud providers they may offer something a bitt different. Reverse proxies intercept requests and route them to where they need to go. These reverse proxies can also do other things like caching, load balancing, URL rewrites, and so on. There are lots of flavors of Ingress Resources (like Nginx, Traefik, etc).\n\n\n\n\n\n\nCloud providers give you an Ingress controller\n\n\n\nIf you‚Äôre using a managed cluster with one of the major cloud providers, an ingress controller is already in place. In Google Kubernetes Engine, the ingress controller is GLBC (GCE L7 Load Balancer), in AWS the Ingress functionality is provided by the AWS Load Balancer Controller, while Azure provides AGIC (Application Gateway Ingress Controller).\n\n\n\n\n\n\n\n\nWhat is a reverse proxy?\n\n\n\nReverse proxies intercept incoming web requests and can take different actions, such as rewriting the URL (changing the path), or routing different paths to different places. They can also provide security by blocking or ignoring certain requests and caching results. For these reasons, reverse proxies are a must-have for any production web application.\nReverse proxies are different than proxy servers. Proxy servers forward traffic from a requestor/client ‚Äì ‚Äúoutbound traffic‚Äù. Reverse proxies intercept client requests and forward them to the server ‚Äì ‚Äúinbound traffic‚Äù. For example, proxy servers can hide the identity of the client. Reverse proxies can hide the identity of the server.\n\n\n\n\nIf you do not go with your cloud provider‚Äôs hosted K8s solution with a provided ingress controller, an experienced devops person will setup the ingress for you. Your DevOps team should deploy an Ingress controller for you. Your DevOps team should provide you with a template for the Ingress Resource, which may reference the Ingress controller. The important part is that you are comfortable enough to know what it is doing to modify an Ingress Resource template. You should not have to create an Ingress Resource from scratch.\nIn the below example, our Ingress controller is a load balancer that is also running Nginx. Below is the Service and the Deployment parts of the Ingress Controller to give you an idea that the controller itself is a LoadBalancer. The Deployment is a Deployment of the Nginx reverse proxy. The Service is a LoadBalancer Service that exposes the Nginx reverse proxy. You probably won‚Äôt and should not have to ever deal this.\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx-controller\n  namespace: kiamol-ingress-nginx\nspec:\n  type: LoadBalancer\n  ports:\n    - name: http\n      port: 80\n      targetPort: http\n    - name: https\n      port: 443\n      targetPort: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/component: controller\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-nginx-controller\n  namespace: kiamol-ingress-nginx\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      containers:\n        - name: controller\n          image: k8s.gcr.io/ingress-nginx/controller:v1.1.1@sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=kiamol-ingress-nginx/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --ingress-class=nginx\n            - --configmap=kiamol-ingress-nginx/ingress-nginx-controller\n          securityContext:\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n      serviceAccountName: ingress-nginx\n---\n\n\nIngress Resource\nThis is an example of an Ingress Resource (the kind of file you may edit). Note that the path is one type of common configuration, and controller-specific options are specified via annotations (in the second example).\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: timeserver-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: timeserver-internal\n            port:\n              number: 80\n      - path: /robohash\n        pathType: Prefix\n        backend:\n          service:\n            name: robohash-internal\n            port:\n              number: 80\nAnother example of an ingress:\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hello-kiamol\n  annotations: # this was added b/c of this https://github.com/sixeyed/kiamol/issues/32\n    kubernetes.io/ingress.class: \"nginx\" \n  labels:\n    kiamol: ch15\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: hello-kiamol\n            port:\n              number: 80\n\n\n\n\n\n\nIngress Annotations\n\n\n\nAn ingress template should be provided to you by a DevOps engineer. You should not need to create one from scratch. This should vary from company to company. The annotations are special parameters passed along to the controller since the Ingress Resource spec is very minimal in the options it can take and because Ingress controllers have different capabilities. The annotations are controller-specific.\n\n\nYou can also have several hosts in the same ingress:\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: timeserver-ingress\nspec:\n  rules:\n  - host: timeserver.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: timeserver-internal\n            port:\n              number: 80\n  - host: robohash.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: robohash-internal\n            port:\n              number: 80\nBy using host-based routing, you can host several services all with the same external IP address. The ingress inspects the Host header in the HTTP request, and routes traffic accordingly. This contrast with Services of type LoadBalancer which each get their own IP address assigned and perform no packet inspection or routing.\nIt is worth reading the docs on Path types and what happens when there are multiple matches.\n\n\nDefault Backend\nYou can direct anything that doesn‚Äôt match any of the rules to a generic 404 page by specifying a default backend:\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kiada\nspec:\n  defaultBackend:\n    service:\n      name: fun404\n      port:\n        name: http\n  rules:\n  ...\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fun404\n  labels:\n    app: fun404\nspec:\n  containers:\n  - name: server\n    image: luksa/static-http-server\n    args:\n    - --listen-port=8080\n    - --response-code=404\n    - --text=This isn't the URL you're looking for.\n    ports:\n    - name: http\n      containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: fun404\n  labels:\n    app: fun404\nspec:\n  selector:\n    app: fun404\n  ports:\n  - name: http\n    port: 80\n    targetPort: http"
  },
  {
    "objectID": "notes/k8s/25-Ingress.html#using-an-ingress",
    "href": "notes/k8s/25-Ingress.html#using-an-ingress",
    "title": "Ingress",
    "section": "Using an Ingress",
    "text": "Using an Ingress\nAfter you have deployed an ingress, you can see the address and port that it is listening on:\n$ kubectl get ing\n\nThe address may not be displayed immediately. This is very common when the cluster is running in the cloud. If the address isn‚Äôt displayed after several minutes, it means that no ingress controller has processed the Ingress object. Check if the controller is running. Since a cluster can run multiple ingress controllers, it‚Äôs possible that they‚Äôll all ignore your Ingress object if you don‚Äôt specify which of them should process it. Check the documentation of your chosen ingress controller to find out if you need to add the kubernetes.io/ingress.class annotation or set the spec.ingressClassName"
  },
  {
    "objectID": "notes/k8s/25-Ingress.html#tls-https-encryption",
    "href": "notes/k8s/25-Ingress.html#tls-https-encryption",
    "title": "Ingress",
    "section": "TLS / HTTPS Encryption",
    "text": "TLS / HTTPS Encryption\nTLS: Transport Layer Security, formerly known as SSL or Secure Sockets Layer.\n\nHow Does HTTPS Work?\nHTTPS uses the HTTP protocol with TLS and SSL encryption added on top. The initial handshake uses asymmetric encryption and then further communication happens with symmetric encryption. This is how it works:\n\nA website references an SSL Certificate that contains a public key. The SSL Certificate is provided by a trusted Certificate Authority (CA).\n\nYou can check the authenticity of the certificate by using a separate public key from the CA to decrypt the CA‚Äôs signature (the CA has their own private key to encrypt its digital signature). Your browser has a list of trusted CAs. Your browser often gets these from other trusted sources like your operating system.\n\nChrome Root CA Policy, 2022-09-01 ‚ÄúWhen making HTTPS connections, Chrome refers to a list of root certificates from CAs that have demonstrated why continued trust in them is justified. This list is known as a ‚ÄúRoot Store.‚Äù CA certificates included in the Chrome Root Store are selected on the basis of publicly available and verified information, such as that within the Common CA Database (CCADB), and ongoing reviews by the Chrome Root Program. CCADB is a datastore run by Mozilla and used by various operating systems, browser vendors, and CA owners to share and disclose information regarding the ownership, historical operation, and audit history of CAs and corresponding certificates and key material. Historically, Chrome has integrated with the Root Store provided by the platform on which it is running. In Chrome 105, Chrome began a platform-by-platform transition from relying on the host operating system‚Äôs Root Store to its own on Windows, macOS, ChromeOS, Linux, and Android. This change makes Chrome more secure and promotes consistent user and developer experiences across platforms. Apple policies prevent the Chrome Root Store and corresponding Chrome Certificate Verifier from being used on Chrome for iOS.‚Äù\n\nThese CAs come from trustworthy sources like Cloudflare, Google, etc.\nLet‚Äôs Encrypt, a popular CA checks that the certificate request comes from a person who actually controls the domain. They do this by sending the client a unique token and then Lets Encrypt makes a web or DNS request to retrieve a key derived from that token.\n\nAfter the initial verification and handshake, the client and server use a symmetric encryption key to communicate securely. The client and server both have the same symmetric key. This is the key that is used to encrypt and decrypt the data.\n\nSee this article and also this one\n\n\nTLS Termination and HTTPS\nSome Ingress controllers like Trafefik have integrations with Let‚Äôs Encrypt to automate the entire process of getting a certificate and renewing it. They also handle TLS termination, which is decrypting the traffic and forwarding it on the appropriate place. Ingress is a nice place to handle TLS because it allows you to have a standardized way of handling TLS for all your services.\nThese ingress controllers allow you to specify a secret that contains the TLS certificate and private key, and they will handle the rest. All you need to do is add a TLS section to your Ingress spec and state the name of the Secret to use:\nFirst you need to create a secret (ideally you would never create a certificate like this, but this is just for demo purposes)\n# create a Secret:\n$ openssl req -x509 -newkey rsa:4096 -keyout example.key -out example.crt \\\n  -sha256 -days 7300 -nodes \\\n  -subj '/CN=*.example.com' \\\n  -addext 'subjectAltName = DNS:*.example.com' \n \n$ kubectl create secret tls tls-example-com --cert=example.crt --key=example.key\nsecret/tls-example-com created\nThen reference the secret from the Ingress spec:\non: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kiada\nspec:\n  tls:\n  - secretName: tls-example-com\n    hosts:\n    - \"*.example.com\" # The hosts specified in tls.hosts must match the names used in the certificate in the secret.\n  rules:\n  ..."
  },
  {
    "objectID": "notes/k8s/25-Ingress.html#multiple-ingress-controllers",
    "href": "notes/k8s/25-Ingress.html#multiple-ingress-controllers",
    "title": "Ingress",
    "section": "Multiple Ingress Controllers",
    "text": "Multiple Ingress Controllers\nWe will not touch this, but this is possible by using an IngressClass."
  },
  {
    "objectID": "notes/k8s/25-Ingress.html#ingress-gateway",
    "href": "notes/k8s/25-Ingress.html#ingress-gateway",
    "title": "Ingress",
    "section": "Ingress Gateway",
    "text": "Ingress Gateway\nK8s has a new API called gateway that is supposed to address some of the limitations of the Ingress resource. I don‚Äôt know anything about it."
  },
  {
    "objectID": "notes/k8s/multi_container_pods/09-Ambassador Sidecars.html",
    "href": "notes/k8s/multi_container_pods/09-Ambassador Sidecars.html",
    "title": "Ambassador Sidecars",
    "section": "",
    "text": "[[Ambassador]]\nThese are [[Sidecar]] containers that act as proxys. You can do this for reliability or security. Proxy containers can do load balancing, retries, or encrypt items. [[service mesh]] uses patterns like this.\nFor example you may want to restrict what web requests or URLs your app is allowed to talk to. With an ambassador sidecar, you can block all traffic besides the allowed one. Here is an example:\n      containers:\n        - name: web\n          image: kiamol/ch03-numbers-web \n          env:\n          - name: http_proxy\n            value: http://localhost:1080\n          - name: RngApi__Url\n            value: http://localhost/api\n        - name: proxy                         # this is a basic proxy\n          image: kiamol/ch07-simple-proxy          \n          env:\n          - name: Proxy__Port                 #Routes network requets given \n            value: \"1080\"                     # the below mapping\n          - name: Proxy__Request__UriMap__Source\n            value: http://localhost/api\n          - name: Proxy__Request__UriMap__Target\n            value: http://numbers-api/sixeyed/kiamol/master/ch03/numbers/rng\nIn the above example, anything that is not in the mapping is blocked. Now the web app is restricted to a single address for outgoing requests, which are logged by the proxy.\nthe app container uses localhost addresses for any services it consumes, and it‚Äôs configured to route all network calls through the proxy container. The proxy is a custom app that logs network calls, maps localhost addresses to real addresses, and blocks any addresses that are not listed in the map. All that becomes functionality in the Pod, but it‚Äôs transparent to the application container.\nYou can also use Ambassador‚Äôs for database connections, to query read-only copies when there are no db updates/writes:"
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html",
    "title": "Multi-Container Pods",
    "section": "",
    "text": "Pods can run more than one container. Pods in a container share the same network and same IP address, so they must listen on different ports. Containers in a pod can communicate over local host. Each container has its own file system, but can mount from the Pod and can share info that way."
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#accessing-containers-in-multi-container-pods",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#accessing-containers-in-multi-container-pods",
    "title": "Multi-Container Pods",
    "section": "Accessing containers in multi-container Pods",
    "text": "Accessing containers in multi-container Pods\nYou can use the -c flag, to narrow down the container\n% kl logs deploy/sleep -c file-reader\nSame thing is necessary for kl exec deploy/sleep ..., you would also add -c file-reader onto that."
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#networking-sharing",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#networking-sharing",
    "title": "Multi-Container Pods",
    "section": "Networking Sharing",
    "text": "Networking Sharing\nTo demonstrate network sharing:\n% cat sleep/sleep-with-server.yaml                                                                                                   \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\n  labels:\n    kiamol: ch07\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n        - name: server\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', \"while true; do echo -e 'HTTP/1.1 200 OK\\nContent-Type: text/plain\\nContent-Length: 7\\n\\nkiamol' | nc -l -p 8080; done\"]\n          ports:\n            - containerPort: 8080    # this exposes a port to this container.\n              name: http\nWe can access the server container on local host from the sleep container:\nkl apply -f sleep/sleep-with-server.yaml\nkl exec deploy/sleep -c sleep -- wget -q -O - localhost:8080\n\nCreating A Serivce to Multi Container Pod\nYou just have to make sure that the port is routing to the correct place.\napiVersion: v1\nkind: Service\nmetadata:\n  name: sleep\nspec:\n  ports:\n    - port: 8020\n      targetPort: 8080\n  selector:\n    app: sleep\n  type: LoadBalancer\nNow from my lapto I can do this, which will allow me to access the container listening in the pod on port 8080\nwget -q -O - localhost:8020"
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#when-to-use-multi-cotainer-pods",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#when-to-use-multi-cotainer-pods",
    "title": "Multi-Container Pods",
    "section": "When to use multi cotainer pods",
    "text": "When to use multi cotainer pods\nYou don‚Äôt want to usually shove different components of an application into a Pod together! Doing so will limit you, as you want to be able to scale/upgrade etc these different components independently.\nThere are two patterns:\n\n[[Sidecar]] runs alongside; pod isn‚Äôt considered ready until all the containers are ready. This is what is shown above.\n[Init containers] you can have multiple init containers, they run in sequence, in order they are specified. Each must complete sucessfully before next one starts, and all must complete sucessfully before the Pod containers start (if mulitple they are sidecars)\n\nInit containers are often used to generate data for container Pods (which is written to a shared mounted directory as previously shown). An example is an init container w/ the git command line installed that clones a repo to a shared file system. Another example is to write configuration files in a specific format that your app expects from env variables and config maps.\nThe below YAML shows the initContainer craeating the index.html file so the next imge can serve it.\n% cat sleep/sleep-with-html-server.yaml                                                                                              \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n...\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n...\n    spec:\n      initContainers:\n        - name: init-html\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', \"echo '<!DOCTYPE html><html><body><h1>KIAMOL Ch07</h1></body></html>' > /data/index.html\"]\n          volumeMounts:\n            - name: data\n              mountPath: /data\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n        - name: server\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', 'while true; do echo -e \"HTTP/1.1 200 OK\\nContent-Type: text/html\\nContent-Length: 62\\n\\n$(cat /data-ro/index.html)\" | nc -l -p 8080; done']\n          ports:\n            - containerPort: 8080\n              name: http\n          volumeMounts:\n            - name: data\n              mountPath: /data-ro\n              readOnly: true\nHere is an example that writes a config file callled appsettings.json:\n...\n    spec:\n      initContainers:\n        - name: init-config\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', \"cat /config-in/appsettings.json | jq --arg APP_ENV \\\"$APP_ENVIRONMENT\\\" '.Application.Environment=$APP_ENV' > /config-out/appsettings.json\"]\n          env:\n          - name: APP_ENVIRONMENT\n            value: TEST\n          volumeMounts:\n            - name: config-map\n              mountPath: /config-in\n            - name: config-dir\n              mountPath: /config-out\n...\n      volumes:\n        - name: config-map     # this is a volume that is mounted as input\n          configMap:\n            name: timecheck-config\n        - name: config-dir     # files are written out here\n          emptyDir: {}"
  },
  {
    "objectID": "notes/k8s/multi_container_pods/10-Downsides of MC Pods.html",
    "href": "notes/k8s/multi_container_pods/10-Downsides of MC Pods.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "MC = multi container\nAdding sidecars and init containers adds to the failure modes for your application.\nYou might see ready = 0 if there is a container in a multi-container pod that is failing!\n\nRestart Conditions\n\nIf a Pod with init containers is replaced, then the new Pod runs all the init containers again. You must ensure your init logic can be run repeatedly.\nIf you deploy a change to the init container image(s) for a Pod, that restarts the Pod. Init containers all execute again, and app containers are replaced.\nIf you deploy a Pod spec change to the app container image(s), the app containers are replaced, but the init containers are not executed again.\nIf an application container exits, then the Pod re-creates it. Until the container is replaced, the Pod is not fully running and won‚Äôt receive Service traffic."
  },
  {
    "objectID": "notes/k8s/multi_container_pods/11-Sharing Processes in MC Pods.html",
    "href": "notes/k8s/multi_container_pods/11-Sharing Processes in MC Pods.html",
    "title": "Sharing Processes in MC Pods",
    "section": "",
    "text": "MC = multi container\nContainers isolate proceses, so containers cannot see eachothers processes. You can set Namespace: true to make processes visible amongst all containers in a pod:\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      shareProcessNamespace: true\n      containers:\n      ...\nIf you enable this: - containers can kill eachother‚Äôs processes - enable interproces communication - fetch metrics about the app process"
  },
  {
    "objectID": "notes/k8s/24-monitoring.html",
    "href": "notes/k8s/24-monitoring.html",
    "title": "Monitoring",
    "section": "",
    "text": "Prometheus discovers new apps you deploy on K8s and starts collecting metrics automatically. Each component you want to monitor has an HTTP endpoint, and Prometheus logs whatever the endpoint returns.\nThis is part of a Prometheus config that filters by a specific namespace:\nscrape_configs:                # This is the YAML inside the ConfigMap.\n - job_name: 'test-pods'      # Used for test apps\n   kubernetes_sd_configs:     # Finds targets from the Kubernetes API\n   - role: pod                # Searches for Pods\n   relabel_configs:           # Applies these filtering rules\n   - source_labels:          \n       - __meta_kubernetes_namespace\n     action: keep             # Includes Pods only where the namespace\n     regex: kiamol-ch14-test  # is the test namespace for this chapter\nAs long as your apps are modeled to suit the rules, they‚Äôll automatically be picked up as monitoring targets. Prometheus uses the rules to find Pods that match, and for each target, it collects metrics by making an HTTP GET request to the /metrics path.\nYour application exposes a /metrics endpoint like this:\n...\n      containers:\n        - name: timecheck\n          image: kiamol/ch07-timecheck\n          env:\n            - name: Metrics__Enabled\n              value: \"true\"\n          ports:\n            - containerPort: 8080\n              name: metrics\n‚Ä¶ Skipping this"
  },
  {
    "objectID": "notes/k8s/storage/05-Dynamic Provisioning.html",
    "href": "notes/k8s/storage/05-Dynamic Provisioning.html",
    "title": "Dynamic Provisioning",
    "section": "",
    "text": "[[k8s]]\nIn [[3. Storage - Basics]], you were shown how to setup a PV, and a PVC that would bind to the PV, and finally how to create a deployment that would reference the PVC\nHowever we can have K8s dynamically provision the PV. So you just create the PVC, and K8s creates the PV for you! You can have clusters configured with different storage classes. You can also use the default class:\nSo if we deploy ths, let‚Äôs see what will happen! kl apply -f todo-list/postgres-persistentVolumeClaim-dynamic.yaml\nIf you do kl get pv you will see a PV has been automatically created."
  },
  {
    "objectID": "notes/k8s/storage/05-Dynamic Provisioning.html#storage-classes",
    "href": "notes/k8s/storage/05-Dynamic Provisioning.html#storage-classes",
    "title": "Dynamic Provisioning",
    "section": "Storage Classes",
    "text": "Storage Classes\nkind: StorageClass\nYou can create a storage class and reference it from the PVC. Three fields:\n\nprovisioner: How to create PV on demand\nreclaimPolicy what happens to dynamically created volumes when PVC is deleted\nvolumeBindingMode if pv is created now or when the related Pod is created\n\nExample of [[StorageClass]]\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{},\"name\":\"kiamol\"},\"provisioner\":\"docker.io/hostpath\",\"reclaimPolicy\":\"Delete\",\"volumeBindingMode\":\"Immediate\"}\n  creationTimestamp: \"2022-12-06T00:45:35Z\"\n  name: kiamol\n  resourceVersion: \"819084\"\n  uid: 79a1b70e-6ebe-4aa8-92ce-595220fc6b14\nprovisioner: docker.io/hostpath\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nYou can see all of the storage classes in your cluster with kl get sc\n% cat sktorageClass/postgres-persistentVolumeClaim-storageClass.yaml                                                                   \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc-kiamol\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: kiamol\n  resources:\n    requests:\n      storage: 100Mi\nYou would use the above storage class in a deployment like this:\nkind: Deployment\n...\n    spec:\n      containers:\n        - name: db\n          image: postgres:11.6-alpine\n...\n          volumeMounts:\n            - name: secret\n              mountPath: \"/secrets\"\n            - name: data\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: secret\n          secret:\n            secretName: todo-db-secret\n            defaultMode: 0400\n            items:\n            - key: POSTGRES_PASSWORD\n              path: postgres_password\n        - name: data\n          persistentVolumeClaim:\n            claimName: postgres-pvc-kiamol"
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html",
    "href": "notes/k8s/storage/04-Basics.html",
    "title": "Storage Basics",
    "section": "",
    "text": "[[k8s]]\nThis is Chapter 5 in KIAMOL\nUnlike compute, storage is more complicated because you don‚Äôt want your data to get lost on pod restarts.\nSolution: you want to mount external file systems that will survive a container restart.\nConfigMaps and Secrets are mounted, but those are read aonly."
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html#pod-storage",
    "href": "notes/k8s/storage/04-Basics.html#pod-storage",
    "title": "Storage Basics",
    "section": "Pod Storage",
    "text": "Pod Storage\nThis kind of storage lives outside the container but on the Pod. It will survive container restarts, but not a Pod restart.\n%cat sleep/sleep-with-emptyDir.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          volumeMounts:             # Mounts a volume call data\n            - name: data\n              mountPath: /data      # into the /data directory\n      volumes:\n        - name: data           # this is the data volume spec\n          emptyDir: {}         # this is the EmptyDir type\nIf you want your data to persist across pod restarts, you have to mount a different type of storage."
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html#hostpath",
    "href": "notes/k8s/storage/04-Basics.html#hostpath",
    "title": "Storage Basics",
    "section": "HostPath",
    "text": "HostPath\nWrites files to a disk on a node. So it will survive pod replacements. However, it is only on that Node and K8s doesn‚Äôt replicate files to other nodes for you. Assumes that the replacement pod will always run on the same node :/\n% cat pi/nginx-with-hostPath.yaml                                                                                                     \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pi-proxy\n  labels:\n    app: pi-proxy\n...\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/nginx/\"\n              readOnly: true\n            - name: cache-volume\n              mountPath: /data/nginx/cache\n      volumes:\n        - name: config\n          configMap:\n            name: pi-proxy-configmap\n        - name: cache-volume\n          hostPath:\n            path: /volumes/nginx/cache  #uses a directory non the node\n            type: DirectoryOrCreate #creates a path if it doesn't exist\n[HostPath] is only a good idea when your app needs temporary storage, because it can dissapear with a node. You could use Pod Storage for this, too so its not clear when this is useful."
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html#persistent-volumes-and-claims",
    "href": "notes/k8s/storage/04-Basics.html#persistent-volumes-and-claims",
    "title": "Storage Basics",
    "section": "Persistent Volumes and Claims",
    "text": "Persistent Volumes and Claims\n\nThis section is largely pedagoical, you will want to use Dynamic volume provisioning in most cases.\n\nYou have to configure shared storage on your cloud provider. For example, if you had a NFS server with the domain name nfs.my.network your PV resource would look like this:\n% cat todo-list/persistentVolume-nfs.yaml                                                                                            \napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv01\nspec:\n  capacity:\n    storage: 50Mi\n  accessModes:\n    - ReadWriteOnce\n  nfs:\n    server: nfs.my.network\n    path: \"/kubernetes-volumes\n\nNode Labeling\nIf you can use a local storage for a PV like this:\n1st make sure your node is labeled: kl label node docker-desktop kiamol=ch05\n% cat todo-list/persistentVolume.yaml                                                                                                 \napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv01\nspec:\n  capacity:\n    storage: 50Mi\n  accessModes:\n    - ReadWriteOnce   # Means that we can only mount this to ONLY ONE POD\n  local:\n    path: /volumes/pv01  # this path must be present on the node\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n        - matchExpressions:\n          - key: kiamol\n            operator: In\n            values:\n              - ch05\nPods cannot use this directly, they need to use a [[PersistenVolumeClaim]] or PVC. The PVC gets matched to a PV by K8s which leaves the underling volume details to the Pv.\n%cat todo-list/postgres-persistentVolumeClaim.yaml                                                                                   \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 40Mi                  # 40 MB\n  storageClassName: \"\"               # A blank name means a PV needs to exist\nPV is like creating storage PVC is requesting storage that Pods use\n\n\nManual Provisioning\nWe have been manually provisioning PV + PVCs\nWhen you kl apply the PVC, it will find unbound PVs and then bind them.\nwhen you run kl get pv you will see if the PV is unclaimed yet or not\nif you create a PVC that requests more than any PV, it will show a pending status instead of Bound.\n\nIf you try to deploy a pod that uses an unbound PVC, the Pod will stay in a Pending state until the PVC gets bound\n\n\n\nBinding To the PVC\nThe deployment references the PVC like so:\n% cat todo-list/postgres/todo-db.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-db\nspec:\n  selector:\n    matchLabels:\n      app: todo-db\n  template:\n    metadata:\n      labels:\n        app: todo-db\n    spec:\n      containers:\n        - name: db\n          image: postgres:11.6-alpine\n          env:\n          - name: POSTGRES_PASSWORD_FILE\n            value: /secrets/postgres_password\n          volumeMounts:\n            - name: secret\n              mountPath: \"/secrets\"\n            - name: data\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: secret\n          secret:\n            secretName: todo-db-secret\n            defaultMode: 0400\n            items:\n            - key: POSTGRES_PASSWORD\n              path: postgres_password\n        - name: data\n          persistentVolumeClaim:\n            claimName: postgres-pvc\n\nIn production, you want to replace the local volume PV with a distributed volume supported by your cloud provider or cluster.\nThe PVC doesn‚Äôt care about the implementation so you will just have to swap out the PV"
  },
  {
    "objectID": "notes/k8s/02-Basics.html",
    "href": "notes/k8s/02-Basics.html",
    "title": "Basics",
    "section": "",
    "text": "These are rough notes and not meant for consumption by others! Any course material will be seperate and may or may not be related to these notes!\nDo stuff: kubectl apply -f\nMultiple resources in one yaml with ---"
  },
  {
    "objectID": "notes/k8s/02-Basics.html#deployments",
    "href": "notes/k8s/02-Basics.html#deployments",
    "title": "Basics",
    "section": "Deployments",
    "text": "Deployments\nkind: deployment\n[[Pods]] can have more than 1 container but usually contain just one\n[Deployments] control pods and will restart Pods if they fail. Deployments are a type of [[controller]]. You usually deploy pods via a deployment. kubectl create deployment. Deployments keep track of pods via labels and a label selector. If you change the pod‚Äôs labels the deployment might lose track of the pods.\nA [[controller]] is a K8s resource that manages other resources.\n-o yaml is great for seeing labels, you can swithc to - json and pipe to jq\nExecute a command in a container by doing kubectl exec -it <pod name> -- sh"
  },
  {
    "objectID": "notes/k8s/02-Basics.html#services",
    "href": "notes/k8s/02-Basics.html#services",
    "title": "Basics",
    "section": "Services",
    "text": "Services\nkind: service\n\nYou can‚Äôt switch a Service from one type to another in every version of Kubernetes, so you‚Äôll need to delete the original ClusterIP Service for the API before you can deploy the ExternalName Service.\n\nYou can\n\nDNS\nWhen you deploy a service, you can reference it from within the namespace by its name. For example, the following service:\napiVersion: v1\nkind: Service\nmetadata:\n  name: numbers-api\nspec:\n  ports:\n    - port: 80\n  selector:\n    app: numbers-api\n  type: ClusterIP # this isn't necessary, it's the default\nCan be referenced with just http://numbers-api ‚Äì that is how it‚Äôs referenced in the application.\nIf the pod trying to reach that service is in a different namespace, you have to use the fully qualified name <service-name>.<namespace>, which would be http://numbers-api.default in this case.\n\nTesting DNS\nThe best way to test internal DNS problems is to use nslookup in a pod. You can deploy a pod specifically for network testing from the official k8s docs:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dnsutils\n  namespace: default\nspec:\n  containers:\n  - name: dnsutils\n    image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3\n    command:\n      - sleep\n      - \"infinity\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\nYou can also reference the yaml like this kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml\nLet‚Äôs say I‚Äôve deployed a service called numbers-api in the default namespace:\n$ kl get svc\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nnumbers-api   ClusterIP   10.98.154.232    <none>        80/TCP           13m\nYou can now test it with nslookup:\n# use nslookup <service-name> if the pod dnsutils ins int he same namespace as the service\nkubectl exec -i -t dnsutils -- nslookup numbers-api\n\n# or if your pod is in a different namespace than the service use <service-name>.<namespace>\nkubectl exec -i -t dnsutils -- nslookup numbers-api.default\nIf you are experiencing further issues, follow these debugging instructions\n\n\nLabels\nIf you have overlapping labels for a particular deployment, the service will route to all deployments that match that label. If you want to control for this, add additional unique labels. Just having one label like ‚Äúmyapp‚Äù can be dangerous for this reason.\nServices deal with networking. These use labels, too via a selector.\n\n\n\nRouting internal traffic ClusterIP\nClusterIP: default service that is internal DNS. type: ClusterIP\nForward port 8080 on your local computer to port 80 in container: kubectl port-forward deploy/numbers-web 8080:80\n\n\nRouting external traffic: LoadBalancer\ntype: LoadBalancer Uses labels too\n\n\nRouting Traffic Outside K8s ExternalNameService\ntype: ExternalName\nYou have to watch out when making HTTP requests through ENS, b/c the header wil still contain the original hostname, which will probably get rejected. It‚Äôs fine for things like TCP etc for databases.\n\n\nNamespaces\nThis is relevant to networking b/c resources outside the default namespace will have a different network address\nkubectl get svc -n default kubectl get svc -n kube-system\nFor example, the internal kube-dns service:\n kl get svc -n kube-system                                                                                                                                                                            (master)kiamol\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\nkube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   9d\nCan be accessed like this kl exec deploy/sleep-1 -- sh - c'nslookup kube-dns.kube-system.svc.cluster.local"
  },
  {
    "objectID": "notes/k8s/02-Basics.html#configuring-applications",
    "href": "notes/k8s/02-Basics.html#configuring-applications",
    "title": "Basics",
    "section": "Configuring Applications",
    "text": "Configuring Applications\nYou can environment variables to Pod specs\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          env:\n          - name: KIAMOL_CHAPTER\n            value: \"04\"\nYou usually don‚Äôt set configs in pod specs. You ususally use [[ConfigMap]]\nHow to reference a configmap instead/in addition to of an env variable:\n% cat sleep/sleep-with-configMap-env-file.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          envFrom: # This section will bring in all env variables from the config map `sleep-config-env-file` which we create below.  This can be thought of as the \"baseline\" config.\n          - configMapRef:\n              name: sleep-config-env-file\n          env: # This section can override any environment variables from the config, including any other configs that are elswhere.  So this will override other things\n          - name: KIAMOL_CHAPTER\n            value: \"04\"\n          - name: KIAMOL_SECTION\n            valueFrom:\n              configMapKeyRef: # this came from another configMap\n                name: sleep-config-literal\n                key: kiamol.section\n\nCreating a [[ConfigMap]]\n\nMethod 1: from env file\nThis way is not recommended b/c you have to use kl create rather than kl apply , and you want to use kl apply for everything\nStart with an env file, like this:\n% cat sleep/ch04.env                                                                                                                                                                                   \nKIAMOL_CHAPTER=ch04\nKIAMOL_SECTION=ch04-4.1\nKIAMOL_EXERCISE=try it now\nCreate a config file from an env file\n% kl create configmap sleep-config-env-file --from-env-file=sleep/ch04.env                                                                                                                             \nconfigmap/sleep-config-env-file created\nUpdate your deployment by making changes to add the reference to the config file (see previous section)\nkl apply -f sleep/sleep-with-configMap-env-file.yaml\n\n\nMethod 2: from ConfigMap spec\nThis is more flexible and powerful, you can embed arbitrary files like json files that can be read by your app.\nCreate a spec:\n% cat todo-list/configMaps/todo-web-config-dev.yaml                                                                                                                                                    \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: todo-web-config-dev\ndata: # we are going to mount this json file into the container so the app can use it\n  config.json: |-\n    {\n      \"ConfigController\": {\n        \"Enabled\" : true\n      }\n    }\nApply this spec: kl apply -f todo-list/configMaps/todo-web-config-dev.yaml\nP.S. You could have also seen the yaml file for the other configmap we created earlier with kl get cm/sleep-config-env-file -o yaml and used that yaml file\nUse the config map in the deployment spec, and additionally mount a volume containing the config:\n% cat todo-list/todo-web-dev.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-web\nspec:\n  selector:\n    matchLabels:\n      app: todo-web\n  template:\n    metadata:\n      labels:\n        app: todo-web\n    spec:\n      containers:\n        - name: web\n          image: kiamol/ch04-todo-list\n          volumeMounts: # This will load the config json file into `/app/config` in your container\n            - name: config\n              mountPath: \"/app/config\" #directory path to mount the volume **BE CAREFUL** if you mounted this to `/app`, then it would have wiped out all the files!\n              readOnly: true\n      volumes: # volumes are defined at pod level\n        - name: config  # Name matches the volume mount\n          configMap: # volume source is the Config Map\n            name: todo-web-config-dev  #ConfigMap name\nBe careful when specifying the mount path, lots of people make mistakes here and overwrite existing data. K8s will not merge directories for you!\nIf you change the config map, it will refresh the files in the directory. You have to make sure your app is watching that directory though.\nInstead of loading the whole config map, you can selectively mount files in the config map like this:\n% cat todo-list/todo-web-dev-no-logging.yaml                                                                                                                                                                     \napiVersion: apps/v1\n...\n      volumes:\n        - name: config\n          configMap:\n            name: todo-web-config-dev\n            items:\n            - key: config.json\n              path: config.json"
  },
  {
    "objectID": "notes/k8s/12-StatefulSet.html",
    "href": "notes/k8s/12-StatefulSet.html",
    "title": "StatefulSet",
    "section": "",
    "text": "Hamel: you probably don‚Äôt need this. JUST SKIP THESE NOTES\n\n[[StatefulSet]] is a Pod controller, just like [[5. ReplicaSets]] or [[DaemonSets]]\nWhen you deploy a StatefulSet, it creates Pods with predictable names, which can be individually accessed over DNS, and starts them in order; the first Pod needs to be up and running before the second Pod is created.\nIf you are trying to model database on K8s, you might use StatefulSet. However, don‚Äôt put DBs on K8s - use a managed service for that instead. StatefulSet just gives you determinstic Pod names and networking, you have to take care of synching your apps yourself. That would be outside the scope of what DS should do IMO.\nHere is kind: StatefulSet\n % cat todo-list/db/todo-db.yaml                                                                                                      \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: todo-db\n  labels:\n    kiamol: ch08\nspec:\n  selector:\n    matchLabels:\n      app: todo-db\n  serviceName: todo-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: todo-db\nWhen you get pods, they will be incremented from 0, this allows you network/communicate with them deterministically.\n% kl get po                                                                                                                         \nNAME        READY   STATUS    RESTARTS   AGE\ntodo-db-0   1/1     Running   0          23s\ntodo-db-1   1/1     Running   0          21s\nStatefulSet is a controller, so if you delete a pod the StatefulSet will recreate it.\n\nInitContainers\nYou can bootstrap pods with initcontainers and stateful sets. For example.\napiVersion: apps/v1\nkind: StatefulSet\n...\n      initContainers:\n        - name: wait-service\n          image: kiamol/ch03-sleep\n          envFrom:\n          - configMapRef:\n              name: todo-db-env\n          command: ['/scripts/wait-service.sh']\nThe script says if its running in Pod 0 do nothing, but if its running in Pod 1 then replicate the master. This is just an idea, you probably should never do this yourself.\n\n\nNetworking In StatefulSets\nYou need to have a special configuration ‚Äúheadless Service‚Äù to setup newtworking for StatefulSets, by setting ClusterIP: None\n%cat todo-list/db/todo-db-service.yaml                                                                                              \napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-db\n  labels:\n    kiamol: ch08\nspec:\n  ports:                  # 5432 is the port Postgres uses\n    - port: 5432\n      targetPort: 5432 \n      name: postgres\n  selector:\n    app: todo-db\n  clusterIP: None             # Note how this is None\nThe pod will be reachable at pod-name.service-name.cluster-domain-suffix. for example:\ntodo-db-0.todo-db.default.svc.cluster.local\nSee: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id\n\nA StatefulSet can use a Headless Service to control the domain of its Pods. The domain managed by this Service takes the form: \\((service name).\\)(namespace).svc.cluster.local, where ‚Äúcluster.local‚Äù is the cluster domain. As each Pod is created, it gets a matching DNS subdomain, taking the form: \\((podname).\\)(governing service domain), where the governing service is defined by the serviceName field on the StatefulSet.\n\nThis is also related to the serviceName field on the StatefulSet\nYou can lookup the cluster-domain-suffix like this :\nkl exec deploy/sleep -- sh -c 'nslookup todo-db'`\nThe headless service still does load balancing, but lets you access the Pod\n\n\nStorage\nFor DBs you want each pod to have its own persistent disk, there is a shortcut: using volumeClaimTemplates which makes sure each Pod in the stateful set always gets its own persistent volume.\n% cat sleep/sleep-with-pvc.yaml                                             \n...\nkind: StatefulSet\n  template:\n    ...\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        kiamol: ch08\n    spec:\n      accessModes:\n       - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Mi\nEach pod will get a PVC created dynamically, which will create a Persistent Volume using the default storage class (or the requested storage class if included in the spec).\nThe link b/w each pod and its PVC is maintained when pods are replaced. For example:\n# create a file\nkl exec sleep-with-pvc-0 -- sh -c 'echo pod 0 > /data/pod.txt'\n\n#delete the pod\nkl delete po sleep-with-pvc-0\n\n# this will show the right contents\nkl exec sleep-with-pvc-0 -- cat /data/pod.txt"
  },
  {
    "objectID": "notes/k8s/scaling/07- Scaling.html",
    "href": "notes/k8s/scaling/07- Scaling.html",
    "title": "Scaling",
    "section": "",
    "text": "replicas can be used for scaling. You must also think about storage."
  },
  {
    "objectID": "notes/k8s/scaling/07- Scaling.html#daemonsets",
    "href": "notes/k8s/scaling/07- Scaling.html#daemonsets",
    "title": "Scaling",
    "section": "DaemonSets",
    "text": "DaemonSets\n[DaemonSets] allow you to run a service on each node. You can do this for node specific things like collecting logs on each node. DaemonSets are yet another kind of controller for Pods beyond [[Deployments]]\nIf you switch from a Deployment to a DaemonSet you should delete the Deployment first. You can‚Äôt automatically change from one kind of controller to another.\nA DaemonSet runs a control loop that will watch for any new nodes and start a pod on that node.\n\nUse cases for DaemonSets:\n\nWant to run a pod on every node\nyou have only a subset of nodes that can receive traffic from the internet -> use labels to achieve this.\n\n\n\nLabeling A Node For DaemonSets\nThis allows you to select which nodes the Daemonset runs on:\n% cat pi/proxy/daemonset/nginx-ds-nodeSelector.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: pi-proxy\n  labels:\n    kiamol: ch06\nspec:\n  selector:\n    matchLabels:\n      app: pi-proxy\n  template:\n    metadata:\n      labels:\n        app: pi-proxy\n    spec:\n...\n      nodeSelector:\n        kiamol: ch06\nTo use thie above yaml, you have to label your node like this:\nkl label node $(kl get nodes...) kiamol=ch06 --overwrite\n\n\nCascade Delete\nTLDR; you probably don‚Äôt need this\nYou can set cascade=False to delete a controller without deleting its managed objects. This is how you can change a controller but still keep pods alive.\nkl delete ds pi-proxy --cascade=orphan  # deletes the daemonset pi-proxy\nControllers use a label selector to find objects they manage, so you just have to make sure the new controller you define has the right label. Hamel: it‚Äôs not clear how to switch from a Daemonset to a deployment."
  },
  {
    "objectID": "notes/k8s/scaling/06-ReplicaSets.html",
    "href": "notes/k8s/scaling/06-ReplicaSets.html",
    "title": "ReplicaSets",
    "section": "",
    "text": "Hierachy is Deployments ->  ReplicaSets -> Pods -> Containers\nYou probably never need to fiddle with ReplicSets directly and will mostly operate at the Deployment abstraction level that‚Äôs mentioned in [[2a. Basics]].\nThe reason the ReplicaSet abstraction is used is that the Deployment turns the replicas count to 0 when you update the metadata of podspec in a Deployment, as a way of gracefully winding down old pods in favor of new pods.\nThis is why sometimes you might see a ReplicaSet with a desired pod count of zero.\nThe way replicas are controled via deployments are through the spec.replicas field:\n%cat  pi/proxy/nginx.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pi-proxy\n  labels:\n    kiamol: ch06\nspec:\n  replicas: 2  # Two replicas for nginx\n  selector:\n    matchLabels:\n      app: pi-proxy\n  template:\n    metadata:\n      labels:\n        app: pi-proxy\n    spec:\n      containers:\n        - image: nginx:1.17-alpine\n          name: nginx\n          ports:\n            - containerPort: 80\n              name: http\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/nginx/\"\n              readOnly: true\n            - name: cache-volume\n              mountPath: /data/nginx/cache\n      volumes:\n        - name: config\n          configMap:\n            name: pi-proxy-configmap\n        - name: cache-volume\n          emptyDir: {}"
  },
  {
    "objectID": "notes/k8s/28-auto-scaling.html",
    "href": "notes/k8s/28-auto-scaling.html",
    "title": "Auto Scaling",
    "section": "",
    "text": "K8s can autoscale by automatically adding or removing pods. There is also cluster scaling which adds and removes nodes, but your cloud platform can do that.\nK8s needs to have a way to measure the load on the pods, this can vary on cloud platforms so check the appropriate documentation. Generally, K8s comes with a metric-server component that can measure basic things like CPU and memory usage, which you can see with the kubectl top command. The metrics.k8s.io API is usually provided by an add-on named Metrics Server, which needs to be launched separately. For more information about resource metrics, see Metrics Server.\nDo the HorizontalPodAutoscaler Walkthrough.\nThis is a basic example of an autoscaler:\nAnother example:\nCPU utilization is a resource metric. Another resource metric is memory.\nYou can see the autoscaler with\nSee these docs."
  },
  {
    "objectID": "notes/k8s/28-auto-scaling.html#cluster-autoscaling",
    "href": "notes/k8s/28-auto-scaling.html#cluster-autoscaling",
    "title": "Auto Scaling",
    "section": "Cluster Autoscaling",
    "text": "Cluster Autoscaling\nCluster autoscaling monitors the scheduler. If there are not enough compute resources to run pending Pods, it adds a new node in the cluster. Cloud providers typically provide cluster autoscaling."
  },
  {
    "objectID": "notes/k8s/22a-Resource-Requests.html",
    "href": "notes/k8s/22a-Resource-Requests.html",
    "title": "Requesting resources",
    "section": "",
    "text": "Example of container resource requests:\nAlso see Resource limits, and these docs"
  },
  {
    "objectID": "notes/k8s/22a-Resource-Requests.html#gpus",
    "href": "notes/k8s/22a-Resource-Requests.html#gpus",
    "title": "Requesting resources",
    "section": "GPUs",
    "text": "GPUs\nGPUs are only supposed to be specified in the limits section, which means:\n\nYou can specify GPU limits without specifying requests, because Kubernetes will use the limit as the request value by default.\nYou can specify GPU in both limits and requests but these two values must be equal.\nYou cannot specify GPU requests without specifying limits.\n\nHere‚Äôs an example manifest for a Pod that requests a GPU:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-vector-add\nspec:\n  restartPolicy: OnFailure\n  containers:\n    - name: example-vector-add\n      image: \"registry.example/example-vector-add:v42\"\n      resources:\n        limits:\n          gpu-vendor.example/example-gpu: 1 # requesting 1 GPU"
  },
  {
    "objectID": "notes/k8s/26-cluster.html",
    "href": "notes/k8s/26-cluster.html",
    "title": "Cluster Components",
    "section": "",
    "text": "You don‚Äôt have to implement these, but these come up when you are reading docs so it can be useful to have some background:\n\ncontrol plane: The thing that receives your kubectl requests and then takes action.\nAPI server: A REST API on HTTPS which kubectl connects to, and also what Pods can use internally (with service accounts).\n\nscheduler: Watches for new pod requests and selects a node to run it on.\ncontroller manager: runs internal components, for example the kube-controller-manager observes node availablity.\netcd is the Kubernetes data store. It is a distributed key-value database, which is replicated across many instances.\nkubelet: background agent that runs on the server (not in a pod/container), receives requests to create Pods and sends heartbeats to the API server.\nkube-proxy: routes traffic b/w pods or Pods to the outside world. This runs as a DaemonSet on every node.\ncontainer runtime: This is typically Docker, but it can be other things like CRI-O or containerd. It is the thing that runs the containers."
  },
  {
    "objectID": "notes/k8s/18-Developer.html",
    "href": "notes/k8s/18-Developer.html",
    "title": "Developer tips",
    "section": "",
    "text": "These notes provide tips on the developer workflow while using K8s. Some people use Docker compose to work with things locally, however you can also run a Kubernetes cluster locally."
  },
  {
    "objectID": "notes/k8s/18-Developer.html#use-the-ifnotpresent-imagepullpolicy",
    "href": "notes/k8s/18-Developer.html#use-the-ifnotpresent-imagepullpolicy",
    "title": "Developer tips",
    "section": "Use the IfNotPresent imagePullPolicy",
    "text": "Use the IfNotPresent imagePullPolicy\nK8s have tricky rules for which container images are used (local vs from repo).\nIf the image doesn‚Äôt have an explict tag in the name (and therefore uses the implicit :latest tag), then K8s will always pull the image first. Otherwise, K8s will use the local image if it exists in the image cache on the node.\nYou can override this behavior by specifying an image pull policy. When developing locally, you want to use the IfNotPresent policy.\nspec:                         # This is the Pod spec within the Deployment.\n containers:\n   - name: bulletin-board\n     image: kiamol/ch11-bulletin-board:dev \n     imagePullPolicy: IfNotPresent   # Prefer the local image if it exists\nIf you forget to do this, it can be very confusing as to why your image doesn‚Äôt seem to be updated!"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#use-namespaces",
    "href": "notes/k8s/18-Developer.html#use-namespaces",
    "title": "Developer tips",
    "section": "Use namespaces",
    "text": "Use namespaces\nYou can use namespaces to test apps on the cluster. For example, a production and a test namespace.\nDeploy with a namespace using the --namespace flag:\n# create a new namespace:\nkubectl create namespace kiamol-ch11-test\n\n# deploy a sleep Pod in the new namespace:\nkubectl apply -f sleep.yaml --namespace kiamol-ch11-test\n\n# list sleep Pods--this won‚Äôt return anything:\nkubectl get pods -l app=sleep\n\n# now list the Pods in the namespace:\nkubectl get pods -l app=sleep -n kiamol-ch11-test\nObjects within a namespace are isolated, so you can deploy the same apps with the same object names in different namespaces.\n\nSetting the namespace in YAML\nFirst create the namespace and then assign the deployment to a namespace.\napiVersion: v1\nkind: Namespace      # Namespace specs need only a name.\nmetadata:\n name: kiamol-ch11-uat\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:                       # The target namespace is part of the \n name: sleep                   # object metadata. The namespace needs\n namespace: kiamol-ch11-uat    # to exist, or the deployment fails.    \n\n  # The Pod spec follows.\n\n\nSee resources in all namespaces with --all namespaces\n# create the namespace and Deployment:\nkubectl apply -f sleep-uat.yaml\n\n# list the sleep Deployments in all namespaces:\nkubectl get deploy -l app=sleep --all-namespaces\n\n# delete the new UAT namespace:\nkubectl delete namespace kiamol-ch11-uat\n\n# list Deployments again:\nkubectl get deploy -l app=sleep --all-namespaces\n\n\nDeleting namespace deletes all resources\nWhen you delete everything in a namespace, like with the above example, you also delete all the resources in the namespace.\nOften people will delete a namespace and re-create it, this will delete everything in the namespace. For example:\nkl delete namespace {namespace}\nkubectl create namespace {namespace}"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#change-the-default-namespace",
    "href": "notes/k8s/18-Developer.html#change-the-default-namespace",
    "title": "Developer tips",
    "section": "Change the default namespace",
    "text": "Change the default namespace\nConstantly passing the --namespace flag is tedious. You can set the default namespace with kl config set-context:\n# list all contexts:\nkubectl config get-contexts\n\n# update the default namespace for the current context:\nkubectl config set-context --current --namespace=kiamol-ch11-test\n\n# list the Pods in the default namespace:\nkubectl get pods\nYou can also get the current context with:\nkl config current-context"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#switching-between-clusters",
    "href": "notes/k8s/18-Developer.html#switching-between-clusters",
    "title": "Developer tips",
    "section": "Switching Between Clusters",
    "text": "Switching Between Clusters\nUse contexts to switch b/w clusters. Config files with contexts live at ~/.kube.\n\nReset the default namespace\nBelow shows you how to reset the default namespace. You can also set another context to a different namespace.\nIt‚Äôs always a good idea to check your config as well.\n# setting the namespace to blank resets the default:\nkubectl config set-context --current --namespace=\n\n# printing out the config file shows your cluster connection:\nkubectl config view\nWhat does Michal do to manage different clusters?"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#private-images",
    "href": "notes/k8s/18-Developer.html#private-images",
    "title": "Developer tips",
    "section": "Private Images",
    "text": "Private Images\nKubernetes supports pulling private images by storing registry credentials in a special type of Secret object named docker-registry.\n % kl create secret --help                                                                             \nCreate a secret using specified subcommand.\n\nAvailable Commands:\n  docker-registry   Create a secret for use with a Docker registry\n  generic           Create a secret from a local file, directory, or literal value\n  tls               Create a TLS secret\nYou can set the secret like this, where we create a docker-registry secret called registry-creds\n# create the Secret using the details from the script:\nkubectl create secret docker-registry registry-creds \n   --docker-server=$REGISTRY_SERVER\n   --docker-username=$REGISTRY_USER\n   --docker-password=$REGISTRY_PASSWORD\n\n# show the Secret details:\nkubectl get secret registry-creds\nThis docker secret is mounted into the container like so:\nyaml title=\"bb-deployment.yaml\"     spec:       containers:         - name: bulletin-board           image: {{ .Values.registryServer }}/{{ .Values.registryUser }}/bulletin-board:{{ .Values.imageBuildNumber }}-kiamol            imagePullPolicy: Always               ports:             - name: http               containerPort: 8080         imagePullSecrets:       - name: {{ .Values.registrySecretName }}\nWhere the Helm values are configured like so:\nyaml title=\"values.yaml\" # port for the Service to listen on servicePort: 8012 # type of the Service: serviceType: LoadBalancer # domain of the registry server - e.g docker.io for Docker Hub registryServer: docker.io # user portion of the image repostory: registryUser: kiamol # build number portion of the image tag: imageBuildNumber: dev # name of the Secret containing registry credentials: registrySecretName: registry-creds"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#local-setup",
    "href": "notes/k8s/18-Developer.html#local-setup",
    "title": "Developer tips",
    "section": "Local Setup",
    "text": "Local Setup\nTry to encapsulate the CI process into a script that you run locally, that also includes a local version of K8s if possible, where you:\n\nBuild container images\nSpin everything up in a local K8s cluster\nRun/test the app\n\nThis won‚Äôt work all the time. You can also develop without containers, and setup GitHub Actions to do the container builds, tests, and deploy K8s in a test namespace."
  },
  {
    "objectID": "notes/k8s/03-Secrets.html",
    "href": "notes/k8s/03-Secrets.html",
    "title": "Secrets",
    "section": "",
    "text": "Unlike [[ConfigMap]], K8s doesn‚Äôt like to show you plain text version of your secret. You must decode it with base64 -d, this is not encrypted, just obfuscated.\nThe container still sees the original plain text data. Let‚Äôs manually create a secret like this:\nkl create secret generic sleep-secret-literal --from-literal=secret=shh...\nThen, we reference this seret in a deployment like this:\n% cat sleep/sleep-with-secret.yaml                                                                                                    \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          env:\n          - name: KIAMOL_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: sleep-secret-literal\n                key: secret\nIf we apply this kl apply -f sleep/sleep-with-secret.yaml , we can see the secret lke this:\n% kl exec -it deploy/sleep -- printenv | grep KIAMOL_SECRET                                                                           \nKIAMOL_SECRET=shh...\n\nYou shouldn‚Äôt expose secrets as environment variables as that is not very secure. You should store secrets in files that have restricted premissions.\n\nYou can also store your secrets in plain text in a YAML file like so:\n%cat todo-list/secrets/todo-db-secret-test.yaml                                                                                      \napiVersion: v1\nkind: Secret\nmetadata:\n  name: todo-db-secret-test\ntype: Opaque\nstringData:                          # use this field when using plain text stuff\n  POSTGRES_PASSWORD: \"kiamol-2*2*\"   # this is the plain text password\nInterestingly, you will be able to see the plain text password if you do this! See the metadata.annotations field:\n%kl get secret/sleep-secret-literal -o yaml\napiVersion: v1\ndata:\n  POSTGRES_PASSWORD: a2lhbW9sLTIqMio=\nkind: Secret\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"todo-db-secret-test\",\"namespace\":\"default\"},\"stringData\":{\"POSTGRES_PASSWORD\":\"kiamol-2*2*\"},\"type\":\"Opaque\"}\n  creationTimestamp: \"2022-12-01T18:22:56Z\"\n  name: todo-db-secret-test\n  namespace: default\n  resourceVersion: \"629050\"\n  uid: 35b42a79-a8dd-447d-a191-a295ca1e4d66\ntype: Opaque"
  },
  {
    "objectID": "notes/k8s/03-Secrets.html#mounting-secrets-as-files",
    "href": "notes/k8s/03-Secrets.html#mounting-secrets-as-files",
    "title": "Secrets",
    "section": "Mounting Secrets as Files",
    "text": "Mounting Secrets as Files\nThis is recommended over env variables\n% cat todo-list/todo-db-test.yaml                                                                                                     \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-db\nspec:\n  selector:\n    matchLabels:\n      app: todo-db\n  template:\n    metadata:\n      labels:\n        app: todo-db\n    spec:\n      containers:\n        - name: db\n          image: postgres:11.6-alpine\n          env:\n          - name: POSTGRES_PASSWORD_FILE\n            value: /secrets/postgres_password\n          volumeMounts:\n            - name: secret                           # Mounts a secret volume\n              mountPath: \"/secrets\"                  # The location\n      volumes:\n        - name: secret\n          secret:                                     # Volumen loaded\n            secretName: todo-db-secret-test           #Name of secret\n            defaultMode: 0400                         #Permissions set for files\n            items:\n            - key: POSTGRES_PASSWORD\n              path: postgres_password\nYou can see that this secret is now mounted as a file\n%kl exec deploy/todo-db -- ls /secrets                                                                                               \npostgres_password\n\n% kl exec deploy/todo-db -- cat /secrets/postgres_password                                                                            \nkiamol-2*2*"
  },
  {
    "objectID": "notes/k8s/03-Secrets.html#bringing-together-config-secrets-deployments-and-services",
    "href": "notes/k8s/03-Secrets.html#bringing-together-config-secrets-deployments-and-services",
    "title": "Secrets",
    "section": "Bringing together config, secrets, deployments and services",
    "text": "Bringing together config, secrets, deployments and services\nHere is an example file that uses both ConfigMaps and Secrets in a deployment\n% cat todo-list/todo-web-test.yaml | pbcopy\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-web-test\nspec:\n  ports:\n    - port: 8081\n      targetPort: 80\n  selector:\n    app: todo-web\n    environment: test\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-web-test\nspec:\n  selector:\n    matchLabels:\n      app: todo-web\n      environment: test\n  template:\n    metadata:\n      labels:\n        app: todo-web\n        environment: test\n    spec:\n      containers:\n        - name: web\n          image: kiamol/ch04-todo-list\n          env:\n          - name: ASPNETCORE_ENVIRONMENT\n            value: Test\n          volumeMounts:\n            - name: config\n              mountPath: \"/app/config\"\n              readOnly: true\n            - name: secret\n              mountPath: \"/app/secrets\"\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: todo-web-config-test\n            items:\n            - key: config.json\n              path: config.json\n        - name: secret\n          secret:\n            secretName: todo-web-secret-test\n            defaultMode: 0400\n            items:\n            - key: secrets.json\n              path: secrets.json\nThat json secret is stored like this:\n% cat todo-list/secrets/todo-web-secret-test.yaml                                                                                     \napiVersion: v1\nkind: Secret\nmetadata:\n  name: todo-web-secret-test\ntype: Opaque\nstringData:\n  secrets.json: |-\n    {\n      \"ConnectionStrings\": {\n        \"ToDoDb\": \"Server=todo-db;Database=todo;User Id=postgres;Password=kiamol-2*2*;\"\n      }\n    }\nYou can see that these files exist now\n% kl exec deploy/todo-web-test -- ls /app/                                                                                            \nconfig\nsecrets\n..."
  },
  {
    "objectID": "notes/k8s/03-Secrets.html#updating-configurations",
    "href": "notes/k8s/03-Secrets.html#updating-configurations",
    "title": "Secrets",
    "section": "Updating Configurations",
    "text": "Updating Configurations\nYour app may only read config when it starts, so if you change configuration settings via [[ConfigMap]] or [[Secrets]] then you would have to restart your app. Two ways to do this:\n\nkl rollout restart deploy/.... (preferred method)\nDelete all pods related to that deployment using a label selector or something similar and let the deployment restart them.\n\nContext From Discord chat with Michal https://discord.com/channels/1043031122721914940/1045846418331537459/1047961668426158192"
  },
  {
    "objectID": "notes/k8s/20-Health-Check.html",
    "href": "notes/k8s/20-Health-Check.html",
    "title": "Probes",
    "section": "",
    "text": "This is Chapter 12."
  },
  {
    "objectID": "notes/k8s/20-Health-Check.html#readiness-probe",
    "href": "notes/k8s/20-Health-Check.html#readiness-probe",
    "title": "Probes",
    "section": "Readiness probe",
    "text": "Readiness probe\nspec:             # This is the Pod spec in the Deployment.\n containers:\n   - image: kiamol/ch03-numbers-api\n     readinessProbe:        # Probes are set at the container level.\n       httpGet:\n         path: /healthz     # This is an HTTP GET, using the health URL.\n         port: 80       \n       periodSeconds: 5     # The probe fires every five seconds.\nThis is using a httpGet action, which is suited more for web apps. Will be marked as ready if code returned is b/w 200 and 399. When a Pod is detected as not ready, the Pod‚Äôs IP address is removed from the Service endpoint list, so it won‚Äôt receive any more traffic.\n\n\n\n\n\n\nWarning\n\n\n\nDeployments do not replace Pods that leave the ready state when a probe fails, so we‚Äôre left with two Pods running but only one receiving traffic.\nYou can get into a situation where no pods are receiving traffic at all\n\n\nThis is why you absolutely have to have a liveness probe, a readiness probe on its own is dangerous!"
  },
  {
    "objectID": "notes/k8s/20-Health-Check.html#liveness-probe",
    "href": "notes/k8s/20-Health-Check.html#liveness-probe",
    "title": "Probes",
    "section": "Liveness Probe",
    "text": "Liveness Probe\nUses the same mechanism as readiness probes, it even looks the same, but it wil restart the Pods if they become unhealthy, unlike readiness probes.\nThe Pod is not replaced, they are restarted (so run on the same node but new container).\nlivenessProbe:\n httpGet:                 # HTTP GET actions can be used in liveness and\n   path: /healthz         # readiness probes--they use the same spec.\n   port: 80\n periodSeconds: 10        \n initialDelaySeconds: 10  # Wait 10 seconds before running the first probe.\n failureThreshold: 2      # Allow two probes to fail before taking action.\n\nTesting Liveness Probe\nThis is a clever way of testing the livenessProbe:\nspec:\n  containers:\n  - name: liveness\n    image: repo/name\n    args:\n    - /bin/sh\n    - -c\n    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 5\n      periodSeconds: 5\nSource\nFailed liveness checks will cause a pod to restart, not to be replaced.\nFor transient issues, it works well, provided the application can restart successfully in a replacement container. Probes are also useful to keep applications healthy during upgrades, because rollouts proceed only as new Pods enter the ready state, so if a readiness probe fails, that will pause the rollout."
  },
  {
    "objectID": "notes/k8s/20-Health-Check.html#production",
    "href": "notes/k8s/20-Health-Check.html#production",
    "title": "Probes",
    "section": "Production",
    "text": "Production\n\nUsing both probes together\nAlso see the exec.command functionality which is very useful.\n\n\ntodo-list/db/todo-db.yaml\n\nspec:             \n containers:\n   - image: postgres:11.6-alpine\n     # full spec includes environment config\n     readinessProbe:\n       tcpSocket:           # The readiness probe tests the\n         port: 5432         # database is listening on the port.\n       periodSeconds: 5\n     livenessProbe:         # The liveness probe runs a Postgres tool,\n       exec:                # which confirms the database is running.\n         command: [\"pg_isready\", \"-h\", \"localhost\"]\n       periodSeconds: 10\n       initialDelaySeconds: 10\n\nDatabase probes mean Postgres won‚Äôt get any traffic until the database is ready, and if the Postgres server fails, then the database Pod will be restarted, with the replacement using the same data files in the EmptyDir volume in the Pod.\n\n\nPrevents Bad Rollouts\nWhat commonly happens is someone repalces the startup command with sleep or something similar for debugging and forgets to revert it back. The probes would catch that and keep the app available (because it would prevent a rollout).\nWhile the new Pod keeps failing, the old one is left running, and the app keeps working."
  },
  {
    "objectID": "notes/k8s/20-Health-Check.html#helm",
    "href": "notes/k8s/20-Health-Check.html#helm",
    "title": "Probes",
    "section": "Helm",
    "text": "Helm\nBecause Helm supports atomic installs & upgrades (--atomic) that rollback automatically if they fail, probes + Helm is a great combo.\nIf the Pod isn‚Äôt ready within the Helm timeout period, so the upgrade is rolled back, and the new Pod is removed; it doesn‚Äôt keep restarting and hit CrashLoopBackOff as it did with the kubectl update.\nJust a reminder: this is how to do a helm install and an upgrade\n# install\nhelm install --atomic todo-list todo-list/helm/v1/todo-list/\n# upgrade\nhelm upgrade --atomic --timeout 30s todo-list todo-list/helm/v2/todo-list/\nThis is what an atomic rollback looks like:\nError: UPGRADE FAILED: release todo-list failed, and has been rolled back due to atomic being set: timed out waiting for the condition"
  },
  {
    "objectID": "notes/k8s/20-Health-Check.html#forcing-a-container-to-exit",
    "href": "notes/k8s/20-Health-Check.html#forcing-a-container-to-exit",
    "title": "Probes",
    "section": "Forcing a container to exit",
    "text": "Forcing a container to exit\nYou can force a container to exit with the following command. This might be useful for testing:\nkl exec -it {pod name} -- killall5\nThis will cause the pod to restart the container, not replace it."
  },
  {
    "objectID": "notes/k8s/27-workload-placement.html",
    "href": "notes/k8s/27-workload-placement.html",
    "title": "Workload Placement",
    "section": "",
    "text": "Workload placement happens in two stages (1) filtering - which excludes any unsuitable nodes then (2) scoring - which ranks the remaining nodes to find the best fit."
  },
  {
    "objectID": "notes/k8s/27-workload-placement.html#adding-labels-to-nodes",
    "href": "notes/k8s/27-workload-placement.html#adding-labels-to-nodes",
    "title": "Workload Placement",
    "section": "Adding Labels To Nodes",
    "text": "Adding Labels To Nodes\nThis article assumes you are familiar with adding labels to nodes. See this article for more."
  },
  {
    "objectID": "notes/k8s/27-workload-placement.html#taint",
    "href": "notes/k8s/27-workload-placement.html#taint",
    "title": "Workload Placement",
    "section": "Taint",
    "text": "Taint\nTaints are a special kind of label with a key-value pair, but it tells the scheduler that a particular node is different. For example, the master taint is applied to control plane nodes by default (so your applications will not get scheduled on this important node).\nYou can use taint to record relevant attributes about nodes, like the type of hardware. When you add a taint, workloads will not be scheduled on that node unless you add a matching toleration to the workload.\nFor example, nothing will be scheduled if you add this taint to all nodes! Note that tainting doesn‚Äôt impact existing workloads, only future ones.\nkubectl taint nodes --all kiamol-disk=hdd:NoSchedule\nThis is how you would add the toleration to a workload:\nspec:                           # The Pod spec in a Deployment\n containers:\n   - name: sleep\n     image: kiamol/ch03-sleep      \n tolerations:                  # Lists taints this Pod is happy with\n     - key: \"kiamol-disk\"      # The key, value, and effect all need \n       operator: \"Equal\"       # to match the taint on the node.\n       value: \"hdd\"\n       effect: \"NoSchedule\"\nThe effect can be these three types: 1. NoSchedule - The Pod will not be scheduled on the node. 2. PreferNoSchedule - The scheduler will try to avoid scheduling the Pod on the node. 3. NoExecute - The Pod will not be scheduled on the node and any existing Pods on the node will be evicted. This taint is retroactive, meaning that it will effect existing Pods as well as new ones (this is different to the other two).\nSee this article for more info.\nYou can add a taint label to a node like this:\n% kubectl taint nodes node1 key1=value1:NoSchedule\n\n#remove the taint like This\n% kubectl taint nodes node1 key1=value1:NoSchedule-\nTaints are only for negative associations - you can‚Äôt use them to say, ‚Äúthis node is good for this workload‚Äù. For that, you need to use nodeSelector or nodeAffinity. You would not use a taint so say a workload should run on a GPU node, for example."
  },
  {
    "objectID": "notes/k8s/27-workload-placement.html#node-selector",
    "href": "notes/k8s/27-workload-placement.html#node-selector",
    "title": "Workload Placement",
    "section": "Node Selector",
    "text": "Node Selector\nThis is an example of using NodeSelector:\nspec:\n containers:\n   - name: sleep\n     image: kiamol/ch03-sleep      \n tolerations:                            # The Pod tolerates nodes \n   - key: \"kiamol-disk\"                  # with the hdd taint.\n     operator: \"Equal\"\n     value: \"hdd\"\n     effect: \"NoSchedule\"\n nodeSelector:                           # The Pod will run only on nodes\n   kubernetes.io/arch: zxSpectrum        # that match this CPU type.\nThe arch example are automatically set by Kubernetes on each node. For example, on my laptop if I do kl get nodes -o yaml it will have the key,value pair architecture: arm64 under nodeInfo.\nNode selectors ensure that apps run only on nodes with specific label values, but you usually want some more flexibility than a straight equality match. A finer level of control comes with affinity and antiaffinity.\nHere is another example of using nodeSelector:\nFirst, label your nodes:\n# see list of nodes w/names\n% kubectl get nodes --show-labels\n\n# apply a label to a node\n% kubectl label nodes <your-node-name> disktype=ssd\nThen, add the nodeSelector to your config:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  nodeSelector:\n    disktype: ssd"
  },
  {
    "objectID": "notes/k8s/27-workload-placement.html#affinity",
    "href": "notes/k8s/27-workload-placement.html#affinity",
    "title": "Workload Placement",
    "section": "Affinity",
    "text": "Affinity\nUnlike taint this is a positive association between a pod and a node. Affinity uses a node selector but with a match expression rather than equality. There is two kinds:\n\nrequiredDuringSchedulingIgnoredDuringExecution: The scheduler can‚Äôt schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax.\npreferredDuringSchedulingIgnoredDuringExecution: The scheduler will try to meet the rule. If a matching node is not available, the Pod will still be scheduled.\n\nYou can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node labels, which allows you to define rules for which Pods can be co-located on a node.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-node-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - antarctica-east1\n            - antarctica-west1\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:2.0\nIn this example, the following rules apply:\n\nThe node must have a label with the key topology.kubernetes.io/zone and the value of that label must be either antarctica-east1 or antarctica-west1.\nThe node preferably has a label with the key another-node-label-key and the value another-node-label-value.\n\nThe operator used above is In, but it can also be In, NotIn, Exists, DoesNotExist, Gt and Lt.\nThe NotIn and DoesNotExist allow you to define antiaffinity rules. For example, you could say ‚Äúdon‚Äôt schedule this pod on a node that already has a pod with this label‚Äù. You could also use taints for this as well.\n\nAffinity Weight\nFor preferredDuringSchedulingIgnoredDuringExecution scheduling, you can set a weight b/w 1-100. The scheduler adds all the weights of all the preferred rules and adds that to the score when making a scheduling decision.\nExample of two different weights:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-affinity-anti-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/os\n            operator: In\n            values:\n            - linux     # The Node MUST have the label `kubernetes.io/os=linux`\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: label-1\n            operator: In\n            values:\n            - key-1\n      - weight: 50\n        preference:\n          matchExpressions:\n          - key: label-2\n            operator: In\n            values:\n            - key-2\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:2.0"
  },
  {
    "objectID": "notes/k8s/27-workload-placement.html#inter-pod-affinity-and-anti-affinity",
    "href": "notes/k8s/27-workload-placement.html#inter-pod-affinity-and-anti-affinity",
    "title": "Workload Placement",
    "section": "Inter-pod affinity and anti-affinity",
    "text": "Inter-pod affinity and anti-affinity\nSo you can either have pods run together on same node or make sure they run on seperate nodes\nSee these docs if you need this. Maybe you want GPU workloads to run separately, for example.\nThe affinity or anti-affinity can be scoped to a node, a zone, a region, etc. To set the scope you set the topologyKey to the appropriate label. For example, if you want to run pods on the same zone, you would set topologyKey to topology.kubernetes.io/zone.\nThis prevents multiple replicas with the label app=store on the same node:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-cache\nspec:\n  selector:\n    matchLabels:\n      app: store\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: store\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - store\n            topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: redis-server\n        image: redis:3.2-alpine\nFrom the docs"
  },
  {
    "objectID": "notes/k8s/27-workload-placement.html#topology",
    "href": "notes/k8s/27-workload-placement.html#topology",
    "title": "Workload Placement",
    "section": "Topology",
    "text": "Topology\nTopology refers to physical layout of your cluster. The hostname label is always present and is unique per node. Cloud providers add region and zone labels. A topology key sets the level where the affinity applies. For example, hostname would force all pods onto the same node, zone would force all pods onto the same zone, etc. Antiaffinity works the same, where you can keep nodes from being scheduled on the same node, zone, etc.\naffinity:                           # Affinity rules for Pods use\n podAffinity:                      # the same spec as node affinity.\n   requiredDuringSchedulingIgnoredDuringExecution:\n     - labelSelector:\n         matchExpressions:         # This looks for the app and\n           - key: app              # component labels to match.\n             operator: In\n             values:\n               - numbers\n           - key: component\n             operator: In\n             values:\n               - api\n       topologyKey: \"kubernetes.io/hostname\" \nThis is another example, where the AntiAffinity rule says ‚Äúdon‚Äôt schedule this pod on a node within the same zone as one or more pods with the label `security=S2‚Äù:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: topology.kubernetes.io/zone\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: security\n              operator: In\n              values:\n              - S2\n          topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: with-pod-affinity\n    image: registry.k8s.io/pause:2.0\nfrom the docs\nRead this article for more info."
  },
  {
    "objectID": "notes/k8s/helm/16-Creating Your Own Helm Charts.html",
    "href": "notes/k8s/helm/16-Creating Your Own Helm Charts.html",
    "title": "Creating Helm Charts",
    "section": "",
    "text": "This is going to be really light, as we don‚Äôt want to get too deep into this. You can really just skip this if you like.\nYou can reference a directory, vs a zip archive when developing locally."
  },
  {
    "objectID": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#validate-with-helm-lint",
    "href": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#validate-with-helm-lint",
    "title": "Creating Helm Charts",
    "section": "Validate with helm lint",
    "text": "Validate with helm lint\nhelm lint directory/"
  },
  {
    "objectID": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#install",
    "href": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#install",
    "title": "Creating Helm Charts",
    "section": "Install",
    "text": "Install\nhelm install directory/"
  },
  {
    "objectID": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#notes.txt",
    "href": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#notes.txt",
    "title": "Creating Helm Charts",
    "section": "NOTES.txt",
    "text": "NOTES.txt\nThis is a file you can put in /templates that will display a nice message. For example:\n\n\nch12/todo-list/helm/v1/todo-list/templates/NOTES.txt\n\nInstalled Kiamol to-do list {{ .Chart.Version }}. This is how to get the URL:\n $ kubectl get svc {{ .Release.Name }}-web -o jsonpath='http://{.status.loadBalancer.ingress[0].*}:{{ .Values.servicePort }}'%\n\nHere are the docs for NOTES.txt"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html",
    "title": "Helm Upgrades & Rollbacks",
    "section": "",
    "text": "Recommended pattern:"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#install-test-the-new-version",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#install-test-the-new-version",
    "title": "Helm Upgrades & Rollbacks",
    "section": "1. Install & test the new version",
    "text": "1. Install & test the new version\nlist all installed releases:\n% helm ls -q \nch10-vweb\nsee which versions are available\n% helm search repo vweb --versions  \nNAME        CHART VERSION   APP VERSION DESCRIPTION\nkiamol/vweb 2.0.0           2.0.0       Simple versioned web app\nkiamol/vweb 1.0.0           1.0.0       Simple versioned web app\nInstall the new version of the app\n# check the values for the new chart version:\nhelm show values kiamol/vweb --version 2.0.0\n\nhelm install --set servicePort=8020 --set replicaCount=1 --set serviceType=ClusterIP ch10-vweb-v2 kiamol/vweb --version 2.0.0\nAfter you test the app,"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#uninstall-the-test-release",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#uninstall-the-test-release",
    "title": "Helm Upgrades & Rollbacks",
    "section": "2. Uninstall the test release",
    "text": "2. Uninstall the test release\nYou can see a list of all releases with helm list\n# see a list of releases\nhelm list\n....\n\nhelm uninstall ch10-vweb-v2"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#perform-an-upgrade",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#perform-an-upgrade",
    "title": "Helm Upgrades & Rollbacks",
    "section": "3. Perform an upgrade",
    "text": "3. Perform an upgrade\nYou can upgrade like this, optionally using the --reuse-values and --atomic flags:\nThe --atomic flag is important, always use this!\n\nwith the atomic flag. It waits for all the resource updates to complete, and if any of them fails, it rolls back every other resource to the previous state.\n\nhelm upgrade --reuse-values --atomic ch10-vweb kiamol/vweb --version 2.0.0\n\nAlways use the --atomic flag!\n\n\nWhen you upgrade, the --reuse-values flag will often be handy. However, this can cause things to break if the api changes between versions. So use with extreme care!"
  },
  {
    "objectID": "notes/k8s/helm/21-Testing-With-Helm.html",
    "href": "notes/k8s/helm/21-Testing-With-Helm.html",
    "title": "Testing With Helm",
    "section": "",
    "text": "In addition to liveness and readiness checks which helps Helm do automatic rollbacks, you can add explicit tests in the form of a Job. Example:\nThis is a cool use of a Job, which basically is running an integration test! We are making sure we can exeucte a SQL query against the database here.\nThis is an example of how you would execute this test:\nHelm manages Jobs for you. It doesn‚Äôt clean up completed Jobs, so you can check the Pod status and logs if you need to, but it replaces them when you repeat the test command, so you can rerun the test suite as often as you like."
  },
  {
    "objectID": "notes/k8s/helm/21-Testing-With-Helm.html#pre-upgrade-jobs",
    "href": "notes/k8s/helm/21-Testing-With-Helm.html#pre-upgrade-jobs",
    "title": "Testing With Helm",
    "section": "Pre-Upgrade Jobs",
    "text": "Pre-Upgrade Jobs\nThere‚Äôs one other use for Jobs that helps to make sure upgrades are safe, by running them before upgrades so you can check the current release is in a valid state to be upgraded.\nA pre-upgrade job:\n\n\ntodo-db-check-job.yaml\n\napiVersion: batch/v1\nkind: Job                         # The standard Job spec again\nmetadata:\n  name:  {{ .Release.Name }}-db-check\n # metadata has labels\n annotations:\n   \"helm.sh/hook\": pre-upgrade   # This runs before an upgrade and\n   \"helm.sh/hook-weight\": \"10\"   # tells Helm the order in which to create\nspec:                             # the object after the ConfigMap\n template:                       # that the Job requires\n   spec:\n     restartPolicy: Never\n     containers:\n       - image: postgres:11.8-alpine\n         # env includes secrets\n         command: [\"/scripts/check-postgres-version.sh\"]\n         volumeMounts:\n           - name: scripts           # Mounts the ConfigMap volume\n             mountPath: \"/scripts\"\n\nFor reference, this is the ConfigMap that needs to run before the above job, and that is the background for the hook-weight\n\n\ntodo-db-check-configMap.yaml\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name:  {{ .Release.Name }}-db-check-scripts\n  labels:\n    kiamol: ch12\n  annotations:\n    \"helm.sh/hook\": pre-upgrade\n    \"helm.sh/hook-weight\": \"1\"\ndata:\n  check-postgres-version.sh: |-\n    #!/bin/sh\n    PG_VERSION=$(pg_config --version)\n    if [ \"$PG_VERSION\" == \"PostgreSQL 11.6\" ]; then\n      echo '** Postgres at expected version - good to upgrade **'\n      exit 0\n    else\n      echo \"** ERROR - Postgres not at expected version - wanted: 11.6, got: $PG_VERSION - CANNOT UPGRADE **\"\n      exit 1\n    fi\n\nIn this example, when we try to upgrade our app it will fail because of a pre-upgrade check:\n#this will fail\n% helm upgrade --atomic --timeout 30s todo-list todo-list/helm/v4/todo-list/\n\n#see logs of the failed job, first get name of job which\n#  corresponds with the YAML\n% kl get job\n\n# see logs\n% kl logs jobs/todo-list-db-check\nAnnotations control where Jobs run in the Helm lifecycle, so the above job will only run for upgrades.\npre-upgrade validation and automatic rollbacks help to keep your application upgrades self-healing, too. Helm isn‚Äôt a prerequisite for that, but if you‚Äôre not using Helm, you should consider implementing these features using kubectl in your deployment pipeline."
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html",
    "href": "notes/k8s/helm/15-Helm.html",
    "title": "Helm Intro",
    "section": "",
    "text": "It‚Äôs a client-side tool\nUses kubectl to connect to your cluster\nAdd repos with a URL helm repo add https://...\nUpdate repos with helm repo update\nIt‚Äôs basically parametrized YAML\n\nHelm templates are not valid YAML, so you can‚Äôt use kubectl\nJeremy Lewi: Use Kustomize, not Helm, if you can."
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#add-a-repo",
    "href": "notes/k8s/helm/15-Helm.html#add-a-repo",
    "title": "Helm Intro",
    "section": "Add a repo",
    "text": "Add a repo\n helm repo add kiamol https://kiamol.net"
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#inspect-default-values-in-chart",
    "href": "notes/k8s/helm/15-Helm.html#inspect-default-values-in-chart",
    "title": "Helm Intro",
    "section": "Inspect default values in chart",
    "text": "Inspect default values in chart\nSee what versions are available\n% helm search repo vweb --versions  \nNAME        CHART VERSION   APP VERSION DESCRIPTION\nkiamol/vweb 2.0.0           2.0.0       Simple versioned web app\nkiamol/vweb 1.0.0           1.0.0       Simple versioned web app\nSee the default values:\n% helm show values kiamol/vweb --version 1.0.0                                                                            \nservicePort: 8090\nreplicaCount: 2"
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#install-the-chart",
    "href": "notes/k8s/helm/15-Helm.html#install-the-chart",
    "title": "Helm Intro",
    "section": "Install the chart",
    "text": "Install the chart\nOverride default values, and name the release ch10-vweb:\n helm install --set servicePort=8010 --set replicaCount=1 ch10-vweb kiamol/vweb --version 1.0.0\nSee the deployment (labels omitted in below output for brevity)\n% kl get deploy --show-labels                                                                                                              \nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nch10-vweb   1/1     1            1           39s\n\nDry runs\nThere is also a --dry-run flag that will generate the YAML for you."
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#update-the-release",
    "href": "notes/k8s/helm/15-Helm.html#update-the-release",
    "title": "Helm Intro",
    "section": "Update the release",
    "text": "Update the release\nUse helm upgrade :\nIn this case we are going to increase the replica count:\n% helm upgrade --set servicePort=8010 --set replicaCount=3 ch10-vweb kiamol/vweb --version 1.0.0\nRelease \"ch10-vweb\" has been upgraded. Happy Helming!\nNAME: ch10-vweb\nLAST DEPLOYED: Tue Dec 13 11:10:04 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None"
  },
  {
    "objectID": "notes/programming-languages/index.html",
    "href": "notes/programming-languages/index.html",
    "title": "programming languages",
    "section": "",
    "text": "High level takeaways after completing the 3-Part Coursera class Programming Languages with Dan Grossman.\nYour GitHub repo for this class (private) is here."
  },
  {
    "objectID": "notes/programming-languages/index.html#sml-standard-ml-part-a",
    "href": "notes/programming-languages/index.html#sml-standard-ml-part-a",
    "title": "programming languages",
    "section": "SML (Standard ML) Part A",
    "text": "SML (Standard ML) Part A\n\nYou setup vim to have an IDE for this. See notes in the VIM section below.\nML is a statically typed language with magical type inference that works really well. It automatically determines the types and is very intuitive and helpful.\nLearned how to use recursion everywhere instead of loops, particularly with hd, tl and cons.\nLocal variable binding with let is very important (which also allows you to bind local/private functions as well)\ncons allows you to append to the beginning of a list\nThere is an option type that is NONE or SOME v\nThis language doesn‚Äôt encourage mutation, which is a feature. Otherwise, you can use a reference which is like a pointer to mutate a variable.\npattern matching with a case expression: This is one of the coolest things that I learned, and something similar is coming to Python v 3.10.\n\nYou can have nested patterns\nYou can pattern match against function arguments which allow for really nice syntax for achieving multiple dispatch type of functionality.. (not sure about python)\nYou can pattern match against types as well as data structures.\nYou can have constants in there as well.\n\ncase name \n     NameType name => ...\n   | (first, \"MyLastName\") => ...\n   | (first, last) => ...\n   | name => ...\n   | _ => ...\nTail recursion with accumulators. Ex- factorial\nThe fn keyword is used to define anonymous functions.\nML uses lexical scope which means function is evaluated in the environment where the function was defined. dynamic scope, which is usually not desired, is the alternative where the function is evaluated in the in the environment it is called.\nClosure - the call stack has a ‚Äúpair‚Äù that is the (function, environment when the function was defined). This pair is called the closure. The call stack has a snapshot of what the environment looked like at the time the function was defined.\nfold is like reduce.\nML supports function composition like this with the keyword o: f1 o f2 o f3\n\nbest to do a val binding to avoid unnecessary wrapping: val newfunc = f1 o f2\nwith o you apply functions from right to left so f1 o f2 x is the same as f1(f2(x)) there is an alternative that is left to right called the pipeline operator.\n\nCurrying and partial application\n\nUniversal way to make a func curryable: ml       fun myfunc x           let fun f2 (z) = z               fun f1 (y) = f2(y)           begin               f1           end\nML has first class support for currying so you don‚Äôt have to do the above hack.\n\nML supports mutual recursion just like let-rec in racket."
  },
  {
    "objectID": "notes/programming-languages/index.html#racket-part-b",
    "href": "notes/programming-languages/index.html#racket-part-b",
    "title": "programming languages",
    "section": "Racket (Part B)",
    "text": "Racket (Part B)\nRacket is related to Lisp and Scheme. Everything is a function. Parenthesis for everything. The position of parenthesis changes the meaning of the code.\n\nRacket has dynamic typing, unlike SML.\nThunks: Wrap a function in a zero argument function to delay evaluation. Applications:\n\nStreams: the function will return a tuple of (value, func), and when you call func it will return (value, func) so you get one value at a time. This is not specific to Racket.\nLazy evaluation: You can use thunks to delay execution like a promise to a later time. This is an example of lazy evalution that doesn‚Äôt actually evaluate anything until being forced to:\n\n\n(define (my-delay f) (mcons #f f))\n\n(define (my-force th)\n\n(if (mcar th) (mcdr th) (begin (set-mcar! th #t) (set-mcdr! th ((mcdr th))) (mcdr th))))\nRacket allows you use macros that will evaluate before the code is run and that will ‚Äúexpand‚Äù into valid racket syntax.\nYou implemented your own small programming language. This used recursive calls to evluate expressions with the base case being the values (Integer, strings, etc). - Interperter: write a program in another language A that takes programs in B and produces answers directly. A better term would be ‚Äúevaluator‚Äù. - Compiler: write program in another language A that takes programs in B and produces an equivalent program in langauage C. A better term here would be ‚Äútranslator‚Äù.\nClosures: for lexical scope, the interpreter has a stack of tuples. The tuples are (1) the function to be called (2) the environment, which contains the value of all variables at the time the function was defined. You also have to track the arguments for the function seperately, so you can evaluate the arguments in the environment the function was run in."
  },
  {
    "objectID": "notes/programming-languages/index.html#ruby-part-c",
    "href": "notes/programming-languages/index.html#ruby-part-c",
    "title": "programming languages",
    "section": "Ruby (Part C)",
    "text": "Ruby (Part C)\nI didn‚Äôt spend too much time some concepts I was mostly familiar with this.\n\nRuby is OOP, dynamically typed.\nRuby is pure OOP, even top level functions and variables are part of the built-in Object class.\nThey have fastcore like shortcuts for getters and setters:\n\nattr_reader :y, :z # defines getters \nattr_accessor :x # defines getters and setters\nnewlines are important. The syntax can change without them.\nDynamic class definitions. The following code will result in Class with the methods foo and bar! The second one doesn‚Äôt override the first one!\nclass Class\n    def foo\n        ...\n    end\nend\n\nclass Class\n    def bar\n        ...\n    end\nend\n\nBlocks\nThey also have a very convenient lambda like thing called Blocks:\nsum = 0 \n[4,6,8].each { |x| sum += x \n               puts sum }\nYou can use Blocks to make accumulators too, and even use inject to initialize the accumulator:\nsum = [4,6,8].inject(0) { |acc,elt| acc + elt }\nTo use blocks in a method, you will have to look that up in the docs. This involves the yield keyword. For example, this code will print ‚Äúhi‚Äù 3 times:\ndef foo x \n  if x \n    yield \n   else \n    yield \n    yield \n   end \nend \n\nfoo (true) { puts \"hi\" } \nfoo (false) { puts \"hi\" }\nBlocks are not first class functions even though they kind of look like lambdas. Lets say you wanted to map over an array but wanted to return an array of functions instead of values. The way to do this is to use the keyword lambda:\nc = a.map {|x| {|y| x >= y} } # wrong, a syntax error\n\nc = a.map {|x| lambda {|y| x >= y} } # this will work\n\nSubclassing\n\nsuper calls the same method in the parent class. You dont have to do super.method_name(), just super.\nInstance variables are preceeded with @\n\nChild classes are defined like this:\nclass Child < Parent\n ...\nend\n\n\n\nTyping\nThey discussed the various ways different type systems are constructed. The interface idiom, that is familar to you from Golang (but not specific to Golang) was introduced."
  },
  {
    "objectID": "notes/programming-languages/index.html#vim",
    "href": "notes/programming-languages/index.html#vim",
    "title": "programming languages",
    "section": "VIM",
    "text": "VIM\nFor the Standard ML programming language I decided to force myself to use vim. I added the following things to my .vimrc to make it manageable. Note the plugin jez/vim-better-sml\n\" from https://github.com/jez/vim-as-an-ide\nset nocompatible\n\ninoremap <C-e> <C-o>A\n\n\nfiletype off\n\nset rtp+=~/.vim/bundle/Vundle.vim\ncall vundle#begin()\n\nPlugin 'VundleVim/Vundle.vim'\n\n\" ----- Making Vim look good ------------------------------------------\nPlugin 'altercation/vim-colors-solarized'\nPlugin 'tomasr/molokai'\nPlugin 'vim-airline/vim-airline'\nPlugin 'vim-airline/vim-airline-themes'\n\n\" ----- Vim as a programmer's text editor -----------------------------\nPlugin 'scrooloose/nerdtree'\nPlugin 'jistr/vim-nerdtree-tabs'\nPlugin 'vim-syntastic/syntastic'\nPlugin 'xolox/vim-misc'\nPlugin 'xolox/vim-easytags'\nPlugin 'majutsushi/tagbar'\nPlugin 'ctrlpvim/ctrlp.vim'\n\" ----- Working with Git ----------------------------------------------\nPlugin 'airblade/vim-gitgutter'\nPlugin 'tpope/vim-fugitive'\nPlugin 'Raimondi/delimitMate'\nPlugin 'jez/vim-better-sml'\nPlugin 'christoomey/vim-tmux-navigator'\nPlugin 'benmills/vimux'\ncall vundle#end()\n\nfiletype plugin indent on\n\nset number\nset ruler\nset showcmd\nset incsearch\nset hlsearch\nset backspace=indent,eol,start\n\nsyntax on\nset mouse=a"
  },
  {
    "objectID": "notes/fastai/03_data.html",
    "href": "notes/fastai/03_data.html",
    "title": "Data",
    "section": "",
    "text": "from fastbook import *"
  },
  {
    "objectID": "notes/fastai/03_data.html#hello-world-datablock",
    "href": "notes/fastai/03_data.html#hello-world-datablock",
    "title": "Data",
    "section": "Hello World DataBlock",
    "text": "Hello World DataBlock\nThe argument get_x and get_y operate on an iterable. Let‚Äôs define an interable as our data:\n\ndata = list(range(100))\n\n\ndef get_x(r): return r\ndef get_y(r): return r + 10\ndblock = DataBlock(get_x=get_x, get_y = get_y)\ndsets = dblock.datasets(data)\n\nYou can see a dataset like so:\n\ndsets.train[0]\n\n(89, 99)\n\n\nYou can also see a DataLoader like so:\n\ndls = dblock.dataloaders(data, bs=5)\n\n\nnext(iter(dls.train))\n\n(tensor([57, 66, 73, 30, 14]), tensor([67, 76, 83, 40, 24]))\n\n\n\nWith A DataFrame\nSimilarly, you can operate on one row at a time:\n\nimport pandas as pd\ndf = pd.DataFrame({'x': range(100), 'y': range(100) })\ndf.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      0\n      0\n    \n    \n      1\n      1\n      1\n    \n    \n      2\n      2\n      2\n    \n    \n      3\n      3\n      3\n    \n    \n      4\n      4\n      4\n    \n  \n\n\n\n\n\ndef get_x(r): return r.x\ndef get_y(r): return r.y + 10\ndblock = DataBlock(get_x=get_x, get_y=get_y)\ndsets = dblock.datasets(df)\n\n\ndsets.train[0]\n\n(78, 88)\n\n\n\ndls = dblock.dataloaders(df, bs=3)\nnext(iter(dls.train))\n\n(tensor([90, 55, 11]), tensor([100,  65,  21]))\n\n\n\ndef tracer(nm):\n    def f(x, nm):\n        # print(f'{nm}:')\n        # print(f'\\tinput: {x}')\n        # import ipdb; ipdb.set_trace()\n        return str(x)\n    return partial(f, nm=nm)\n\n\ndef mult_0(x): return x * 0\ndef add_1(x): return x +1 \ntb = TransformBlock(item_tfms=[tracer('item_tfms')])\n# def get_y(l): return sum(l)\ndb = DataBlock(blocks=(TransformBlock, TransformBlock),\n               get_x=mult_0,\n               get_y=add_1,\n               item_tfms=lambda x: str(x))\n\n\ndata = L(range(10))\nresult = db.datasets(data)\n\n\ndb.summary(data)\n\nSetting-up type transforms pipelines\nCollecting items from [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFound 10 items\n2 datasets of sizes 8,2\nSetting up Pipeline: mult_0\nSetting up Pipeline: add_1\n\nBuilding one sample\n  Pipeline: mult_0\n    starting from\n      1\n    applying mult_0 gives\n      0\n  Pipeline: add_1\n    starting from\n      1\n    applying add_1 gives\n      2\n\nFinal sample: (0, 2)\n\n\nCollecting items from [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFound 10 items\n2 datasets of sizes 8,2\nSetting up Pipeline: mult_0\nSetting up Pipeline: add_1\nSetting up after_item: Pipeline: <lambda> -> ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: \n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: <lambda> -> ToTensor\n    starting from\n      (0, 2)\n    applying <lambda> gives\n      (0, 2)\n    applying ToTensor gives\n      (0, 2)\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nNo batch_tfms to apply\n\n\n\nresult.train[0]\n\n(0, 5)\n\n\n\nresult = db.dataloaders(data, bs=3)\n\n\nthing = iter(result.train)\n\n\nnext(thing)\n\n(('0', '0', '0'), ('6', '7', '4'))\n\n\n\nnext(thing)\n\n(('0', '0', '0'), ('9', '5', '3'))\n\n\n\n??TransformBlock\n\n\ndb = DataBlock(blocks=(TransformBlock, tb),\n              get_y=lambda x: str(x),\n              batch_tfms=tracer('batch_tfms'))\n\n\nresult = db.datasets(data)\nresult = db.dataloaders(data, bs=3)\n\n\nresult\n\n<fastai.data.core.DataLoaders>\n\n\n\nthing = iter(result.train)\n\n\nnext(thing)\n\n(('1', '5', '6'), ('1', '5', '6'))\n\n\n\nf = aug_transforms()[0]\n\n\nf\n\nFlip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5}:\nencodes: (TensorImage,object) -> encodes\n(TensorMask,object) -> encodes\n(TensorBBox,object) -> encodes\n(TensorPoint,object) -> encodes\ndecodes:"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#dataloaders",
    "href": "notes/fastai/01_fundamentals.html#dataloaders",
    "title": "Fundamentals",
    "section": "DataLoaders",
    "text": "DataLoaders\nDataLoaders is a thin class around DataLoader, and makes them available as train and valid.\nSame thing applies to Datasets and Dataset.\nIn pytorch, Dataset is fed into a DataLoader."
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#datablocks",
    "href": "notes/fastai/01_fundamentals.html#datablocks",
    "title": "Fundamentals",
    "section": "DataBlocks",
    "text": "DataBlocks\n\nUse this to create DataLoaders\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\nDataBlocks are a template for creating DataLoaders, and need to be instantiated somehow - for example given a path where to find the data:\ndls = bears.dataloaders(path)\nYou can modify the settings of a DataBlock with new:\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) #book has more examples\ndls = bears.dataloaders(path)\nYou can sanity check / see transformed data with show_batch:\n>>> dls.train.show_batch(max_n=8, nrows=2, unique=True)\n... images\nYou also use DataBlocks for data augmentation, with batch_tfms:\nbears = bears.new(\n    item_tfms=Resize(128),        \n    batch_tfms=aug_transforms(mult=2)\n)\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#training",
    "href": "notes/fastai/01_fundamentals.html#training",
    "title": "Fundamentals",
    "section": "Training",
    "text": "Training\nMost things use learn.fine_tune(), when you cannot fine-tune like tabular data, you often use learn.fit_one_cycle\nYou can also do learn.show_results(...)\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): \n    return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n        path=str(path), \n        fnames=get_image_files(path), \n        valid_pct=0.2, \n        seed=42,\n        label_func=is_cat, \n        item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\nMore info on what this is in later sections.\n\nInterpetability\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\nAlso see top losses:\ninterp.plot_top_losses(5, nrows=1)\n\n\nCleaning\nYou can get a ImageClassifierCleaner which allows you to choose (1) a category and (2) data partition (train/val) and shows you the highest loss items so you can decide whether to Keep, Delete, Change etc.\ncleaner = ImageClassifierCleaner(learn)\ncleaner\nThe thing doesn‚Äôt actually delete/change anything but gives you the idxs that allow you to do things with them\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\nLoading / Saving\nSaving a model can be done with learn.export, when you do this, fastai will save a file called ‚Äúexport.pkl‚Äù\nlearn.export()\nload_learner can be used to load a model\nlearn_inf = load_learner(path/'export.pkl')\n\n\nPredicting\nWhen you call predict, you will get three things: (1) class, (2) the index of the predicted category (3) Probabilities of each category\n>>> learn_inf.predict('images/grizzly.jpg')\n('grizzly', tensor(1), tensor([9.0767e-06, 9.9999e-01, 1.5748e-07]))\nYou can see all the classes with dls.vocab:\n>>> learn_inf.dls.vocab\n(#3) ['black','grizzly','teddy']\nZach: learn.dls.vocab or learn.dls.categorize.vocab is another way to get the class names."
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#computer-vision",
    "href": "notes/fastai/01_fundamentals.html#computer-vision",
    "title": "Fundamentals",
    "section": "Computer Vision",
    "text": "Computer Vision\nYou can open an image with Pilow (PIL)\nim3 = Image.open(im3_path)\nim3\n\n#convert to numpy\narray(im3)\n# convert to pytorch tensor\ntensor(im3)\n\nPixel Similarity Baseline\n\nCompute avg pixel value for 3‚Äôs and 7‚Äôs\nAt inference time, see which one its similar too, using RMSE (L2 Norm) and MAE (L1 Norm)\n\nKind of like KNN\nTaking an inference tensor, a_3 and calculate distance to mean 3 and 7:\n# MAE & RMSE for 3  vs avg3\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n\n# MAE & RMSE for 3  vs avg7\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n\n# Use Pytorch Losses to do the same thing for 3 vs avg 7\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n\nnumpy\nTake the mean over an axis:\ndef mnist_distance(a,b): \n    #(-2,1) means take the average of the last 2 axis\n    return (a-b).abs().mean((-2,-1))"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#sgd-from-scratch",
    "href": "notes/fastai/01_fundamentals.html#sgd-from-scratch",
    "title": "Fundamentals",
    "section": "SGD from scratch",
    "text": "SGD from scratch\n\nMinimal Example\n# the loss function\ndef mse(y, yhat): \n    return (y - yhat).square().mean().sqrt()\n\n# the function that produces the data\ndef quadratic(x, params=[.75, -25.5, 15]):\n    a,b,c = params\n    noise = (torch.randn(len(x)) * 3)\n    return a*(x**2) + b*x +c + noise\n\n# generate training data\nx = torch.arange(1, 40, 1)\ny = quadratic(x)\n\n# define the training loop\ndef apply_step(params, pr=True):\n    lr = 1.05e-4\n    preds = quadratic(x, params)\n    loss = mse(preds, y)\n    loss.backward()\n    params.data -= params.grad.data * lr\n    if pr: print(f'loss: {loss}')\n    params.grad = None\n\n# initialize random params\nparams = torch.rand(3)\nparams.requires_grad_()\nassert params.requires_grad\n\n# train the model\nfor _ in range(1000):\n    apply_step(params)\n\n\nMNIST\nA Dataset in pytorch is required to return a tuple of (x,y) when indexed. You can do this in python as follows:\n# Turn mnist data into vectors 3dim -> 2dim\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n# Generate label tensor\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n# Create dataset\ndset = list(zip(train_x,train_y))\n\n# See shapes from first datum in the dataset\n>>> x,y = dset[0]\n>>> x.shape, y.shape\n(torch.Size([784]), torch.Size([1]))\n\n\n# Do the same thing for the validation set\n....\n\nMini Batch SGD\n# `@` and dot product is the same:\na, b = torch.rand(10), torch.rand(10)\nassert a.dot(b) == a@b\n\n# define model\ndef init_params(size, std=1.0): \n    return (torch.randn(size)*std).requires_grad_()\nweights = init_params((28*28,1))\nbias = init_params(1)\n\ndef linear1(xb): return xb@weights + bias\n\n#naive loss (for illustration)\ncorrects = (preds>0.0).float() == train_y\ncorrects.float().mean().item()\n\n# define loss\ndef mnist_loss(preds, targets):\n    preds = preds.sigmoid() #squash b/w 0 and 1\n    return torch.where(targets==1, 1-preds, preds).mean() # average distance loss\n\nCreate a dataloader\nYou want to load your data in batches, so you will want to create a dataloader. Recall that in pytorch, a Dataset is required to return a tuple of (x,y) when indexed, which is quite easy to do:\n# define a data loader using `dset`\ndset = list(zip(train_x,train_y))\nPytorch offers a utility to then create a Dataloader from a dataset, but Jeremy basically rolled his own (w/same api):\ndl = DataLoader(dset, batch_size=256)\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\n\nThe Training Loop\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_() #updates in place\n\n### Calculate metrics\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n# Train model\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n# Train model w/epochs\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n\n\nUsing Pytorch\nBlueprint: 1. Define a dataset and then a dataloader 2. Create a model, which will have parameters 3. Create an optimizer, that: - Updates the params: params.data -= parmas.grad.data * lr - Zeros out the gradients: setting params.grad = None or zeroing out the gradients with params.grad.zero_() 4. Generate the predictions 5. Calculate the loss 6. Calculate the gradients loss.backward() 7. Using the optimizer, update the weights step and zero out the gradients zero_grad 8. Put 4-7 in a loop.\nCreate an optimizer and use nn.Linear\nlinear_model = nn.Linear(28*28,1)\nw,b = linear_model.parameters()\n\n# Define an optimizer\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nopt = BasicOptim(linear_model.parameters(), lr)\n# alternative, fastai provides SGD\nopt = SGD(linear_model.parameters(), lr)\n\n# Define Metrics\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\n# Helper to calculate metrics on validation set\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\ntrain_model(linear_model, 20)\n\nUsing fastai\nWe can substitute the above with learner.fit from fastai We just have to supply the following:\n\nDataloaders\nModel\nOptimization function\nLoss function\nMetrics\n\ndls = DataLoaders(dl, valid_dl)\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, \n                loss_func=mnist_loss,\n                metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\nWhat if you used the full power of fastai? It would look like this:\ndls = ImageDataLoaders.from_folder(path)\n# Lots of things have defaults like optimization func\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, \n                     metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)"
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html#simple-neural-nets",
    "href": "notes/fastai/01_fundamentals.html#simple-neural-nets",
    "title": "Fundamentals",
    "section": "Simple Neural Nets",
    "text": "Simple Neural Nets\nThe next step is to introduce a non-linearity\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n# Construct the learner as before\nlearn = learner(dls, simple_net, opt_func=SGD,\n               loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearner.fit(40, 0.1)\n\nInspecting Training History\nThe training history is saved in learn.recorder. You can plot your training progress with:\nplt.plot(learn.recorder.values).itemgot(2)"
  },
  {
    "objectID": "notes/fastai/batch_predicitions.html",
    "href": "notes/fastai/batch_predicitions.html",
    "title": "Batch Predictions",
    "section": "",
    "text": "How to make batch predictions in fastai\nMaking batch predictions on new data is not provided ‚Äúout of the box‚Äù in fastai. This is how you can achieve that:\nAdd this method to learner:\n@patch\ndef predict_batch(self:Learner, item, rm_type_tfms=None, with_input=False):\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n    i = getattr(self.dls, 'n_inp', -1)\n    inp = (inp,) if i==1 else tuplify(inp)\n    dec_inp, nm = zip(*self.dls.decode_batch(inp + tuplify(dec_preds)))\n    res = preds,nm,dec_preds\n    if with_input: res = (dec_inp,) + res\n    return res\nYou can then use this method like so:\n>>> from fastai.text.all import *\n>>> from predict_batch import predict_batch # this file.  If you don't import just define in your script.\n>>> dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n>>> learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n>>> learn.fine_tune(4, 1e-2)\n>>> learn.predict_batch([\"hello world\"]*4)\n(TensorText([[0.0029, 0.9971],\n         [0.0029, 0.9971],\n         [0.0029, 0.9971],\n         [0.0029, 0.9971]]),\n ('pos', 'pos', 'pos', 'pos'),\n TensorText([1, 1, 1, 1]))\nAlternatively, you can just patch the predict function so it works on batches:\n@patch\ndef predict(self:Learner, item, rm_type_tfms=None, with_input=False):\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n    i = getattr(self.dls, 'n_inp', -1)\n    inp = (inp,) if i==1 else tuplify(inp)\n    dec = self.dls.decode_batch(inp + tuplify(dec_preds))\n    dec_inp,dec_targ = (tuple(map(detuplify, d)) for d in zip(*dec.map(lambda x: (x[:i], x[i:]))))\n    res = dec_targ,dec_preds,preds\n    if with_input: res = (dec_inp,) + res\n    return res\nOther notes h/t zach:\nlearn.dls.vocab or learn.dls.categorize.vocab is another way to get the class names."
  },
  {
    "objectID": "notes/fastai/02_cv.html#data-prep",
    "href": "notes/fastai/02_cv.html#data-prep",
    "title": "Image Classification",
    "section": "Data Prep",
    "text": "Data Prep\nRemember, Datablock helps create DataLoaders.\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\nDebugging\nYou can debug the Datablock by calling .summary(), which will show you if you have any errors.\npets.summary(path/\"images\")\nIf everything looks good, you can use the DataBlock to create a DataLoaders instance:\ndls = pets.dataloaders(path/\"images\")\nOnce you have a DataLoaders instance, it is a good idea to call show_batch to spot check that things look reasonable:\nYou can debug this by using show_batch:\n>>> dls.show_batch(nrows=1, ncols=3)\n... [shows images]\nFinally, you can see what a batch looks like by calling dls.one_batch()\nx,y = dls.one_batch()\nYou always want to train a model ASAP as your final debugging step. If you wait too long, you will not discover problems\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n\nExample of an error in data prep\nA common error is forgetting to use Resize in your DataBlock as an item transform. For example, the below code will cause an error:\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 #forgot to pass `item_tfms=Resize(...),`\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")\nThis will complain that it is not able to collate the images because they are of different sizes."
  },
  {
    "objectID": "notes/fastai/02_cv.html#interpretation",
    "href": "notes/fastai/02_cv.html#interpretation",
    "title": "Image Classification",
    "section": "Interpretation",
    "text": "Interpretation\nYou can get diagnostics for your model using this:\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\nWhich will return a confusion matrix. You can see the ‚Äúmost confused‚Äù items by doing this:\n>>> interp.most_confused(min_val=5)\n[('Bengal', 'Egyptian_Mau', 10),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 8),\n ('Ragdoll', 'Birman', 7),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 6),\n ('american_pit_bull_terrier', 'american_bulldog', 5)]"
  },
  {
    "objectID": "notes/fastai/02_cv.html#improving-the-model",
    "href": "notes/fastai/02_cv.html#improving-the-model",
    "title": "Image Classification",
    "section": "Improving the model",
    "text": "Improving the model\n\nLearning Rate Finder\nStart with a very, very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then we do another mini-batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\nThe last point where the loss was clearly decreasing\n\nThe learning rate finder computes those points and more on the curve to help you. Additional learning rate suggestion algorithms can be passed into the function, by default only the valley paradigm is used. The learning rate finder can be called with learn.lr_find:\n>>> learn = cnn_learner(dls, resnet34, metrics=error_rate)\n>>> lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\nThe default valley hueristic works just fine. Note, you will want to re-run this anytime you change your model such as unfreeze layers. You might want to run this periodically if you are checkpointing during training.\n\n\nFine Tuning models\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:\n\nTrains the randomly added layers for one epoch, with all other layers frozen\nUnfreezes all of the layers, and trains them all for the number of epochs requested\n\nAlthough this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The fine_tune method has a number of parameters you can use to change its behavior, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior.\nfit_one_cycle is the suggested way to train models without using fine_tune. We‚Äôll see why later in the book; in short, what fit_one_cycle does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3) # train the head\nlearn.unfreeze() # unfreeze everything\nlearn.lr_find() # find new lr after unfreezing\nlearn.fit_one_cycle(6, lr_max=1e-5) #fine tune it all\n\n\nDiscriminative Learning Rates\nOne important aspect of fine tuning is discriminative learning rates: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers).\nfastai lets you pass a Python slice object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let‚Äôs use this approach to replicate the previous training, but this time we‚Äôll only set the lowest layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let‚Äôs train for a while and see what happens:\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\nWe can accomplish everything we did above by calling fine_tune instead. fine_tune will automatically apply discriminative learning rates for you:\n>>> learn.fine_tune??\nSignature:\nlearn.fine_tune(\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n)\nSource:   \n@patch\n@delegates(Learner.fit_one_cycle)\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr /= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nFile:      ~/anaconda3/lib/python3.9/site-packages/fastai/callback/schedule.py\nType:      method"
  },
  {
    "objectID": "notes/fastai/02_cv.html#mixed-precision-training",
    "href": "notes/fastai/02_cv.html#mixed-precision-training",
    "title": "Image Classification",
    "section": "Mixed Precision Training",
    "text": "Mixed Precision Training\nYou can achieve mixed precision training to speed up training and give you more memory headroom for bigger models with to_fp16()\nfrom fastai.callback.fp16 import *\nlearn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\nNote how you can use the freeze_epochs parameter to keep the base frozen for longer."
  },
  {
    "objectID": "notes/fastai/02_cv.html#datablock-api-multi-label-data",
    "href": "notes/fastai/02_cv.html#datablock-api-multi-label-data",
    "title": "Image Classification",
    "section": "DataBlock API: Multi-Label Data",
    "text": "DataBlock API: Multi-Label Data\nLet‚Äôs say you have a Dataframe with filenames and multiple labels per filename. The best way to get started in to use the DataBlock api to construct Datasets and DataLoaders. A review of terminology:\n\nDataset: collection that returns a tuple of (x,y) for single item. Can do this with list(zip(x,y))\nDataLoader: an iterator that provides a stream of minibatches of (x,y) instead of a single item.\nDatasets: object that contains a training Dataset and a Validation dataset.\nDataLoaders: object that contains a training DataLoader and a validation DataLoader.\n\n\nCreating Datsets\nYou can use a DataBlock:\n>>> from fastbook import *\n>>> from fastai.vision.all import *\n\n>>> path = untar_data(URLs.PASCAL_2007)\n>>> df = pd.read_csv(path/'train.csv')\n>>> def get_x(r): return path/'train'/r['fname']\n>>> def get_y(r): return r['labels'].split(' ')\n\n>>> dblock = DataBlock(get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(Path('/home/hamel/.fastai/data/pascal_2007/train/006162.jpg'), ['aeroplane'])\nNext we need to convert our images into tensors. We can do this by using the ImageBlock and MultiCategoryBlock:\n\n\nUsing Blocks For Transforms\n>>> dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                       get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\nInspecting Vocabulary\nYou can inspect the vocabulary with the vocab attribute:\ndsets.train.vocab\n\n\nUsing a splitter\nThe dataframe has a column called is_valid, we can use that do a train validation split. By default, the DataBlock uses a RandomSplitter. By default, RandomSplitter uses 20% of the data for the validation set.\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\n\n\nCreating DataLoaders\nDataLoaders build upon Datasets by streaming mini-batches instead of one example at a time. One prerequisite to making DataLoaders is that all the images are the same size. To do this you can use RandomResizedCrop:\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\nWhen you are done with this, you want to debug things by calling show_batch:\ndls.show_batch(nrows=1, ncols=3)"
  },
  {
    "objectID": "notes/fastai/02_cv.html#multi-label-model",
    "href": "notes/fastai/02_cv.html#multi-label-model",
    "title": "Image Classification",
    "section": "Multi-Label Model",
    "text": "Multi-Label Model\nYou can create a learner like so:\nlearn = cnn_learner(dls, resnet18)\nOne useful thing is to debug / verify that the output shape conforms to what you are expecting. You can do this by running a tensor through your model and inspecting it‚Äôs output:\nx,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\n\nThis is what you would use to extract embeddings / activations out of another model\n\nIt‚Äôs a good idea to see what the activations look like:\n>>> activs[0]\nTensorBase([ 2.0858,  2.8195,  0.0460,  1.7563,  3.3371,  2.4251,  2.3295, -2.8101,  3.3967, -3.2120,  3.3452, -2.3762, -0.3137, -4.6004,  0.7441, -2.6875,  0.0873, -0.2247, -3.1242,  3.6477],\n       grad_fn=<AliasBackward0>)\nWe can see these are not b/w 0 and 1, because the sigmoid has not been applied yet.\n\nLoss Functions\nPyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names!\nF.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross-entropy on a one-hot-encoded target, but do not include the initial sigmoid. Normally for one-hot-encoded targets you‚Äôll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example.\nThe equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax.\nSince we have a one-hot-encoded target, we will use BCEWithLogitsLoss:\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nWe don‚Äôt actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default.\n\n\nMetrics\nWe need to make sure we have a metric that works for multi-label classfication:\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\nWe can use partial to set the parameters we want in the metrics function and pass it like this:\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\nYou can change your metrics anytime and recalculate things. validate() will return the validation loss and metrics.\n>>> learn.metrics = partial(accuracy_multi, thresh=0.1)\n>>> learn.validate() # returns validation loss and metrics\n(#2) [0.10417556017637253,0.9376891851425171]\nYou can debug metrics by getting the predictions on the validation set with get_preds:\npreds,targs = learn.get_preds()\nassert preds.shape[0] == dls.valid.n\nOnce you have the predictions, you can run the metric function seperately:\naccuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\n\n\nChoosing A Prediction Threshold\nxs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);\n\nIn this case, we‚Äôre using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be overfitting to the validation set, since we‚Äôre trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we‚Äôre clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don‚Äôt try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it‚Äôs fine to do this)."
  },
  {
    "objectID": "notes/fastai/02_cv.html#image-regression",
    "href": "notes/fastai/02_cv.html#image-regression",
    "title": "Image Classification",
    "section": "Image Regression",
    "text": "Image Regression\nYes, X is images and y are floats. Ex: key point model -> predicting location of something like the center of someone‚Äôs face.\n\nGet Data\nFirst step is to get data with get_image_files\n# view the data and it's structure\npath = untar_data(URLs.BIWI_HEAD_POSE)\nPath.BASE_PATH = path\npath.ls().sorted()\n(path/'01').ls().sorted()\n\n# next get all the data systematically\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\nIt is a good idea to see what you are working with as a general rule.\n\nYou can inspect images with the following code\nim = PILImage.create(img_files[0])\nim.to_thumb(160)\n\nDefine the functions to extract the data you need from the files. You can ignore what this does and treat it as a helper function, b/c your problem is likely to be specific.\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nDefine the DataBlock\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)),  \n)\nNote the splitter function: we want to ensure that our model can generalize to people that it hasn‚Äôt seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person‚Äôs images.\n\nPoints and Data Augmentation: We‚Äôre not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you‚Äôre working with another library, you may need to disable data augmentation for these kinds of problems.\n\nThe only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images\n\n\nDebug the DataBlock\nUsing showbatch:\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\nInspect the shape:\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n\n\nTrain The Model\nAs usual, we can use cnn_learner to create our Learner. Remember way back in <> how we used y_range to tell fastai the range of our targets? We‚Äôll do the same here - coordinates in fastai and PyTorch are always rescaled between -1 and +1 by the PointBlock, which is why you pass (-1, 1) to y_range.\n\nSetting y_range\n# Always use y_range when predicting a continous target\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\nWe didn‚Äôt specify a loss function, which means we‚Äôre getting whatever fastai chooses as the default.\n>>> dls.loss_func\nFlattenedLoss of MSELoss()\nNote also that we didn‚Äôt specify any metrics. That‚Äôs because the MSE is already a useful metric for this task (although it‚Äôs probably more interpretable after we take the square root).\n\nYou should always set y_range when predicting continuous targets. y_range is implemented in fastai using sigmoid_range, which is defined as:\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\nThis is set as the final layer of the model, if y_range is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range (lo,hi).\nHere‚Äôs what it looks like:\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n\n\n\n\nFind the learning rate and then train\nlearn.lr_find()\nlr = 1e-2\nlearn.fine_tune(3, lr)\n\n\nInspect the Results\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))"
  },
  {
    "objectID": "notes/fastai/02_cv.html#loss-functions-1",
    "href": "notes/fastai/02_cv.html#loss-functions-1",
    "title": "Image Classification",
    "section": "Loss Functions",
    "text": "Loss Functions\nfastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want:\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "notes/linux/osx.html",
    "href": "notes/linux/osx.html",
    "title": "OSX Shell Tips",
    "section": "",
    "text": "Key Repeat Rate\nAdd days to your lifespan by Increasing the key repeat rate. Run the following in the terminal then restart. Protip by Michael Musson.\ndefaults write -g InitialKeyRepeat -int 13 # normal minimum is 15 (225 ms)\ndefaults write -g KeyRepeat -int 1 # normal minimum is 2 (30 ms)\n\n\nA better way to search text: ack\nInstall ack:\nbrew install ack\nSearch files for text, super fast and returns results in a very nice way. By default will search recursively from the current directory and it skips unimportant files by default.\nack \"search string\"\n\n\nKeyboard Tricks (OS X)\nSet your option key to Esc+ in iTerm under Profiles>Keys\n\n\ncontrol-W delete word backwards\noption-D delete word forwards\ncontrol-K delete until end of line\n\n\n\nMy .zshrc file\nStored at ~/.zshrc\nI used to have ohmyzsh but it made my shell too slow. This is good enough for me.\n# #speed startup time https://medium.com/@dannysmith/little-thing-2-speeding-up-zsh-f1860390f92\nautoload -Uz compinit\nfor dump in ~/.zcompdump(N.mh+24); do\n  compinit\ndone\ncompinit -C\n####\n\nPROMPT='%(?.%F{green}‚àö.%F{red}?%?)%f %B%F{157}%1~%f%b %F{231}%# '\n\nautoload -Uz vcs_info\nprecmd_vcs_info() { vcs_info }\nprecmd_functions+=( precmd_vcs_info )\nsetopt prompt_subst\nRPROMPT=\\$vcs_info_msg_0_\nzstyle ':vcs_info:git:*' formats '%F{141}(%b)%r%f'\nzstyle ':vcs_info:*' enable git\n\nalias ls=\"colorls\"\nalias python=\"python3\"\n\n# install jupyter kernel with pipenv\nfunction install-jupyter {\n  if [ -n \"${PIPENV_ACTIVE+1}\" ]; then\n    VENV_NAME=`echo ${VIRTUAL_ENV} | cut -d '/' -f 7`\n    echo \"creating Jupyter kernel named $VENV_NAME\"\n    pipenv install --skip-lock ipykernel\n    python -m ipykernel install --user --name=$VENV_NAME\n  fi\n}\n\n## automatically activate pipenv shell upon cd\nfunction auto_pipenv_shell {\n    if [ ! -n \"${PIPENV_ACTIVE+1}\" ]; then\n        if [ -f \"Pipfile\" ] ; then\n            pipenv shell\n        fi\n    fi\n}\n\nfunction cd {\n    builtin cd \"$@\"\n    auto_pipenv_shell\n}\n\n#extra stuff\nexport CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\nGREP_OPTIONS=\"--color=always\";export GREP_OPTIONS\n__git_files () { \n    _wanted files expl 'local files' _files     \n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html",
    "href": "notes/linux/bash_scripting.html",
    "title": "Cheatsheet",
    "section": "",
    "text": "Link to class.\nLink to GitHub repo\n\n;\n\n\n\nwas originally a program called bin/sh\nBourne Shell: introduced more advanced structure into the shell.\n\nBourne Again Shell (Bash): Second iteration of Bourne Shell.\n\n\n\n\nls -a ~/ | grep bash\n    Ôíâ  .bash_history\n    Ôíâ  .bash_profile\n    ÔÖõ  .bash_profile.backup\n    ÔÖõ  .bash_profile.bensherman\n    ÔÖõ  .bash_profile_copy\n    ÔÑï  .bash_sessions/\n    Ôíâ  git-completion.bash\n\n\n\n.bash_profile: executed when you login -> configures the shell when you get an initial command prompt. This is different than .bashrc.\n\ncommonly loads the ~/.bashrc file as well.\nbin is traditionally the folder for binaries.\nbash_profile is designed to run when you login, so if you change it will not refresh until you login next time.\n\n\n\n\n\n.bashrc it is executed simply before the command shell comes up, does not have to wait until you login.\netc/bashrc are system bashrc files which is like a ‚Äútemplate‚Äù for user bashrc files. Anytime a new user is created, it inherits from this template and sometimes automated customizations are applied. This is usually done by simply importing etc/bashrc from each user‚Äôs bashrc file.\nenv will list all env variables.\nto apply .bashrc you just have to run the command bash as it will start another shell from your current one. However, if you run bash you can now exit without closing the shell, because a shell is running inside another shell.\n\n\n\n\n\n~/.bash_history contains lots of history. By default will only capture last 100 but you can change this setting.\n\nyou can exlude something from saving to history (like passwords) by using an ignorespace\nthe environment variable HIST_CONTROL can be used to control how much history to keep and settings about what should not be logged. One way to turn off loggin is: bash     export HISTCONTROL=$HISTCONTROL:ignorespace this allow you to skip logging by adding a space to the the beginning of any command. If you want to see what is in HIST_CONTROL you will see:\n> cat ~/.bash_history | grep HISTCONTROL\nHISTCONTROL=ignoredups:ignorespace\nignoredups was already set to this variable.\n\n\n\n\n\nDoesn‚Äôt always exist on a system. in most cases the contents of the ~/.bash_logout will be empty or contain a comment.\nThe role of this file is to execute things when you exit the shell. If you close the shell it will not work, you have to do a clean exit instead.\nCommon use is to use this to clear out ~/.bashrc with the original to clear out any changes the user may have made. You can accomplish this by copying a backup:\ncp ~/.bashrc.original ~/.bashrc\n\n\n\n\n\nPut your shell scripts in a folder you can find them. We can put them in ~/bin:\n> mkdir bin\nMake sure in ~/.bash_profile you have:\nPATH=$PATH:$HOME/bin\nexport PATH\n\n\n\nTo make test.sh executable run command chmod u+x test.sh\n\nYou can also run chmod 755\n\n\n\n\ncan use any name that is not an environment variable (check with env).\nby convention variable names in ALLCAPS. bash     > FIRSTNAME=\"Hamel\"\n  - No space b/w = and value.\n  - Good idea to __always__ put value in double quotes `\"`, although this is not required in every case.  \nAs a practice you want to use export command to set is as an environment variable. This makes the variable available to any subprocess that starts from the shell. Read more about this here.\n> export FIRSTNAME\n> echo \"Hello, $FIRSTNAME\"\n\"Hello Hamel\"\n\n> export FIRSTNAME=\"Hamel\" # do this in one step\nThe above example could work without export, too just reinforcing that its a good idea to use this as a habit. You can do this in one step:\n\n\n\n> export TODAYSDATE=`date`  # executes date command\n\n\n\n\n```bash\nMYUSERNAME='hamel'\nMYPASSWORD='password'\nSTARTOFSCRIPT=`date`\n\necho \"My login name for this app is $MYUSERNAME\"\necho \"My login password for this app is $MYPASSWORD\"\necho \"I started this script at $STARTOFSCRIPT\"\n\nENDOFSCRIPT=`date`\n\necho \"I ended the script at $ENDOFSCRIPT\"\n```\n\nThese variables only live within the sub-shell that executes the script.\n\n\n\n\n\nMethod 1 (Static): Assign command result to variable. Only runs the command at time of variable assignment.\n\n    TODAYSDATE=`date`\n    USERFILES=`find /home -user user` # find all directories owned by the user \"user\"\n\n    echo \"Today's Date: $TODAYSDATE\"\n    echo \"All files owned by USER: $USERFILES\"\n\nMethod 2: Use an alias, which allows you to run a command every time you call the alias. For aliases to work this way you must use the shopt command, which allows aliases to be useable in shell scripts. Technically referred to as ‚Äúexpanding aliases within a subshell‚Äù.\n\n    #!/bin/bash\n    shopt -s expand_aliases\n\n    # notice that we don't use backticks here because the command we want to execute is put in \"..\"\n    alias TODAY=\"date\" \n    alias UFILES=\"find /home -user user\"\n\n\n    A=`TODAY` #Executes the command date\n    B=`UFILES`#Executes the command \n    echo \"With Alias, TODAY is: $A\" echo \"With Alias, UFILES is: $B\"\n\n\n\n\nValue = 0 means everything is ok\nValue != 0 means something is wrong.\nSee last exit status w/ the $? command:\n\n    > ls\n    > echo $?\n    0\n\n\n\nUnlike python, shell scripts will continue executing even if there is an error. You can prevent this by using set -e\n\n    set -e # means exit the shell if there is an error, don't continue.\n\n\n\n\n    expr 1 + 2\n    expr 2 \\* 2 # you have to escape the *\n    expr \\( 2 + 2 \\) \\* 4  # you must also escape the ( )\n\nCaveat: You need a space on each side of the operator.\n\n\n\n\n\n\nenv and printenv will tell you your global vars\nset will give you things from your session. This will also usually contain everything from your global scope. set is a superset of env.\nReserved names: see study guide or google it.\n\n\n\nunset MY_VAR\n\n\n\n\n\n$ escapes a single character.\nsingle quotes '..' treats something as a string, escapes the whole thing\ndouble quotes do not escape anything.\n\n> echo \"\\$COL\"  # this will escape the $\n$COL\n\n> echo '$COL' # single quotes escape things, means the literal string\n$COL\n\n> echo \"$COL\" # does not escape anything\n250\n\n> echo \"The date is: `date`\" # command substitution with bacticks\nThe date is Mon Jul 25"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#using-devnull",
    "href": "notes/linux/bash_scripting.html#using-devnull",
    "title": "Cheatsheet",
    "section": "Using dev/null",
    "text": "Using dev/null\nUse dev/null when you want to discard output and don‚Äôt want to put in the background. /dev/null is a device, and like everything is a file in linux. Everything you write to dev/null just dissapears.\nFor example:\n#!/bin/bash\n#redirect to dev/null example\n\necho \"This is going to the blackhole.\" >> /dev/null\nNote >> (append) or > (overwrite) will work for dev/null, although out of habit in other scenarios it is better to append when unsure using >>."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#redirect-std-error",
    "href": "notes/linux/bash_scripting.html#redirect-std-error",
    "title": "Cheatsheet",
    "section": "Redirect Std Error",
    "text": "Redirect Std Error\nls -l /bin/usr 2> ls-error.txt"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#redirect-std-out-err-into-one-file",
    "href": "notes/linux/bash_scripting.html#redirect-std-out-err-into-one-file",
    "title": "Cheatsheet",
    "section": "Redirect Std Out & Err into one file",
    "text": "Redirect Std Out & Err into one file\nls  -l /bin/sur > ls-output.txt 2>&1\nShortcut: use &\nls  -l /bin/sur &> ls-output.txt"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#dispose-std-err-output-devnull",
    "href": "notes/linux/bash_scripting.html#dispose-std-err-output-devnull",
    "title": "Cheatsheet",
    "section": "Dispose Std Err output /dev/null",
    "text": "Dispose Std Err output /dev/null\nls -l /bin/sur 2> /dev/null"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#brace-expansion",
    "href": "notes/linux/bash_scripting.html#brace-expansion",
    "title": "Cheatsheet",
    "section": "Brace Expansion",
    "text": "Brace Expansion\n> echo Hello-{Foo,Bar,Baz}-World                             \nHello-Foo-World Hello-Bar-World Hello-Baz-World"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#parameter-expansion-like-coalesce",
    "href": "notes/linux/bash_scripting.html#parameter-expansion-like-coalesce",
    "title": "Cheatsheet",
    "section": "Parameter Expansion, Like Coalesce",
    "text": "Parameter Expansion, Like Coalesce\n{parameter:-word}\nIf parameter is unset (i.e., does not exist) or is empty, this expansion results in the value of word. If parameter is not empty, the expansion results in the value of parameter."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#types-of-variables-1",
    "href": "notes/linux/bash_scripting.html#types-of-variables-1",
    "title": "Cheatsheet",
    "section": "Types of Variables",
    "text": "Types of Variables\n# declare int variable:\n> declare -i NEWVAR=10\n\n# inpsect type of NEWVAR\n> declare -p NEWVAR\ndeclare -i NEWVAR=\"10\"\n\n# declare readonly variable\n> declare -r READONLY=\"This is something we cannot overwrite\"\n\n# try to cancel READONLY type\n> declare +r READONLY\n### will result in an error\nVariables in bash are implicitly typed, the type will be inferred from the value you assign.\n\ndetermine the type of a variable: declare -p $MYVAR\ndeclare variable as integer: bash      declare -i NEWVAR=10\nIf you explicitly declare a variable as an int but assign it to a string, it will implicitly convert the value to 0."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#the-if-statement",
    "href": "notes/linux/bash_scripting.html#the-if-statement",
    "title": "Cheatsheet",
    "section": "The if statement",
    "text": "The if statement\n3\necho ‚ÄúGuess the Secret Number‚Äù\necho ‚Äú======================‚Äú\necho ‚Äú‚Äù\necho ‚ÄúEnter a Number Between 1 and 5‚Äù\nread GUESS\n\n\nif [ $GUESS -eq 3 ]\n    then\n        echo ‚ÄúYou guessed the Correct Number!‚Äù\nfi\nTest if a file exists\nFILENAME=$1\necho ‚ÄúTesting for the existence of a file called $FILENAME‚Äù\n\nif [ -a $FILENAME ]\n    then\n        echo ‚Äú$FILENAME does exist!‚Äù\nfi\n\n# negation operator \nif [! -a $FILENAME ]\n    then\n        echo ‚Äú$FILENAME does not exist!‚Äù\nfi\n\n# test multiple expressions in if statement\n\nif [ -f $FILENAME ] && [ -R $FILENAME]\n    then\n        echo ‚ÄúFile $FILENAME exists and is readable.‚Äù\nfi\n-a is the same as -f w.r.t. testing for the existence of a file."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#ifthenelse",
    "href": "notes/linux/bash_scripting.html#ifthenelse",
    "title": "Cheatsheet",
    "section": "If/Then/Else",
    "text": "If/Then/Else\necho ‚ÄúEnter a number between 1 and 3:‚Äù\nread VALUE\n\n# use semicolons for readability\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ]; then\n    echo ‚ÄúYou entered $VALUE‚Äù\nfi\nUsing an OR statement:\n# another variation\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ] || [ ‚Äú$VALUE‚Äù -eq ‚Äú2‚Äù ] || [ ‚Äú$VALUE‚Äù -eq ‚Äú3‚Äù ]; then\n    echo ‚ÄúYou entered $VALUE‚Äù\nelse\n    echo ‚ÄúYou didn‚Äôt follow directions!‚Äù\nfi\nRedirect errors to /dev/null\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ] 2>/dev/null || [ ‚Äú$VALUE‚Äù -eq ‚Äú2‚Äù ] 2>/dev/null || [ ‚Äú$VALUE‚Äù -eq ‚Äú3‚Äù ] 2>/dev/null; then\n    echo ‚ÄúYou entered $VALUE‚Äù\nelse\n    echo ‚ÄúYou didn‚Äôt follow directions!‚Äù\nfi\n\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ] 2>/dev/null; then\n    echo ‚ÄúYou entered #1‚Äù\nelif ‚Äú \"$VAL‚ÄùE\" -e‚Äú ‚Äù2\" ] 2>/dev/null; then\n    ech‚Äú \"You entered ‚Äù2\"\nelif ‚Äú \"$VAL‚ÄùE\" -e‚Äú ‚Äù3\" ] 2>/dev/null; then\n    ech‚Äú \"You entered ‚Äù3\"\nelse\n    ech‚Äú \"You di‚Äôn't follow direction‚Äù!\"\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#file-expressions",
    "href": "notes/linux/bash_scripting.html#file-expressions",
    "title": "Cheatsheet",
    "section": "File Expressions",
    "text": "File Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#string-expressions",
    "href": "notes/linux/bash_scripting.html#string-expressions",
    "title": "Cheatsheet",
    "section": "String Expressions",
    "text": "String Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#integer-expressions",
    "href": "notes/linux/bash_scripting.html#integer-expressions",
    "title": "Cheatsheet",
    "section": "Integer Expressions",
    "text": "Integer Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#for-loop",
    "href": "notes/linux/bash_scripting.html#for-loop",
    "title": "Cheatsheet",
    "section": "For Loop",
    "text": "For Loop\n#!/bin/bash\necho ‚ÄúList all the shell scripts contents of the directory‚Äù\nSHELLSCRIPTS=`ls *.sh`\n\n# alternate using for loop\n\nfor FILE in *.sh; do\n    echo ‚Äú$FILE‚Äù\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#case-statement",
    "href": "notes/linux/bash_scripting.html#case-statement",
    "title": "Cheatsheet",
    "section": "Case Statement",
    "text": "Case Statement\n#!/bin/bash\n\necho ‚Äú1) Choice 2‚Äù\necho ‚Äú2) Choice 2‚Äù\necho ‚Äú3) Choice 3‚Äù\necho ‚ÄúEnter Choice:‚Äù\n\nread MENUCHOICE\n\ncase $MENUCHOICE in\n    1)\n        echo ‚ÄúYou have choosen the first option‚Äù;;\n    2)\n        echo ‚ÄúYou have chosen the second option‚Äù;;\n    3) \n        echo ‚ÄúYou have selected the third option‚Äù;;\n    *)\n        echo ‚ÄúYou have choosen unwisely‚Äù;;\n\nMatch Multiple Case Statements\nAllow many matches to occur"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#while-loop",
    "href": "notes/linux/bash_scripting.html#while-loop",
    "title": "Cheatsheet",
    "section": "While Loop",
    "text": "While Loop\n#!/bin/bash\n\necho ‚ÄúEnter number of times to display message:‚Äù\nread NUM\n\nCOUNT=1\n\n# -le means less than or equal to\nwhile [ $COUNT -le $NUM ]\ndo\n    echo ‚ÄúHello World $COUNT‚Äù\n    COUNT=‚Äú`expr $COUNT + 1`‚Äù\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#asynchronous-execution-with-wait",
    "href": "notes/linux/bash_scripting.html#asynchronous-execution-with-wait",
    "title": "Cheatsheet",
    "section": "Asynchronous Execution with wait",
    "text": "Asynchronous Execution with wait\n\nThis is the most straightforward implementation of async I have ever seen. You basically decide when to block and wait for a process that you previously decided to run in a child process."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#short-circuit-expressions",
    "href": "notes/linux/bash_scripting.html#short-circuit-expressions",
    "title": "Cheatsheet",
    "section": "Short Circuit Expressions",
    "text": "Short Circuit Expressions\n\n&&: command1 && command2:\nonly run command2 if command1 is successful\n\n\n||: command1 || command2:\nonly run command2 if command1 fails"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#reading-files",
    "href": "notes/linux/bash_scripting.html#reading-files",
    "title": "Cheatsheet",
    "section": "Reading Files",
    "text": "Reading Files\necho ‚ÄúEnter a filename‚Äù \nread FILE\n\nwhile read -r SUPERHERO; do\n    echo ‚ÄúSuperhero Name: $SUPERHERO‚Äù\ndone < ‚Äú$FILE‚Äù"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#reading-files-with-loops",
    "href": "notes/linux/bash_scripting.html#reading-files-with-loops",
    "title": "Cheatsheet",
    "section": "Reading Files with loops",
    "text": "Reading Files with loops"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#file-descriptors",
    "href": "notes/linux/bash_scripting.html#file-descriptors",
    "title": "Cheatsheet",
    "section": "File Descriptors",
    "text": "File Descriptors\nUse a number >= 3 for file descriptors.\n0 - stdin 1 - stdout 2 - stderr\n/dev/null -> generic place where you can redirect streams into nothing.\n#!/bin/bash\n\necho ‚ÄúEnter file name: ‚Äú\nread FILE\n\n# < means readonly,  > means write only,  <> means allow read & write\n# assign file descriptor to filename\nexec 5<>$FILE\n\nwhile read -r SUPERHERO; do\n    echo ‚ÄúSuperhero Name: $SUPERHERO‚Äù\ndone <&5 #use & to reference the file descriptor\n\n# append to end of file.\necho \"File Was Read On: `date`\" >&5\n\n# close file descriptor\nexec 5>&-"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#delimiters-ifs",
    "href": "notes/linux/bash_scripting.html#delimiters-ifs",
    "title": "Cheatsheet",
    "section": "Delimiters (IFS)",
    "text": "Delimiters (IFS)\nIFS - Internal Field Seperator Default is a space\n# this will return a space\necho $IFS\necho \"Enter filename to parse: \"\nread FILE # spacedelim.txt\n\n# https://stackoverflow.com/questions/24337385/bash-preserve-string-with-spaces-input-on-command-line\n\nwhile read -r CPU MEM DISK; do\n    echo \"CPU: $CPU\"\n    echo \"Memory: $MEM\"\n    echo \"Disk: $DISK\"\ndone <\"$FILE\""
  },
  {
    "objectID": "notes/linux/bash_scripting.html#traps-and-signals",
    "href": "notes/linux/bash_scripting.html#traps-and-signals",
    "title": "Cheatsheet",
    "section": "Traps and Signals",
    "text": "Traps and Signals\nhttps://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html - cntrl+c = SIGINT - cntrl+z = SIGTSTP - kill command (without -9 flag) = SIGTERM - kill -9 = SIGKILL; this signal is not sent to the process, it is just killed.\nclear\n\n# first argument is what to exexute \ntrap 'echo \" - Please Press Q to Exit.\"' SIGINT SIGTERM SIGTSTP\n\n# cntrl+c = SIGINT\n# cntrl+z = SIGTSTP  (Suspend, send to background)\n\n\n\nwhile [ \"$CHOICE\" != \"Q\" ] && [ \"$CHOICE\" != \"q\" ]; do\n    echo \"Main Menu\"\n    echo \"=======\"\n    echo \"1) Choice One\"\n    echo \"2) Choice Two\"\n    echo \"3) Choice Three\"\n    echo \"Q) Quit\"\n    read CHOICE\n\n    clear\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#structure-of-functions-in-a-shell-script",
    "href": "notes/linux/bash_scripting.html#structure-of-functions-in-a-shell-script",
    "title": "Cheatsheet",
    "section": "structure of functions in a shell script",
    "text": "structure of functions in a shell script\nUnlike python, you must define your functions before you call them."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#scope",
    "href": "notes/linux/bash_scripting.html#scope",
    "title": "Cheatsheet",
    "section": "Scope",
    "text": "Scope\nsetting a variable within a function defines that variable globally after that function is called!!!\nGLOBALVAR=‚ÄúGlobally Visible‚Äù\n\n# sample function for function variable scope\nfuncExample () {\n    # local\n    LOCALVAR=‚ÄúLocally Visible‚Äù\n\n    echo ‚ÄúFrom within the function, the variable‚Äôs value is set to $LOCALVAR ‚Ä¶‚Äù\n}\n\n# script start\n\necho ‚Äúthis happens before the function call‚Äù\necho ‚Äú‚Äù\necho ‚ÄúLocal Variable = $LOCALVAR after the function call.‚Äù\necho ‚ÄúGlobal Variable = $GLOBALVAR (before the function call).‚Äù\n\nfuncExample\n\necho ‚Äúthis happens after the function call‚Äù\necho ‚ÄúLocal Variable = $LOCALVAR after the function call.‚Äù\necho ‚ÄúGlobal Variable = $GLOBALVAR (before the function call).‚Äù\nOutput of above code:\nÓÇ∞ ./scope.sh\nthis happens before the function call\n\nLocal Variable =  after the function call.\nGlobal Variable = Globally Visible (before the function call).\nFrom within the function, the variable‚Äôs value is set to Locally Visible ‚Ä¶\nthis happens after the function call\nLocal Variable = Locally Visible after the function call.\nGlobal Variable = Globally Visible (before the function call)."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#functions-with-parameters",
    "href": "notes/linux/bash_scripting.html#functions-with-parameters",
    "title": "Cheatsheet",
    "section": "Functions With Parameters",
    "text": "Functions With Parameters\n# global\nUSERNAME=$1\n\nfuncAgeInDays () {\n    echo ‚ÄúHello $USERNAME, You are $1 Years old.‚Äù\n    echo ‚ÄúThat makes you approx `expr 365 \\* $1` days old‚Äù\n}\n\n#script - start\nread -r -p ‚ÄúEnter your age:‚Äù AGE\n\n# pass in arguments like this\nfuncAgeInDays $AGE"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#nested-functions",
    "href": "notes/linux/bash_scripting.html#nested-functions",
    "title": "Cheatsheet",
    "section": "Nested Functions",
    "text": "Nested Functions\nAuthor of course uses this for organization purposes. When you call a function if it has nested functions the functions defined within will be exposed to the script also.\n# global\nGENDER=$1\n\nfuncHuman () {\n    ARMS=2\n    LEGS=2\n\n    funcMale () {\n        BEARD=1\n        echo ‚ÄúThis man has $ARMS arms and $LEGS legs with $BEARD beard‚Äù\n    }\n\n    funcFemale () {\n        BEARD=0\n        echo ‚ÄúThis woman has $ARMS arms and $LEGS legs with $BEARD beard‚Äù\n    }\n}\n\n# script start\nclear\n\n# determine the actual gender and display the characteristics.\nif  [ ‚Äú$GENDER‚Äù == ‚Äúmale‚Äù ]; then\n    funcHuman\n    funcMale # this function is available after the parent function is called.\nelse\n    funcHuman\n    funcFemale\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#function-return-and-exit",
    "href": "notes/linux/bash_scripting.html#function-return-and-exit",
    "title": "Cheatsheet",
    "section": "Function Return and Exit",
    "text": "Function Return and Exit\nThis allows you to get arguments from the command line and then exit with a proper code and also use function returns inside scripts.\n# demo of return values and testing results\n\nYES=0\nNO=1\nFIRST=$1\nSECOND=$2\nTHIRD=$3\n\n# function definitions\n\nfuncCheckParams () {\n    # did we get three\n    # -z equivalent to isnull (in this case means not-null b/c of !)\n    if [ ! -z ‚Äú$THIRD‚Äù ]; then\n        echo ‚ÄúWe got three params‚Äù\n        return $YES\n    else\n        echo ‚ÄúWe did not get three params‚Äù\n        return $NO\n    fi\n}\n\n# script start\n\nfuncCheckParams\n# the return value from the function gets stored in $?\nRETURN_VALS=$?\n\nif [ ‚Äú$RETURN_VALS‚Äù -eq ‚Äú$YES‚Äù ]; then\n    echo ‚ÄúWe received three params and they are:‚Äù\n    echo ‚ÄúParam 1: $FIRST‚Äù\n    echo ‚ÄúParam 2: $SECOND‚Äù\n    echo ‚ÄúParam 3: $THIRD‚Äù\nelse\n    echo ‚ÄúUsage: funcreturn.sh [param1] [param2] [param3]‚Äù\n    exit 1\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#infobox",
    "href": "notes/linux/bash_scripting.html#infobox",
    "title": "Cheatsheet",
    "section": "Infobox",
    "text": "Infobox\nDissappears unless you sleep (see below). Does not come with any buttons.\n# globals\nINFOBOX=${INFOBOX=dialog}\nTITLE=‚ÄúDefault‚Äù\nMESSAGE=‚ÄúSomething to say‚Äù\nXCOORD=10\nYCOORD=20\n\nfuncDisplayInfoBox () {\n    $INFOBOX ‚Äîtitle ‚Äú$1‚Äù ‚Äîinfobox ‚Äú$2‚Äù ‚Äú$3‚Äù ‚Äú$4‚Äù\n    sleep ‚Äú$5‚Äù\n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#msgbox",
    "href": "notes/linux/bash_scripting.html#msgbox",
    "title": "Cheatsheet",
    "section": "Msgbox",
    "text": "Msgbox\nMsgbox - dissapears unless you sleep pass --msgbox argument, comes with default ok button and stays on screen.\n# global\nMSGBOX=${MSGBOX=dialog}\nTITLE=‚ÄúDefault‚Äù\nMESSAGE=‚ÄúSome Message‚Äù\nXCOORD=10\nYCOORD=20\n\nfuncDisplayMsgBox () {\n    $MSGBOX ‚Äîtitle ‚Äú$1‚Äù ‚Äîmsgbox ‚Äú$2‚Äù ‚Äú$3‚Äù ‚Äú$4‚Äù\n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#menus",
    "href": "notes/linux/bash_scripting.html#menus",
    "title": "Cheatsheet",
    "section": "Menus",
    "text": "Menus\nSee pdf notes/scripts"
  },
  {
    "objectID": "notes/linux/permprocdata.html",
    "href": "notes/linux/permprocdata.html",
    "title": "Processes, Permissions and Moving Data",
    "section": "",
    "text": ";"
  },
  {
    "objectID": "notes/linux/permprocdata.html#references",
    "href": "notes/linux/permprocdata.html#references",
    "title": "Processes, Permissions and Moving Data",
    "section": "References",
    "text": "References\nFiles associated with this tutorial can be found here."
  },
  {
    "objectID": "notes/linux/permprocdata.html#managing-processes-ps-kill-pkill",
    "href": "notes/linux/permprocdata.html#managing-processes-ps-kill-pkill",
    "title": "Processes, Permissions and Moving Data",
    "section": "Managing Processes (ps, kill, pkill)",
    "text": "Managing Processes (ps, kill, pkill)\n\nKill Single Process (ps, kill)\nA common scenario is that you might run a python script to train a model:\n$ python train.py\nLet‚Äôs say you want to kill this script for whatever reason. You might not always be able to type Cntrl + C to stop it, especially if this process is running in the background. (Aside: A way make a program run in the background is with a & for example:$ python train.py & )\nIn order to find this running program, you can use the command ps\n$ ps Gives you basic information (good enough most of the time)\nFlags:\n\n-e Allows you to see all running processes including from other users\n-f Allows you to see additional information about each process\n\nIn order to kill the process you will want to identify it‚Äôs PID for example, if the PID is 501 you can kill this process with the command:\n$ kill 501\n\n\nKilling Multiple Processes (pkill)\nIf you use process-based threading in python with a library like multi-processing, python will instantiate many processes for you. This is common thing to do in python for a task like data processing.\nLet‚Äôs consider the below example. When you run this in the background it will produce 8 processes:\nfrom multiprocessing import Pool\nfrom time import sleep\n\ndef f(x):\n    sleep(1000) # simulate some computation\n    return x*x\n\nif __name__ == '__main__':\n    with Pool(8) as p:\n        print(p.map(f, range(8)))\n\n$ python train_multi.py &\n\nAfter a few seconds, calling the command ps will yield something like this:\nPID TTY           TIME CMD\n 3982 ttys002    0:00.09 ...MacOS/Python train_multi.py\n 4219 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4220 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4221 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4222 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4223 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4224 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4225 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4226 ttys002    0:00.00 ...MacOS/Python train_multi.py\nYou can find all processes with the file train_multi.py with the pkill command and the -f flag:\n\n\n\nSee Parent / Child Processes (pstree)\npstree is also a helpful utility to see parent/child relationships between processes. You can install pstree on a mac with brew install pstree\nIn the above example, there are 8 sub-processes created by one python process. Running the command\n$ pstree -s train_multi.py\nWill show the process hierarchy. The -s flag allows you to filter parents and descendants of processes containing a string in their command. In the below example, PID 41592 will kill all the 8 child processes seen below\n\n\n\nKilling Process Options\n\nReminder: view processes with ps or top To show processes from all users ps aux\n\nTo restart pid 6996 kill -1 6996\nkill pid 6996 kill -9 6996\n\nYou can kill processes by name (which is also usually listed as the command that started the processes). killall will search for the string int he relevant process.\n\n\nBringing processes back into the foreground\nReminder you put processes in the background with & example is myscript.sh &\nYou can move processes back into the foreground with fg\nfg 1234 brings process 1234 back into the foreground."
  },
  {
    "objectID": "notes/linux/permprocdata.html#bundling-archiving-files-tar",
    "href": "notes/linux/permprocdata.html#bundling-archiving-files-tar",
    "title": "Processes, Permissions and Moving Data",
    "section": "Bundling & Archiving Files (tar)",
    "text": "Bundling & Archiving Files (tar)\nYou commonly want to package a bunch of files together, such as a collection of photos or CSVs, and optionally compress these with its directory structure intact. A common tool for this is tar . This is how you would bundle and compress a directory of CSV files:\n\n\nSending An Archive To A Remote Machine\nIt is often the case you want to send data to a remote machine. The below command creates a directory called data , compresses all files in a local folder named csv_data , with the exception of the sub-directory csv_data/intermediate_files without creating any temporary files locally:\nOptionally, create the directory on the remote machine:\n\nThen, stream the archive directly to remote. Note that providing a ‚Äî instead of a destination filename allows tar to write to a stream (stdout) that can be sent directly to the remote server.\n\n\n\nMoving Files In Different Directories Into An Archive\nIf your files exist in sibling directories, rather than under one parent directory you can use find along with tar . Suppose you want to archive all csv files relative to a directory:\n\nWhen you archive files on the fly above with find you cannot compress the files until the archive is finished being built, therefore you have to compress the tar file with the gzip command:\n\n$ gzip data.tar\n\nTip: some people like to use locate with updatedb instead of find. There are tradeoffs so make sure you read the documentation carefully!\n\n\nUnpacking & Decompressing Archives\nYou can decompress and unpack a tar file, for example data.tar.gz with the following command:\n\n$ tar -xzvf data.tar.gz\n\nIf the data is not compressed, you can leave out the -z flag:\n\n$ tar -xvf data.tar"
  },
  {
    "objectID": "notes/linux/permprocdata.html#file-permissions",
    "href": "notes/linux/permprocdata.html#file-permissions",
    "title": "Processes, Permissions and Moving Data",
    "section": "File Permissions",
    "text": "File Permissions\nBefore we begin, we must introduce some nomenclature:\n\nIf you run the command ls -a you will see something similar to the below output for all of your files in the current directory.\n\nThe file permissions are shown in three-character groupings for three different groups (nine characters total). These three groups are the owner , group , and other users. In this case, the owner name is hamel and the group name is staff\nFor the owner, the file permissions are rwx which means that the owner has read r , write w , and execute x permissions.\nFor the group, the file permissions are r-x which means the group has read and execute permissions, but not write permissions. A group is a collection of users with common permissions.\nFinally, all other users have file permissions of r‚Äì which means only read permissions.\n\nChanging File Permissions\nThere are several ways to change file permissions.\nMethod 1: Using Characters and +, -\nRefer to the nomenclature above to follow along\n\nchmod o-r csvfiles.tar.gz\nRemoves - the ability of other users o to read r the file.\nchmod g+w csvfiles.tar.gz\nAdds + the ability of the group g to write w to the file.\nchmod u+x csvfiles.tar.gz\nAdds + the ability of the owner u to execute x the file.\nchomd a+x csvfiles.tar.gz\nAdds + the ability of all users a to execute x the file.\n\nMethod 2: using numbers\nThis method works by adding up the numbers corresponding to the permissions separately for each user group (owner, group, others). For example:\n\nchmod 777 csvfiles.tar.gz\nThis gives all users the ability to read (4), write( 2), and execute (1) files. In other words 4+2+1 = 7, for the owner, group and other users.\nchmod 732 csvfiles.tar.gz\nThis gives the owner the ability to read, write and execute ( 4+2+1=7), the group the ability to write and execute (2+1=3) and all other users only the ability to write (2).\n\n\n\nChanging Ownership\nYou can change the owner or group assigned to a file like this:\nchown newuser:newgroup file\nThe :newgroup is optional, if you do not specify that the group will stay the same."
  },
  {
    "objectID": "notes/linux/cookbook.html",
    "href": "notes/linux/cookbook.html",
    "title": "Cookbook",
    "section": "",
    "text": "You should browse the table of contents of this book and use the shell scripts contained within off the shelf if possible.\n\nGitHub: https://github.com/hamelsmu/wicked_cool_shell_scripts_2e/\nLink to book on GitHub: https://github.com/hamelsmu/wicked_cool_shell_scripts_2e/blob/master/WickedCoolShellScripts2E.pdf\nBook: https://nostarch.com/wcss2"
  },
  {
    "objectID": "notes/linux/cookbook.html#shift-and-pop-args-off-and-count-args",
    "href": "notes/linux/cookbook.html#shift-and-pop-args-off-and-count-args",
    "title": "Cookbook",
    "section": "shift and $# pop args off and count args",
    "text": "shift and $# pop args off and count args\nshift.sh\n#!/bin/bash\nwhile (( $# )); do\n    echo \"process args: $1\"\n    shift\ndone\nResults in:\n$ ./shift.sh foo bar bash                                                                             \nprocess args: foo\nprocess args: bar\nprocess args: bash\n\nUsing shift for CLI options:\n#!/bin/bash\n# newquota--A frontend to quota that works with full-word flags a la GNU\n\n# quota has three possible flags, -g, -v, and -q, but this script\n#   allows them to be '--group', '--verbose', and '--quiet' too:\n\nflags=\"\"\nrealquota=\"$(which quota)\"\n\nwhile [ $# -gt 0 ]\ndo\n  case $1\n  in\n    --help)  echo \"Usage: $0 [--group --verbose --quiet -gvq]\" >&2\n                       exit 1 ;;\n    --group )  flags=\"$flags -g\";       shift ;;\n    --verbose)  flags=\"$flags -v\";   shift ;;\n    --quiet)  flags=\"$flags -q\";       shift ;;\n    --)  shift;           break ;;\n    *)  break;          # done with 'while' loop!\n  esac\ndone\n\nexec $realquota $flags \"$@\""
  },
  {
    "objectID": "notes/linux/cookbook.html#collect-all-arguments",
    "href": "notes/linux/cookbook.html#collect-all-arguments",
    "title": "Cookbook",
    "section": "$* collect all arguments",
    "text": "$* collect all arguments\nshift2.sh\n#!/bin/bash\nfor var in $*; do\n    echo $var\ndone\nResults in:\n$ ./shift2.sh foo bar bash                                                                             \nprocess args: foo\nprocess args: bar\nprocess args: bash"
  },
  {
    "objectID": "notes/linux/cookbook.html#multi-option-case-statement",
    "href": "notes/linux/cookbook.html#multi-option-case-statement",
    "title": "Cookbook",
    "section": "Multi Option Case Statement",
    "text": "Multi Option Case Statement\nwhile read command args\ndo\n  case $command\n  in\n    quit|exit) exit 0                                  ;;\n    help|\\?)   show_help                               ;;\n    scale)     scale=$args                             ;;\n    *)         scriptbc -p $scale \"$command\" \"$args\"  ;;\n  esac\n\n  /bin/echo -n \"calc> \"\ndone\n\nAnother example of case statement\n  case $1 in\n    1 ) month=\"Jan\"    ;;  2 ) month=\"Feb\"    ;;\n    3 ) month=\"Mar\"    ;;  4 ) month=\"Apr\"    ;;\n    5 ) month=\"May\"    ;;  6 ) month=\"Jun\"    ;;\n    7 ) month=\"Jul\"    ;;  8 ) month=\"Aug\"    ;;\n    9 ) month=\"Sep\"    ;;  10) month=\"Oct\"    ;;\n    11) month=\"Nov\"    ;;  12) month=\"Dec\"    ;;\n    * ) echo \"$0: Unknown numeric month value $1\" >&2; exit 1\n  esac\n  return 0"
  },
  {
    "objectID": "notes/linux/cookbook.html#collecting-stdout-with--",
    "href": "notes/linux/cookbook.html#collecting-stdout-with--",
    "title": "Cookbook",
    "section": "Collecting stdout with -",
    "text": "Collecting stdout with -\necho \"Enter something: \" | cat -"
  },
  {
    "objectID": "notes/linux/cookbook.html#formatting-long-lines-fmt",
    "href": "notes/linux/cookbook.html#formatting-long-lines-fmt",
    "title": "Cookbook",
    "section": "Formatting Long Lines fmt",
    "text": "Formatting Long Lines fmt\nWill make lines no longer than 30 characters, not cutting off any words.\nfmt -w30 long_text.txt"
  },
  {
    "objectID": "notes/linux/cookbook.html#ifs---internal-field-seperator",
    "href": "notes/linux/cookbook.html#ifs---internal-field-seperator",
    "title": "Cookbook",
    "section": "IFS - Internal Field Seperator",
    "text": "IFS - Internal Field Seperator\nSets the internal delimiter\nifs_variable.sh\n#!/bin/bash\nIFS=\":\"\nvar='a:b-c~d'\nfor n in $var\ndo\n    echo \"$n\"\ndone\nResults in\n$ ./1/ifs_variable.sh\na\nb-c~d\n\nIFS in Great Expectations Action\nI‚Äôm using this in the Great Expectations Action to parse a list of arguments given as a string to an input\n# Loop through checkpoints\nSTATUS=0\nIFS=','\nfor c in $INPUT_CHECKPOINTS;do\n    echo \"\"\n    echo \"Validating Checkpoint: ${c}\"\n    if ! great_expectations checkpoint run $c; then\n        STATUS=1\n    fi\ndone\n\n\nIFS for iterating through $PATH\n#!/bin/bash\nIFS=\":\"\nfor directory in $PATH ; do\n   echo $directory\ndone\n\n\nIFS: Double vs.¬†Single Quotes\nWith double quotes the outcome of the command expansion would be fed as one parameter to the source command. Without quotes it would be broken up into multiple parameters, depending on the value of IFS which contains space, TAB and newline by default.\nvar=\"some value\"\n\n# $var fed into cmd as one parameter\ncmd \"$var\"\n\n# $var is fed into cmd as two parameters\n#  delimted by the default IFS character, space\ncmd '$var'"
  },
  {
    "objectID": "notes/linux/cookbook.html#random",
    "href": "notes/linux/cookbook.html#random",
    "title": "Cookbook",
    "section": "$RANDOM",
    "text": "$RANDOM\necho $RANDOM will print out a random number"
  },
  {
    "objectID": "notes/linux/cookbook.html#debugging-shell-scripts--x",
    "href": "notes/linux/cookbook.html#debugging-shell-scripts--x",
    "title": "Cookbook",
    "section": "Debugging Shell Scripts -x",
    "text": "Debugging Shell Scripts -x\nDebug a script:\nbash -x myscript.sh\nOR, within a script:\nset -x # start debugging\n./myscript.sh\nset +x # stop debugging\nAll variables will be substituted and lines that are run will be printed to screen, showing the control flow of the program"
  },
  {
    "objectID": "notes/linux/cookbook.html#sourcing-files-with-.",
    "href": "notes/linux/cookbook.html#sourcing-files-with-.",
    "title": "Cookbook",
    "section": "Sourcing files with .",
    "text": "Sourcing files with .\nSo you can ‚Äúimport‚Äù scripts\n. myscript.sh\n# is equivalent to\nsource myscript.sh"
  },
  {
    "objectID": "notes/linux/cookbook.html#using-functions-to-set-exit-codes",
    "href": "notes/linux/cookbook.html#using-functions-to-set-exit-codes",
    "title": "Cookbook",
    "section": "Using functions to set exit codes",
    "text": "Using functions to set exit codes\n\nvalidAlphaNum()\n{\n  # Validate arg: returns 0 if all upper+lower+digits, 1 otherwise.\n  # Remove all unacceptable chars.\n  validchars=\"$(echo $1 | sed -e 's/[^[:alnum:]]//g')\"\n\n  if [ \"$validchars\" = \"$1\" ] ; then\n    return 0\n  else\n    return 1\n  fi\n}\n\nexit validAlphaNum"
  },
  {
    "objectID": "notes/linux/cookbook.html#know-if-someone-running-the-script-directly-with-bash_source",
    "href": "notes/linux/cookbook.html#know-if-someone-running-the-script-directly-with-bash_source",
    "title": "Cookbook",
    "section": "Know if someone running the script directly with $BASH_SOURCE",
    "text": "Know if someone running the script directly with $BASH_SOURCE\nThe variable $BASH_SOURCE can let you differentiate between when a script is run standalone vs when its invoked from another script:\nif [ \"$BASH_SOURCE\" = \"$0\" ]"
  },
  {
    "objectID": "notes/linux/cookbook.html#xargs",
    "href": "notes/linux/cookbook.html#xargs",
    "title": "Cookbook",
    "section": "xargs",
    "text": "xargs\nhttps://www.cyberciti.biz/faq/linux-unix-bsd-xargs-construct-argument-lists-utility/\n> echo 1 2 3 4 | xargs -n2 -I {} echo hello {} world                                                                                                                                                                                                                                                   \nhello 1 2 world\nhello 3 4 world"
  },
  {
    "objectID": "notes/linux/misc_utils.html",
    "href": "notes/linux/misc_utils.html",
    "title": "Misc Utilities",
    "section": "",
    "text": ";"
  },
  {
    "objectID": "notes/linux/misc_utils.html#history",
    "href": "notes/linux/misc_utils.html#history",
    "title": "Misc Utilities",
    "section": "History",
    "text": "History\n\nSee history with history command\nYou will get a number for each history item.\n\nYou can replay any number n with command !n\nHistory on OS X is stored in ~/.zsh_history\n\n!n refer to command number n in history when you call history"
  },
  {
    "objectID": "notes/linux/misc_utils.html#diff",
    "href": "notes/linux/misc_utils.html#diff",
    "title": "Misc Utilities",
    "section": "Diff",
    "text": "Diff\nYou can difff two files, you usually want to see a unified diff b/c that is easier to read\ndiff -u file1.txt file2.txt"
  },
  {
    "objectID": "notes/linux/misc_utils.html#here-documents",
    "href": "notes/linux/misc_utils.html#here-documents",
    "title": "Misc Utilities",
    "section": "Here Documents",
    "text": "Here Documents\nInstead of using echo, our script now uses cat and a here document. The string EOF (meaning end of file, a common convention) was selected as the token and marks the end of the embedded text. Note that the token must appear alone and that there must not be trailing spaces on the line.\n\nUnlike Echo, all double quotes and single quotes are escaped. Here is an example of the same thing at the command line.\n[me@linuxbox ~]$ foo=\"some text\"\n[me@linuxbox ~]$ cat << _EOF_\n> $foo\n> \"$foo\"\n> '$foo'\n> \\$foo\n> _EOF_ \nsome text \n\"some text\" \n'some text' \n$foo"
  },
  {
    "objectID": "notes/linux/misc_utils.html#mounting-devices",
    "href": "notes/linux/misc_utils.html#mounting-devices",
    "title": "Misc Utilities",
    "section": "Mounting devices",
    "text": "Mounting devices\nSometimes you need to mount these devices. Two common mount points are /mnt and /media. If you mount the device into an existing directory it will cover the contents of that directory making them invisible and unavailable.\nEx: mount device to /mnt\nmount /dev/sb1 /mnt\nEx: mount flash drive\nmount /dev/sdc1 /media\nYou can unmount a device with unmount:\nunmount /dev/sb1"
  },
  {
    "objectID": "notes/linux/misc_utils.html#getting-information-on-mounted-drives",
    "href": "notes/linux/misc_utils.html#getting-information-on-mounted-drives",
    "title": "Misc Utilities",
    "section": "Getting information on mounted drives",
    "text": "Getting information on mounted drives\ndf -h"
  },
  {
    "objectID": "notes/linux/misc_utils.html#permanently-deleting-files-with-shred",
    "href": "notes/linux/misc_utils.html#permanently-deleting-files-with-shred",
    "title": "Misc Utilities",
    "section": "Permanently deleting files with shred",
    "text": "Permanently deleting files with shred\nThis utility writes over files many times in order to erase things. Helpful for sensitive data."
  },
  {
    "objectID": "notes/actions/resources.html",
    "href": "notes/actions/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Introduction\nThese are resources that can help you get started with GitHub Actions:\n\nTalk: Getting started with Actions\nBlog: An Intro To Actions For Data Scientists\n\n\n\nGoing Deeper\nOnce you have a basic understanding, these resources can help you learn more.\n\nSee mlops-github.com for a collection of resources specifically targeted at Data Scientists using GitHub Actions.\nActions official documentation."
  },
  {
    "objectID": "notes/actions/ocotkit.html",
    "href": "notes/actions/ocotkit.html",
    "title": "ocotokit.js",
    "section": "",
    "text": "ocotokit.js is a javascript library that can help you interact with the GitHub API in a easy manner. Some javascript knowledge is helpful, but not required for many simple tasks.\nYou can use the octokit.js client along with the github-script action to quickly interface with the GitHub API to do useful things in Actions (like commenting on an issue.)\nIt is helpful to install node.js when developing scripts that interface with the GitHub API so you can test them locally."
  },
  {
    "objectID": "notes/actions/ocotkit.html#example-1-create-a-comment-on-a-pr",
    "href": "notes/actions/ocotkit.html#example-1-create-a-comment-on-a-pr",
    "title": "ocotokit.js",
    "section": "Example 1: Create A Comment On A PR",
    "text": "Example 1: Create A Comment On A PR\nLet‚Äôs say you want to programatically make a comment on a pull request with a url that includes the branch name, but you are only given the pull request number. We first lookup the branch name associated with the pull request and pass that to the method call that makes an issue comment:\n//Instantiate octokit client\nconst { Octokit } = require(\"@octokit/rest\");\nconst octokit = new Octokit({\n    auth: \"<YOUR_PERSONAL_ACCESS_TOKEN>\",\n  });\n\n  //Take an action (create a comment) triggered by an issue comment\n\n  // Get information about the pr\n  octokit.pulls.get({\n    owner: 'hamelsmu',\n    repo: 'test_html',\n    pull_number: 1\n  }).then( (pr) => {\n    // use the branch name from the pr to make a pr comment\n    var BRANCH_NAME = pr.data.head.ref\n    octokit.issues.createComment({\n        issue_number: 1,\n        owner: 'hamelsmu',\n        repo: 'test_html',\n        body: `[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/hamelsmu/test_html/${BRANCH_NAME}) :point_left: Launch a binder notebook on this branch`\n      })\n  })"
  },
  {
    "objectID": "notes/actions/ocotkit.html#example-2-issue-comment",
    "href": "notes/actions/ocotkit.html#example-2-issue-comment",
    "title": "ocotokit.js",
    "section": "Example 2: Issue Comment",
    "text": "Example 2: Issue Comment\nThis is a simple example of how you can create an issue comment.\n  //Instantiate octokit client\n  const { Octokit } = require(\"@octokit/rest\");\n  const octokit = new Octokit({\n    auth: \"<YOUR_PERSONAL_ACCESS_TOKEN>\",\n    });\n\n  // Create an issue commment\n  var BRANCH_NAME = 'hamelsmu-patch-1'\n  octokit.issues.createComment({\n      issue_number: 1,\n      owner: 'hamelsmu',\n      repo: 'test_html',\n      body: `[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/hamelsmu/test_html/${BRANCH_NAME}) :point_left: Launch a binder notebook on this branch`\n    })"
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html",
    "href": "notes/web-scraping/browser-to-python.html",
    "title": "Browser requests to code",
    "section": "",
    "text": "I learned this from Zachary Blackwood‚Äôs 2022 NormConf Talk."
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "href": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "title": "Browser requests to code",
    "section": "Example: Get A List of Subway Restaurants With Python",
    "text": "Example: Get A List of Subway Restaurants With Python\n\nGo to https://www.subway.com/en-US/locator in Google Chrome\n\n\n\nOpen developer tools using Option + CMD + I\nGo the the network tab, and hit the clear button\n\n\n\nType in a zipcode and search. Look for a network request that seems like it is getting data, in this case GetLocations.ashx... looks super promising.\n\n\n\nRight click on that particular event and select Copy -> Copy as Curl\n\n\n\nGo to curlconverter.com and paste the curl command there.\n\n\nEnjoy your python code that uses this otherwise undocumented API :)"
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "href": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "title": "Browser requests to code",
    "section": "Bonus: Parse The Response",
    "text": "Bonus: Parse The Response\nYou can parse the response data in a hacky way.\n\n# run the code from curlconverter.com, which will give you a `response` object.\n\n>>> import json\n... response_string = response.text\n... json_string = response_string[response_string.index(\"(\") +1:response_string.index('\"AdditionalData\":')-1]+'}'\n... parsed_string = json.loads(json_string)\n... stores = parsed_string['ResultData']\n\n>>> stores\n[{'LocationId': {'StoreNumber': 21809, 'SatelliteNumber': 0},\n  'Address': {'Address1': '4888 NW Bethany Blvd',\n   'Address2': 'Suite K-1',\n   'Address3': 'Bethany Village Centre',\n   'City': 'Portland',\n   'StateProvCode': 'OR',\n   'PostalCode': '97229',\n   'CountryCode': 'US',\n   'CountryCode3': 'USA'},\n  'Geo': {'Latitude': 45.5548,\n   'Longitude': -122.8358,\n   'TimeZoneId': 'America/Los_Angeles',\n   'CurrentUtcOffset': 0},\n  'ListingNumber': 1,\n  'OrderingUrl': 'http://order.subway.com/Stores/Redirect.aspx?s=21809&sa=0&f=r&scc=US&spc=OR',\n  'CateringUrl': 'https://www.ezcater.com/catering/pvt/subway-portland-nw-bethany-blvd',\n  'ExtendedProperties': None},\n..."
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "href": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "title": "Browser requests to code",
    "section": "When to use this approach",
    "text": "When to use this approach\nThis is great for adhoc things, but you probably want to use a headless browser and actually scrape the HTML if you want to do this in a repeatable way. But many times you want to do a one-off scrape, this isn‚Äôt so bad!"
  },
  {
    "objectID": "notes/video_editing.html",
    "href": "notes/video_editing.html",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -> ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location\n\n\n\nHere is a circular camera filter with OBS, which might be easier than DVR.\nYou can crop like this\n\n\n\nYou can add pause recording as a hotkey in OBS"
  },
  {
    "objectID": "notes/video_editing.html#other-tools-to-look-into",
    "href": "notes/video_editing.html#other-tools-to-look-into",
    "title": "Video Editing",
    "section": "Other tools to look into",
    "text": "Other tools to look into\n\nDescript\nRunwayML\ncapcut - from Rajeev\nAdobe Premiere\nFrame - Video collaboration that you use for Upwork etc\nEpidemic Sound - Sound by mood (Sanyam)\nCayla - Artlist\nCayla - Premium Beat\n\nCayla recommmends 1080p / 24 FPS for Youtube"
  },
  {
    "objectID": "notes/quarto/listings-from-data.html",
    "href": "notes/quarto/listings-from-data.html",
    "title": "Listings from data",
    "section": "",
    "text": "You don‚Äôt need to have blog posts to create a listing on a Quarto page. For example, you can combine the following three yaml files:\nThis file specifies a list of blog posts that you can have elsewhere\nIn the front matter of any page (like index.qmd) you can reference blogs.yml and _metadata.yml like so:"
  },
  {
    "objectID": "notes/quarto/listings-from-data.html#results",
    "href": "notes/quarto/listings-from-data.html#results",
    "title": "Listings from data",
    "section": "Results",
    "text": "Results\nThis will generate a list of blog posts that you can see here on my page, this is in the table format. However, you can have pictures on your listing as well, which you can see from the Minimal Example."
  },
  {
    "objectID": "notes/quarto/listings-from-data.html#minimal-example",
    "href": "notes/quarto/listings-from-data.html#minimal-example",
    "title": "Listings from data",
    "section": "Minimal Example",
    "text": "Minimal Example\nHere is a minimal example of creating an index page of all your blog posts. It uses slightly different options than I did in the above example. You can see the code for that here."
  },
  {
    "objectID": "notes/quarto/listings-from-data.html#resources",
    "href": "notes/quarto/listings-from-data.html#resources",
    "title": "Listings from data",
    "section": "Resources",
    "text": "Resources\n\nQuarto listings\nQuarto shared metadata"
  },
  {
    "objectID": "notes/quarto/merging.html",
    "href": "notes/quarto/merging.html",
    "title": "Merge listings",
    "section": "",
    "text": "You can now merge listings by referencing multiple directories or files in the front matter. This allows you to create a single listing of all your external blogs you may write on other platforms, Medium, substack, your work blog, etc, with your own blogs.\nThe kind folks at Quarto have made an update in their latest pre-release that allows you to merge multiple listings like this:\nThe blog_data/blogs.yml is a listing from data while blog/posts is a Quarto posts directory of blog posts.\nSee source code for my blog for an example."
  },
  {
    "objectID": "notes/quarto/merging.html#resources",
    "href": "notes/quarto/merging.html#resources",
    "title": "Merge listings",
    "section": "Resources",
    "text": "Resources\n\nQuarto listings\nQuarto blog posts"
  },
  {
    "objectID": "notes/quarto/highlighting.html",
    "href": "notes/quarto/highlighting.html",
    "title": "Syntax Highlighting",
    "section": "",
    "text": "Syntax highlighting in Quarto follows the way pandoc handles syntax highlighting. There are two important concepts concerning syntax highlighting:"
  },
  {
    "objectID": "notes/quarto/highlighting.html#syntax-color-themes",
    "href": "notes/quarto/highlighting.html#syntax-color-themes",
    "title": "Syntax Highlighting",
    "section": "1. Syntax Color Themes",
    "text": "1. Syntax Color Themes\nSyntax color themes allow you to customize the colors shown in syntax highlighting. These are expressed with the highlight-style setting. You can change the syntax color theme in either your front matter or site-wide in _quarto.yml like this:\n\nFront Matter_quarto.yml\n\n\n---\nhighlight-style: custom.theme\n---\nIf you have both light and dark themes, you will likely want to set those separately like this:\n---\nhighlight-style:\n  light: custom-light.theme\n  dark: custom-dark.theme\n---\n\n\n\n\n_quarto.yml\n\nformat:\n  html:\n    theme: \n      light: assets/ek-theme-light.scss\n      dark: assets/ek-theme-dark.scss\n    highlight-style: \n      light: assets/ek-light.theme\n      dark: assets/ek-dark.theme\n\n\n\n\nThese color themes are defined by json files with the schema defined here. However, it is recommended that you choose one of the themes that quarto already provides and edit that. By default, Quarto uses the arrow-light theme. This means if you are happy with the way Quarto is highlighting syntax, you can just tweak this theme. Personally, my favorite theme is dracula. It is useful to look through these different themes to get a sense of the types of things you can change."
  },
  {
    "objectID": "notes/quarto/highlighting.html#syntax-definitions",
    "href": "notes/quarto/highlighting.html#syntax-definitions",
    "title": "Syntax Highlighting",
    "section": "2. Syntax Definitions",
    "text": "2. Syntax Definitions\nSyntax definitions define the rules by which syntax is highlighted. A rule is a string, character or regular expression against which to match the text being analyzed. This is helpful if you need to document a language that isn‚Äôt supported by Quarto out of the box. You can see the list of supported languages with this command:\nquarto pandoc --list-highlight-languages\nSyntax definitions are defined in xml files that follow this schema. Examples of syntax definitions for various languages can be found here.\nQuarto has additional example syntax definitions here which are useful to look at. Note how the name of the language and its file extensions are defined in the XML file.\nIn order to supply an additional syntax definition or override an existing one, set the syntax-definitions in your _quarto.yml file like this:\n\n\n_quarto.yml\n\nformat:\n  html:\n    syntax-definitions: \n    - new_language.xml\n\nAn example of defining a new language is illustrated below.\n\nExample\nSuppose you have a new programming language called Fomo that is just like Python, except you can define functions with fomo in addition to def. For example, consider this Python code:\ndef hello_world():\n    \"An example\"\n    pass\nUnfortunately, If you try to use the Python code fence for Fomo it looks like this:\nfomo hello_world():\n    \"An example\"\n    pass\nSince Fomo is almost identical to Python, you can start by copying the python syntax definition into a file named fomo.xml and edit the language name, style and extension fields like so:\n\n\nfomo.xml\n\n- <language name=\"Python\" version=\"26\" style=\"python\" indenter=\"python\" kateversion=\"5.0\" section=\"Scripts\" extensions=\"*.py;*.pyw;*.pyi;SConstruct;SConscript;*.FCMacro\" ...\n+ <language name=\"Fomo\" version=\"26\" style=\"fomo\" indenter=\"python\" kateversion=\"5.0\" section=\"Scripts\" extensions=\"*.fomo\" ...\n\nYou also have to add fomo to the list of defs like this:\n\n\nfomo.xml\n\n        <list name=\"defs\">\n            <item>class</item>\n            <item>def</item>\n+           <item>fomo</item>\n            <item>del</item>\n            <item>global</item>\n            <item>lambda</item>\n            <item>nonlocal</item>\n        </list>\n\nAfter that, you can add the Fomo syntax definition to your Quarto project with the syntax-definitions option like so:\n\n\n_quarto.yml\n\nformat:\n  html:\n    syntax-definitions: \n    - fomo.xml\n\nIn this case, fomo.xml is in the root of the Quarto project, but you can put it in a sub-folder as well.\nAfter doing this, you can use the ```fomo code fence, and your code will be highlighted correctly!\nfomo hello_world():\n    \"An example\"\n    pass"
  },
  {
    "objectID": "notes/jupyter/remote_browser.html",
    "href": "notes/jupyter/remote_browser.html",
    "title": "Remote Browser For Jupyter",
    "section": "",
    "text": "It is very common to connect to a remote Jupyter server with your local browser. However, if you lose connection with your remote server, logs printed to the screen may stop streaming. This is common when training deep learning models where training runs can last days or weeks where progress bars are printed to the screen in a notebook.\nTo avoid the issue with your browser loosing connection you can run the browser remotely on the same machine as the Jupyter server, even if your remote server does not have a desktop/GUI interface."
  },
  {
    "objectID": "notes/jupyter/remote_browser.html#fast.ai",
    "href": "notes/jupyter/remote_browser.html#fast.ai",
    "title": "Remote Browser For Jupyter",
    "section": "fast.ai",
    "text": "fast.ai\nThe below youtube link (at timestamp 1:58:33), from fastai Lesson 10 Part 2 (2018) will walk you through how to accomplish this."
  },
  {
    "objectID": "notes/jupyter/Best Way To Launch Jupyter On A Remote Server.html",
    "href": "notes/jupyter/Best Way To Launch Jupyter On A Remote Server.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "jupyter lab --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port 8081"
  },
  {
    "objectID": "notes/jupyter/shortcuts.html",
    "href": "notes/jupyter/shortcuts.html",
    "title": "My Jupyter Shortcuts",
    "section": "",
    "text": "People complain about ‚Äústate‚Äù in Jupyter. This can be easily avoided by frequently restarting the kernel and running all cells from the top. Thankfully, you can set a hotkey that allows you to do this effortlessly. In Jupyter Lab, go to Settings then Advanced Settings Editor. Copy and paste the below json into the User Prefences pane. If you already have user-defined shortcuts, modify this appropriately.\n{\n    \"shortcuts\": [\n        {\n            \"args\": {},\n            \"command\": \"application:activate-next-tab\",\n            \"keys\": [\n                \"Ctrl Shift ]\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-next-tab-bar\",\n            \"keys\": [\n                \"Ctrl Shift .\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-previous-tab\",\n            \"keys\": [\n                \"Ctrl Shift [\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-previous-tab-bar\",\n            \"keys\": [\n                \"Ctrl Shift ,\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:close\",\n            \"keys\": [\n                \"Alt W\"\n            ],\n            \"selector\": \".jp-Activity\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:toggle-left-area\",\n            \"keys\": [\n                \"Accel B\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:toggle-mode\",\n            \"keys\": [\n                \"Accel Shift D\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"apputils:activate-command-palette\",\n            \"keys\": [\n                \"Accel Shift C\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"apputils:print\",\n            \"keys\": [\n                \"Accel P\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-console\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-CodeConsole-promptCell .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-file\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-FileEditor .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-notebook\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:linebreak\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='notebook'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:linebreak\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-forced\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='notebook'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-forced\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-unforced\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:continue\",\n            \"keys\": [\n                \"F9\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-console\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-CodeConsole\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-file\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-FileEditor\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-notebook\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-Notebook\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:next\",\n            \"keys\": [\n                \"F10\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:stepIn\",\n            \"keys\": [\n                \"F11\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:stepOut\",\n            \"keys\": [\n                \"Shift F11\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:terminate\",\n            \"keys\": [\n                \"Shift F9\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"docmanager:save\",\n            \"keys\": [\n                \"Accel S\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"docmanager:save-as\",\n            \"keys\": [\n                \"Accel Shift S\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:highlightNext\",\n            \"keys\": [\n                \"Accel G\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:highlightPrevious\",\n            \"keys\": [\n                \"Accel Shift G\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:start\",\n            \"keys\": [\n                \"Accel F\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"editmenu:redo\",\n            \"keys\": [\n                \"Accel Shift Z\"\n            ],\n            \"selector\": \"[data-jp-undoer]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"editmenu:undo\",\n            \"keys\": [\n                \"Accel Z\"\n            ],\n            \"selector\": \"[data-jp-undoer]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:copy\",\n            \"keys\": [\n                \"Accel C\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:create-main-launcher\",\n            \"keys\": [\n                \"Accel Shift L\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:cut\",\n            \"keys\": [\n                \"Accel X\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:delete\",\n            \"keys\": [\n                \"Delete\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:duplicate\",\n            \"keys\": [\n                \"Accel D\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:go-up\",\n            \"keys\": [\n                \"Backspace\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:paste\",\n            \"keys\": [\n                \"Accel V\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:rename\",\n            \"keys\": [\n                \"F2\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:toggle-main\",\n            \"keys\": [\n                \"Accel Shift F\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filemenu:close-and-cleanup\",\n            \"keys\": [\n                \"Ctrl Shift Q\"\n            ],\n            \"selector\": \".jp-Activity\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:flip-horizontal\",\n            \"keys\": [\n                \"H\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:flip-vertical\",\n            \"keys\": [\n                \"V\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:invert-colors\",\n            \"keys\": [\n                \"I\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:reset-image\",\n            \"keys\": [\n                \"0\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:rotate-clockwise\",\n            \"keys\": [\n                \"]\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:rotate-counterclockwise\",\n            \"keys\": [\n                \"[\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:zoom-in\",\n            \"keys\": [\n                \"=\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:zoom-out\",\n            \"keys\": [\n                \"-\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"inspector:open\",\n            \"keys\": [\n                \"Accel I\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"kernelmenu:interrupt\",\n            \"keys\": [\n                \"I\",\n                \"I\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"kernelmenu:restart\",\n            \"keys\": [\n                \"0\",\n                \"0\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"runmenu:restart-and-run-all\",\n            \"keys\": [\n                \"0\",\n                \"R\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:restart-and-run-to-selected\",\n            \"keys\": [\n                \"0\",\n                \"S\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-code\",\n            \"keys\": [\n                \"Y\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-1\",\n            \"keys\": [\n                \"1\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-2\",\n            \"keys\": [\n                \"2\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-3\",\n            \"keys\": [\n                \"3\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-4\",\n            \"keys\": [\n                \"4\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-5\",\n            \"keys\": [\n                \"5\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-6\",\n            \"keys\": [\n                \"6\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-markdown\",\n            \"keys\": [\n                \"M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-raw\",\n            \"keys\": [\n                \"R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:copy-cell\",\n            \"keys\": [\n                \"C\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:cut-cell\",\n            \"keys\": [\n                \"X\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:delete-cell\",\n            \"keys\": [\n                \"D\",\n                \"D\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-command-mode\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-command-mode\",\n            \"keys\": [\n                \"Ctrl M\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-edit-mode\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-above\",\n            \"keys\": [\n                \"Shift ArrowUp\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-above\",\n            \"keys\": [\n                \"Shift K\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-below\",\n            \"keys\": [\n                \"Shift ArrowDown\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-below\",\n            \"keys\": [\n                \"Shift J\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-bottom\",\n            \"keys\": [\n                \"Shift End\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-top\",\n            \"keys\": [\n                \"Shift Home\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:insert-cell-above\",\n            \"keys\": [\n                \"A\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:insert-cell-below\",\n            \"keys\": [\n                \"B\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cell-above\",\n            \"keys\": [\n                \"Ctrl Backspace\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cell-below\",\n            \"keys\": [\n                \"Ctrl Shift M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cells\",\n            \"keys\": [\n                \"Shift M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-down\",\n            \"keys\": [\n                \"ArrowDown\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-down\",\n            \"keys\": [\n                \"J\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-up\",\n            \"keys\": [\n                \"ArrowUp\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-up\",\n            \"keys\": [\n                \"K\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:paste-cell-below\",\n            \"keys\": [\n                \"V\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:redo-cell-action\",\n            \"keys\": [\n                \"Shift Z\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-insert-below\",\n            \"keys\": [\n                \"Alt Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-insert-below\",\n            \"keys\": [\n                \"Alt Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-select-next\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"runmenu:run\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \"[data-jp-code-runner]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"settingeditor:open\",\n            \"keys\": [\n                \"Accel ,\"\n            ],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"settingeditor:save\",\n            \"keys\": [\n                \"Accel S\"\n            ],\n            \"selector\": \".jp-SettingEditor\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tabsmenu:activate-previously-used-tab\",\n            \"keys\": [\n                \"Accel Shift '\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:dismiss\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \"body.jp-mod-tooltip .jp-Notebook\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:dismiss\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \"body.jp-mod-tooltip .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-console\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-CodeConsole-promptCell .jp-InputArea-editor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-file\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-FileEditor .jp-CodeMirrorEditor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-notebook\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode .jp-InputArea-editor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace):not(.jp-mod-completer-active)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-all-cell-line-numbers\",\n            \"keys\": [\n                \"Shift L\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-cell-line-numbers\",\n            \"keys\": [\n                \"L\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:split-cell-at-cursor\",\n            \"keys\": [\n                \"Ctrl Shift -\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-render-side-by-side\",\n            \"keys\": [\n                \"Ctrl Shift R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:select-all\",\n            \"keys\": [\n                \"Accel A\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-render-side-by-side-current\",\n            \"keys\": [\n                \"Shift R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:undo-cell-action\",\n            \"keys\": [\n                \"Z\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        }\n    ]\n}"
  },
  {
    "objectID": "notes/jupyter/Fix Jupyter CUDA cache.html",
    "href": "notes/jupyter/Fix Jupyter CUDA cache.html",
    "title": "Fix Jupyter CUDA cache",
    "section": "",
    "text": "[[CUDA]] [[Jupyter tip]]\napparently this is meant to work %config ZMQInteractiveShell.cache_size = 0 %reset -f out is meant to remove all stuff in the cache\nhttps://discord.com/channels/689892369998676007/766837559920951316/1037245359027658762"
  },
  {
    "objectID": "oss/opensource.html",
    "href": "oss/opensource.html",
    "title": " Open Source",
    "section": "",
    "text": "My open soruce work has been focused on developer tools and infrastructure. I‚Äôve contributed to projects such as fastai, Metaflow, Kubeflow, Jupyter, and Great Expectations, as well as many others. I list some of these below:"
  },
  {
    "objectID": "oss/opensource.html#fastai",
    "href": "oss/opensource.html#fastai",
    "title": " Open Source",
    "section": " fastai",
    "text": "fastai\nI maintain and contribute to a variety of fastai projects. Below are the projects I‚Äôve been very involved in:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nfastpages \n\n\nAn easy to use blogging platform for Jupyter Notebooks.\n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nnbdev \n\n\nWrite, test, document, and distribute software packages and technical articles all in one place, your notebook.\n\n\nCore Contributor\n\n\nBlog, Talk\n\n\n\n\nfastcore \n\n\nA Python language extension for exploratory and literate programming.\n\n\nCore Contributor\n\n\nBlog\n\n\n\n\nghapi \n\n\nA Python client for the GitHub API\n\n\nCore Contributor\n\n\nBlog\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#metaflow",
    "href": "oss/opensource.html#metaflow",
    "title": " Open Source",
    "section": " Metaflow",
    "text": "Metaflow\nI created notebook cards: A tool that allows you to use notebooks to generate reports, visualizations and diagnostics in Metaflow production workflows. Blog"
  },
  {
    "objectID": "oss/opensource.html#kubeflow",
    "href": "oss/opensource.html#kubeflow",
    "title": " Open Source",
    "section": " Kubeflow",
    "text": "Kubeflow\nI‚Äôve worked on several projects related to Kubeflow, mainly around examples and documentation:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nGitHub Issue Summarization\n\n\nAn end-to-end example of using Kubeflow to summarize GitHub Issues. Became one of the most popular tutorials of Kubeflow.\n\n\nAuthor\n\n\nInterview with Jeremy Lewi\n\n\n\n\nkubeflow/codei-intelligence\n\n\nVarious tutorials and applied examples of Kubeflow.\n\n\nCore Contributor\n\n\nTalk\n\n\n\n\nThe Kubeflow Blog\n\n\nI used fastpages to create the official Kubeflow blog.\n\n\nCore Contributor\n\n\nSite\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/opensource.html#jupyter",
    "href": "oss/opensource.html#jupyter",
    "title": " Open Source",
    "section": " Jupyter",
    "text": "Jupyter\nI created the Repo2Docker GitHub Action, which allows you to trigger repo2docker to build a Jupyter enabled Docker images from your GitHub repository. This Action allows you to pre-cache images for your own BinderHub cluster or for mybinder.org.\nThis project was accepted into the official JupyterHub GitHub org."
  },
  {
    "objectID": "oss/opensource.html#great-expectations",
    "href": "oss/opensource.html#great-expectations",
    "title": " Open Source",
    "section": " Great Expectations",
    "text": "Great Expectations\nI developed the Great Expectations GitHub Action that allows you to use Great Expectations in CI/CD Workflows. Blog."
  },
  {
    "objectID": "oss/opensource.html#other",
    "href": "oss/opensource.html#other",
    "title": " Open Source",
    "section": " Other",
    "text": "Other\nI worked as a staff machine learning engineer at GitHub from 2017 - 2022. I led or created the following open source projects that explored the intersection of machine learning, data and the developer workflow:\n\n\n\n\n\n\nProject\n\n\nDescription\n\n\nRole\n\n\nOther References\n\n\n\n\n\n\nCode Search Net \n\n\nDatasets, tools, and benchmarks for representation learning of code. This was a big part of the inspiration for GitHub‚Äôs eventual work on CoPilot.\n\n\nLead\n\n\nBlog, Paper\n\n\n\n\nMachine Learning Ops\n\n\nA collection of resources on how to facilitate Machine Learning Ops with GitHub. This project explored integrations with a wide variety of data science tools with GitHub Actions.\n\n\nCreator\n\n\nBlog\n\n\n\n\nIssue Label Bot\n\n\nA GitHub App powered by machine learning that auto-labels issues.\n\n\nCreator\n\n\nBlog, Talk\n\n\n\n\nCovid19-dashboard \n\n\nA demonstration of how to use GitHub Actions, Jupyter Notebooks and fastpages to create interactive dashboards that update daily.\n\n\nCreator\n\n\nNews Article\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/abir2023.html",
    "href": "blog/posts/abir2023.html",
    "title": "Abir et al.¬†(2023)",
    "section": "",
    "text": "The purpose of exploration is to reduce goal-relevant uncertainty. This can be achieved by choosing to explore the parts of the environment one is most uncertain about. Humans, however, often choose to avoid uncertainty. How do humans balance approaching and avoiding uncertainty during exploration? To answer this question, we developed a task requiring participants to explore a simulated environment towards a clear goal. We compared human choices to the predictions of the optimal exploration policy and a hierarchy of simpler strategies. We found that participants generally explored the object they were more uncertain about. However, when overall uncertainty about choice options was high, participants avoided objects they were more uncertain about, learning instead about better known objects. We examined reaction times and individual differences to understand the costs and benefits of this strategy. We conclude that balancing approaching and avoiding uncertainty ameliorates the costs of exploration in a resource-rational manner."
  },
  {
    "objectID": "blog/posts/abir2023.html#references",
    "href": "blog/posts/abir2023.html#references",
    "title": "Abir et al.¬†(2023)",
    "section": "References",
    "text": "References\njournal: PsyArXiv\npaper_url: https://doi.org/10.31234/osf.io/gtxam\ndata_url: https://osf.io/6zyev/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About OpenData",
    "section": "",
    "text": "OpenData is a database of publicly available behavioral datasets. To browse the database, click on the links above or use the search bar at the top-right of this page.\n\nWhat is the goal of OpenData?\n\nThe goal of this project is simply to make it easier for researchers to find and use publicly available behavioral data as part of research. There‚Äôs already so much out there that can be used to:\n\nTest new hypotheses or models\nCalculate effect sizes for power analysis\nEstimate meta-analytic effects across studies\n\n\nWhat is the scope of OpenData?\n\nThe scope of this project is to catalogue any and all open datasets involving experimental or cognitive tasks (e.g., Stroop, delay discounting, 2-arm bandits). Datasets involving more naturalistic behaviors are also welcomed. The only firm requirement is that trial-level data must be available.\n\nWho maintains OpenData?\n\nThis project is maintained by Bo Yuan with help from other members of the SCMD Lab.\n\nHow can I add a dataset?\n\nPlease see the contributing page.\n\nHow can I report an issue?\n\nPlease open an issue on our Github or directly contact the maintainer."
  },
  {
    "objectID": "blog/posts/algermissen2021.html",
    "href": "blog/posts/algermissen2021.html",
    "title": "Algermissen et al.¬†(2021)",
    "section": "",
    "text": "Action selection is biased by the valence of anticipated outcomes. To assess mechanisms by which these motivational biases are expressed and controlled, we measured simultaneous EEG-fMRI during a motivational Go/NoGo learning task (N = 36), leveraging the temporal resolution of EEG and subcortical access of fMRI. VmPFC BOLD encoded cue valence, importantly predicting trial-by-trial valence-driven response speed differences and EEG theta power around cue onset. In contrast, striatal BOLD encoded selection of active Go responses and correlated with theta power around response time. Within trials, theta power ramped in the fashion of an evidence accumulation signal for the value of making a Go response, capturing the faster responding to reward cues. Our findings reveal a dual nature of midfrontal theta power, with early components reflecting the vmPFC contribution to motivational biases, and late components reflecting their striatal translation into behavior, in line with influential recent value of work theories of striatal processing."
  },
  {
    "objectID": "blog/posts/algermissen2021.html#references",
    "href": "blog/posts/algermissen2021.html#references",
    "title": "Algermissen et al.¬†(2021)",
    "section": "References",
    "text": "References\njournal: Cereb. Cortex\npaper_url: https://doi.org/10.1093/cercor/bhab391\ndata_url: https://data.donders.ru.nl/collections/di/dccn/DSC_3017042.02_604?6"
  },
  {
    "objectID": "blog/posts/albrecht2016.html",
    "href": "blog/posts/albrecht2016.html",
    "title": "Albrecht et al.¬†(2016)",
    "section": "",
    "text": "The negative symptoms of schizophrenia (SZ) are associated with a pattern of reinforcement learning (RL) deficits likely related to degraded representations of reward values. However, the RL tasks used to date have required active responses to both reward and punishing stimuli. Pavlovian biases have been shown to affect performance on these tasks through invigoration of action to reward and inhibition of action to punishment, and may be partially responsible for the effects found in patients. Forty-five patients with schizophrenia and 30 demographically-matched controls completed a four-stimulus reinforcement learning task that crossed action (Go or NoGo) and the valence of the optimal outcome (reward or punishment-avoidance), such that all combinations of action and outcome valence were tested. Behaviour was modelled using a six-parameter RL model and EEG was simultaneously recorded. Patients demonstrated a reduction in Pavlovian performance bias that was evident in a reduced Go bias across the full group. In a subset of patients administered clozapine, the reduction in Pavlovian bias was enhanced. The reduction in Pavlovian bias in SZ patients was accompanied by feedback processing differences at the time of the P3a component. The reduced Pavlovian bias in patients is suggested to be due to reduced fidelity in the communication between striatal regions and frontal cortex. It may also partially account for previous findings of poorer Go-learning in schizophrenia where Go responses or Pavlovian consistent responses are required for optimal performance. An attenuated P3a component dynamic in patients is consistent with a view that deficits in operant learning are due to impairments in adaptively using feedback to update representations of stimulus value."
  },
  {
    "objectID": "blog/posts/albrecht2016.html#references",
    "href": "blog/posts/albrecht2016.html#references",
    "title": "Albrecht et al.¬†(2016)",
    "section": "References",
    "text": "References\njournal: PLoS One\npaper_url: https://doi.org/10.1371/journal.pone.0152781\ndata_url: https://zenodo.org/record/29601"
  }
]